{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DS 4300 - Spring 2025 \n",
    "# Practical 02\n",
    "\n",
    "# Due: \n",
    "# Project needs to be functional for Exam on March 24.\n",
    "# Final deliverable and repo to be submitted by March 26 @ 11:59pm. \n",
    "\n",
    "# Overview:\n",
    "\n",
    "# In this project, you and your team will build a local Retrieval-Augmented Generation system that allows a user to query the collective DS4300 notes from members of your team.  Your system will do the following:\n",
    "\n",
    "# Ingest a collection of documents that represent material, such as course notes, you and your team have collected throughout the semester. \n",
    "# Index those documents using embedding and a vector database\n",
    "# Accept a query from the user. \n",
    "# Retrieve relevant context based on the user’s query\n",
    "# Package the relevant context up into a prompt that is passed to a locally-running LLM to generate a response. \n",
    "\n",
    "# In this, you’ll experiment with different variables - chunking strategies, embedding models, some prompt engineering, local LLM, and vector database options.  You’ll analyze how these changes affect the system’s performance and output quality. \n",
    "\n",
    "# Teams and Corpus:\n",
    "\n",
    "# Each team should be composed of 2 - 4 members.  They can be the same as Practical 01, but teams may switch up. \n",
    "\n",
    "# Each team should gather a collection of course notes taken by the team members.  This can be the slide decks, personal notes taken throughout, and additional documentation for the tools/systems we have used.  \n",
    "\n",
    "# Tools:\n",
    "\n",
    "# Python for building the pipeline\n",
    "# Ollama for running LLMs locally (you’ll compare and contrast at least 2 different models\n",
    "# Vector Databases (Redis Vector DB, Chroma, and one other of your choosing)\n",
    "# Embedding Models (you’ll compare and contrast at least 3 different options)  \n",
    "\n",
    "# Variables to Explore:\n",
    "\n",
    "# Text preprocessing & chunking \n",
    "# Try various size chunks: 200, 500, 1000 tokens, for example\n",
    "# Try different chunk overlap sizes: 0, 50, 100 token overlap for example\n",
    "# Try various basic text prep strategies such as removing whitespace, punctuation, and any other “noise”. \n",
    "# Embedding Models - Choose 3 to compare and contrast. Examples include: \n",
    "# sentence-transformers/all-MiniLM-L6-v2\n",
    "# sentence-transformers/all-mpnet-base-v2\n",
    "# InstructorXL\n",
    "# The model we used in class\n",
    "# Measure interesting properties of using the various embedding models such as speed, memory usage, and retrieval quality (qualitative). \n",
    "# Vector Database - At a minimum, compare and contrast Redis Vector DB and Chroma (you’ll have to do a little research on this db) and one other vector database you choose based on research.  Examine the speed of indexing and querying as well as the memory usage. \n",
    "# Tweaks to the System prompt. Use the one from the class example as a starting point. \n",
    "# Try at least 2 different local LLMs.  Examples include Llama 2 7B and Mistral 7B.  You aren’t required to use these two specifically, however. \n",
    "\n",
    "# Suggested Steps:\n",
    "\n",
    "# Collect and clean the data.  \n",
    "# If you’re using PDFs, review the output of whichever Python PDF library you’re using.  Is it what you expect? \n",
    "# Do you want to pre-process the raw text in some way before indexing?  Perhaps remove extra white space or remove stop words?\n",
    "# Implement a driver Python script to execute your various versions of the indexing pipeline and to collect important data about the process (memory, time, etc).  Systematically vary the chunking strategies, embedding models, various prompt tweaks, choice of Vector DB, and choice of LLM. \n",
    "# Develop a set of user questions that you give to each pipeline and qualitatively review the responses. \n",
    "# Choose which pipeline you think works the best, and justify your choice. \n",
    "\n",
    "# Deliverables:\n",
    "\n",
    "# As a team, you’ll produce a slide deck communicating your findings as well as your final choice of pipelines with justification. Be specific.  (Template for deck will be forthcoming.)\n",
    "\n",
    "# You will also include a public GitHub repository containing well-organized set of scripts related to the various pipelines your team tests.  The README should describe how to execute your project.  \n",
    "\n",
    "# More details on deliverables will be shared soon. \n",
    "\n",
    "\n",
    "# Areas for Evaluation:\n",
    "# Robustness of Experimentation (30%)\n",
    "# Analysis of collected data (30%)\n",
    "# Recommendation of pipeline organizations (20%)\n",
    "# Professionalism of Slide Deck (20%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.ingest\n",
    "import src.search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing Redis store...\n",
      "Redis store cleared.\n",
      "Index created successfully.\n",
      "Stored embedding for: STARTING TREE for LL and LR Original Tree Structure: A / \\ B T3 / \\ T1 T2 Before an insertion, height(A.left) and height(A.right) differ by 1. Thus, this tree is AVL-balanced initially. AVL trees maintain balance by ensuring the heights of two child subtrees of any node differ by no more than one. ⸻ Case 1: Left-Left Imbalance (Single Rotation) Imbalance situation (after inserting into subtree T1): A (imbalanced) / \\ B T3 / \\ T1 T2 In this scenario, after insertion into subtree T1, node A becomes imbalanced, specifically due to a left-heavy subtree (B → T1). This is referred to as an LL imbalance (Left-Left). Resolving LL Imbalance: A single right rotation around the imbalanced node (A) is required to correct this imbalance: • The left child B becomes the new root of this subtree. • Node A becomes the right child of node B. • Subtrees T2 and T3 are attached appropriately. Result after rotation: B / \\ T1 A / \\ T2 T3\n",
      "Stored embedding for: After rotation, node B now has the same height that A previously had before rotation. This restores AVL balance. Summary of LL rotation (pseudo code): fun rotateRight(oldRoot, parentOld): newRoot = oldRoot.left oldRoot.left = newRoot.right newRoot.right = oldRoot updateHeight(oldRoot) updateHeight(newRoot) ⸻ Case 2: Left-Right Imbalance (Double Rotation) Original imbalance scenario (after insertion into subtree T2): A (imbalanced) / \\ B T3 / \\ T1 C / \\ T2.L T2.R In this scenario, insertion into subtree T2 makes the node C the focal point of imbalance. Here we have a Left-Right imbalance (B → C), meaning the imbalance goes first left from the root node (A) and then right (C) from node (B). A single rotation does not resolve this imbalance effectively because node C isn’t directly beneath node B on one side; it is diagonal (B left child and then C right child). Hence, a double rotation is required. Steps of LR double rotation: Step 1: Rotate left around node B with its right child C: A / \\ C T3\n",
      "Stored embedding for: / \\ B T2.R / \\ T1 T2.L Step 2: Rotate right around node A with its new left child C: C / \\ B A / \\ / \\ T1 T2.L T2.R T3 Now, the AVL property is restored. Summary of LR double rotation (pseudo code): fun doubleRotateLeftRight(oldRoot, parentOld): rotateLeft(oldRoot.left, parentOld) rotateRight(oldRoot, parentOld) ⸻ STARTING TREE for RR and RL Original Tree Structure: A / \\ T1 B / \\ T2 T3 Again, initially, height(A.left) and height(A.right) differ by at most one. Tree is AVL- balanced at this stage. ⸻ Case 3: Right-Right Imbalance (Single Rotation) Imbalance situation (after inserting into subtree T3):\n",
      "Stored embedding for: A (imbalanced) / \\ T1 B / \\ T2 T3 In this case, insertion in T3 makes the right subtree too deep (Right-Right imbalance), creating imbalance at node A. This is similar to the LL case but mirrored on the right side. Resolving RR imbalance: Perform a single left rotation around node A: • The right child (B) becomes the new subtree root. • Node A moves to become left child of node B. • Subtrees T1 and T2 are repositioned correctly. Result after rotation: B / \\ A T3 / \\ T1 T2 This restores AVL balance. ⸻ Case 4: Right-Left Imbalance (Double Rotation) Original imbalance scenario (after inserting into subtree T2): A (imbalanced) / \\ T1 B / \\ C T3 / \\ T2.L T2.R This is the Right-Left imbalance, similar to the Left-Right imbalance, but mirrored on the right side.\n",
      "Stored embedding for: Steps of RL double rotation: Step 1: Rotate right around node B with its left child C: A / \\ T1 C / \\ T2.L B / \\ T2.R T3 Step 2: Rotate left around node A with its new right child C: C / \\ A B / \\ / \\ T1 T2.L T2.R T3 AVL property is now restored after the double rotation. Summary of RL double rotation (pseudo code): fun doubleRotateRightLeft(oldRoot, parentOld): rotateRight(oldRoot.right, parentOld) rotateLeft(oldRoot, parentOld) ⸻ Summary & Intuition of AVL Rotations: • Single Rotation (LL or RR): • Used when insertion happens on the “outside” subtree (left-left or right-right). • Single rotations shift the imbalanced node down, moving the child node up to root the subtree. • Double Rotation (LR or RL): • Used when insertion happens on the “inside” subtree (left-right or right-left). • Requires two rotations: • First rotation moves the child subtree into a position suitable\n",
      "Stored embedding for: for the second rotation. • Second rotation restores AVL balance by shifting the median node up to subtree root position. ⸻ This detailed expansion helps solidify understanding of AVL tree rotations, clearly differentiating the situations when single and double rotations are needed and illustrating clearly how each rotation corrects the imbalance.\n",
      " -----> Processed STARTING TREE for LL and LR.pdf\n",
      "Stored embedding for: DS 4300 Large Scale Information Storage and Retrieval B+ Tree Walkthrough Mark Fontenot, PhD Northeastern University\n",
      "Stored embedding for: 2 B+ Tree : m = 4 Insert: 42, 21, 63, 89 ●Initially, the ﬁrst node is a leaf node AND root node. ●21, 42, … represent keys of some set of K:V pairs ●Leaf nodes store keys and data, although data not shown ●Inserting another key will cause the node to split.\n",
      "Stored embedding for: 3 B+ Tree : m = 4 Insert: 35 ● Leaf node needs to split to accommodate 35. New leaf node allocated to the right of existing node ● 5/2 values stay in original node; remaining values moved to new node ● Smallest value from new leaf node (42) is copied up to the parent, which needs to be created in this case. It will be an internal node.\n",
      "Stored embedding for: 4 B+ Tree : m = 4 Insert: 10, 27, 96 ● The insert process starts at the root node. The keys of the root node are searched to ﬁnd out which child node we need to descend to. ○ EX: 10. Since 10 < 42, we follow the pointer to the left of 42 ● Note - none of these new values cause a node to split\n",
      "Stored embedding for: 5 B+ Tree : m = 4 Insert: 30 ● Starting at root, we descend to the left-most child (we’ll call curr). ○ curr is a leaf node. Thus, we insert 30 into curr. ○ BUT curr is full. So we have to split. ○ Create a new node to the right of curr, temporarily called newNode. ○ Insert newNode into the doubly linked list of leaf nodes.\n",
      "Stored embedding for: 6 B+ Tree : m = 4 Insert: 30 cont’d. ● re-distribute the keys ● copy the smallest key (27 in this case) from newNode to parent; rearrange keys and pointers in parent node. ● Parent of newNode is also root. So, nothing else to do.\n",
      "Stored embedding for: 7 B+ Tree : m = 4 Fast forward to this state of the tree… ● Observation: The root node is full. ○ The next insertion that splits a leaf will cause the root to split, and thus the tree will get 1 level deeper.\n",
      "Stored embedding for: 8 B+ Tree : m = 4 Insert 37. Step 1\n",
      "Stored embedding for: 9 B+ Tree : m = 4 Insert 37. Step 2. ● When splitting an internal node, we move the middle element to the parent (instead of copying it). ● In this particular tree, that means we have to create a new internal node which is also now the root.\n",
      " -----> Processed 04 - B+Tree Walkthrough.pdf\n",
      "Stored embedding for: DS 4300 Redis in Docker Setup Mark Fontenot, PhD Northeastern University\n",
      "Stored embedding for: Pre-Requisites 2 - You have installed Docker Desktop - You have installed Jetbrains DataGrip\n",
      "Stored embedding for: Step 1 - Find the Redis Image - Open Docker Desktop - Use the Built In search to ﬁnd the Redis Image - Click Run 3\n",
      "Stored embedding for: Step 2 - Conﬁgure & Run the Container - Give the new container a name - Enter 6379 in Host Port ﬁeld - Click Run - Give Docker some time to download and start Redis 4\n",
      "Stored embedding for: Step 3 - Set up Data Source in DataGrip - Start DataGrip - Create a new Redis Data Source - You can use the + in the Database Explorer OR - You can use New from the File Menu 5\n",
      "Stored embedding for: Step 4 - Conﬁgure the Data Source - Give the data source a name - Install Drivers if needed (message above Test Connection) - Test the Connection to Redis - Click OK if connection test was successful 6 There will be a message to install drivers above Test Connection if they aren’t already installed\n",
      " -----> Processed 05b - Redis in Docker.pdf\n",
      "Stored embedding for: Introduction to the Graph Data Model Overview (Text Explanation): • This slide serves as the cover page for a presentation on graph databases and graph theory. • The primary focus is on how graph data structures underpin modern graph databases and why they are useful in representing complex relationships. No image content other than the title slide—just the title, course number, presenter name, and contact information. ⸻ Slide 2: What is a Graph Database? Main Points (Text Explanation): 1. Data model based on the graph data structure • Graph data structures consist of two main components: nodes (sometimes called vertices) and edges (relationships). 2. Composed of nodes and edges • Each node represents an entity (e.g., a person, place, thing). • Each edge represents a relationship (e.g., “owns,” “manages,” “friends with”). 3. Examples of node properties • Properties are key-value pairs associated with nodes. Examples might include: • Name (e.g., “Alice”), • Occupation (e.g., “Software Engineer”), • Location (e.g., “New York City”). 4. Edges define relationships • The edges connect nodes, showing how they are related (e.g., a friendship link, a transaction link, etc.). 5. Relationships can have properties • Edges can also carry their own properties, such as start\n",
      "Stored embedding for: date of a friendship, distance in miles, or cost. Expanded Explanation: • In a graph database, the structure (nodes and edges) is central. This contrasts with traditional relational databases where data is stored in tables, and relationships are defined via foreign keys. Graph databases allow more intuitive modeling of highly interconnected data. • Because edges can have properties, one can store details about the relationship itself (e.g., the strength of a connection, a timestamp, or a weight). Image Description: • Not shown here, but typically, a conceptual diagram might illustrate circles (nodes) connected by lines (edges). Each node can have labels or attributes, and each edge can be labeled with its relationship type. ⸻ Slide 3: Where Do Graphs Show Up? Main Points (Text Explanation): • Social Networks: Platforms like Facebook, Instagram, Twitter, etc. Each user is a node; relationships like “follows” or “friends with” are edges. • Messaging: Communication networks, where phone numbers or user IDs are nodes, and calls or messages form edges. • Airline Route Networks: Airports are nodes, and flights between them are edges. • Chemical and Biological Data: Molecules represented as graphs (atoms are nodes, bonds are edges). Protein interaction networks are also graphs. • Knowledge Graphs: Entities (concepts, objects) and the relationships among them. • Web Graphs: Websites as nodes; hyperlinks as edges. Expanded Explanation: • Graphs are extremely versatile. Anytime there is a set of\n",
      "Stored embedding for: entities and relationships between them, a graph model can be used. • In social networks, the “importance” of a node might represent a user’s influence (related to centrality). In airline networks, analyzing shortest paths can help with route optimization. Image Description: • The slide may show icons for social media platforms, phone icons, airplane icons, chemical structures, etc., each signifying a different domain where graphs are naturally applied. ⸻ Slide 4: Basics of Graphs and Graph Theory Text Explanation: • This slide introduces the core concepts of graph theory, setting up the terminology that will be used throughout the presentation. Expanded Explanation: • Graph theory is a branch of mathematics that studies the properties of graphs (nodes/vertices and edges). • It provides foundational concepts such as paths, cycles, connectivity, degree of nodes, etc. No specific image content is described on this slide, so likely just a heading. ⸻ Slide 5: What is a Graph? Title: Labeled Property Graph Main Points (Text Explanation): 1. Composed of a set of node (vertex) objects and\n",
      "Stored embedding for: relationships (edges) among them. 2. Nodes can be assigned labels to define the group or type they belong to. • Example: A node labeled “Person” might have properties like name, age, occupation. A node labeled “Car” might have properties like model, year, manufacturer. 3. Node properties are attributes (key-value pairs) that define characteristics of that node. 4. Relationships can also have properties (e.g., a “purchased_on” date or “distance” for a road). 5. Edges not connected to nodes are not permitted in a well- formed graph database (i.e., every edge must connect two existing nodes). Expanded Explanation: • In many graph database systems (like Neo4j), the Labeled Property Graph model is standard. • A node can have multiple labels if it fits into multiple categories (e.g., “Person” and “Employee”). • Edges are directional in some graph systems but can also be considered undirected if the relationship is mutual. Image Description (Hypothetical): • Often, such a slide features a simple diagram of nodes labeled with “Person” or “Car,” each with sample properties like “name = Alice,” “model = Tesla,” and an arrow (relationship) labeled “OWNS.” ⸻ Slide 6: Example Text Explanation: • 2 Labels: 1 = Person, 2 = Car • Attributes: name, model, year, etc. • Relationship Types: owns, drives • Edges can have properties.\n",
      "Stored embedding for: Expanded Explanation: • This slide likely shows a small graph with a single “Person” node connected to a “Car” node. • There could be multiple edges (like “owns” and “drives”), each representing a different relationship. • Properties on the edges could be something like “purchase date,” “frequency of use,” or “rental contract terms.” Image Description: • The image typically has two nodes: • A Person node labeled with properties such as Name: Alice. • A Car node labeled with properties such as Model: Honda Civic, Year: 2020. • An arrow labeled “owns” or “drives” connecting the Person node to the Car node. ⸻ Slide 7: Paths Text Explanation: • A path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated. • Example of a valid path: 1 → 2 → 3 • Example of a path that is not valid because it repeats nodes: 1 → 2 → 3 → 2 → 3 Expanded Explanation: • In graph theory, a path that revisits a node or edge is no longer considered a “simple path.” The concept of a simple path requires no repetition of nodes or edges. • Paths are central to many algorithms (like BFS, DFS, and shortest path calculations). Image Description: • Likely a small set of three or four circles (nodes) labeled 1, 2, 3, 4, with arrows or lines showing which nodes are connected. • The invalid path might be shown with a line looping back to\n",
      "Stored embedding for: an earlier node. ⸻ Slide 8: Flavors of Graphs Main Points (Text Explanation): 1. Connected vs. Disconnected • A graph is connected if there is a path between every pair of nodes. If some nodes are isolated or in separate components, the graph is disconnected. 2. Weighted vs. Unweighted • A weighted graph has edges with numerical values (cost, distance, capacity). An unweighted graph does not have such values (or you can consider all edges as having the same weight). 3. Directed vs. Undirected • In a directed graph, edges have a direction (an arrow from one node to another). In an undirected graph, edges have no direction (they are bidirectional). 4. Acyclic vs. Cyclic • An acyclic graph has no loops or cycles. A cyclic graph has at least one cycle. Expanded Explanation: • These properties can be combined. For example, you can have a weighted directed acyclic graph (often called a DAG) which is important in scheduling problems. • Understanding these distinctions helps in selecting the right algorithms for graph analysis. Image Description: • Typically, there are small, separate diagrams showing a connected vs. disconnected graph, a weighted edge (with a number on it) vs. an unweighted edge, an arrow indicating direction for a directed edge, and a circular arrangement of edges indicating a cycle.\n",
      "Stored embedding for: ⸻ Slide 9: Connected vs. Disconnected Text Explanation: • Connected Graph: Every node can be reached from every other node. • Disconnected Graph: At least one node is isolated or only reachable within a subcomponent of the graph. Image Description: • Two separate diagrams: • One shows a single set of nodes all connected by edges (a connected graph). • The other shows at least two clusters of nodes not connected to each other (a disconnected graph). ⸻ Slide 10: Weighted vs. Unweighted Text Explanation: • Weighted Graph: Edges have numerical weights (e.g., distance, cost, capacity). • Unweighted Graph: No edge weights, or all edges can be considered to have the same weight. Expanded Explanation: • Weighted graphs are used in scenarios like shortest path problems where distance or cost matters (e.g., in routing, traveling, network bandwidth). • Unweighted graphs are simpler but still useful in many contexts (e.g., social networks for “friendship” links). Image Description: • Diagram with one set of edges labeled with numbers (e.g., 3, 5, 2) indicating weights, and another diagram with plain edges\n",
      "Stored embedding for: having no labels. ⸻ Slide 11: Directed vs. Undirected Text Explanation: • Directed Graph: Each edge has a direction (e.g., A → B). • Undirected Graph: Edges are bidirectional (e.g., A — B). Expanded Explanation: • Directed edges are useful for representing one-way relationships (like “A follows B,” “A owes B money,” or “A links to B”). • Undirected edges are used for mutual relationships (like “A is friends with B,” “A is connected to B with no inherent direction”). Image Description: • Side-by-side diagrams: one with arrows showing direction, another with simple lines indicating undirected edges. ⸻ Slide 12: Cyclic vs. Acyclic Text Explanation: • Cyclic Graph: A graph that contains at least one cycle (a path that starts and ends on the same node without reusing edges). • Acyclic Graph: A graph with no cycles. Expanded Explanation: • A Directed Acyclic Graph (DAG) is particularly important in modeling dependencies (e.g., task scheduling, version control branching). • Cyclic graphs appear often in transportation networks or circular dependencies.\n",
      "Stored embedding for: Image Description: • One diagram shows a cycle (e.g., 1 → 2 → 3 → 1). • Another diagram shows no cycles (a tree-like structure). ⸻ Slide 13: Sparse vs. Dense Text Explanation: • Sparse Graph: Few edges relative to the number of nodes. • Dense Graph: Many edges; close to the maximum possible number of edges for the given number of nodes. Expanded Explanation: • Sparse graphs are common in large networks where not all nodes are heavily interconnected (e.g., certain social networks, large road networks). • Dense graphs appear in situations where most nodes connect to most other nodes. Image Description: • Two examples: one with only a few edges connecting the nodes, and another where almost every node is connected to almost every other node. ⸻ Slide 14: Flavors of Graphs (Recap) Text Explanation (Recap): • Connected vs. Disconnected • Weighted vs. Unweighted • Directed vs. Undirected • Acyclic vs. Cyclic Expanded Explanation: • This slide likely reiterates the different types of graphs and\n",
      "Stored embedding for: their properties. • These classifications help in choosing algorithms (e.g., BFS for unweighted shortest paths, Dijkstra for weighted, DAG-based dynamic programming for acyclic graphs, etc.). ⸻ Slide 15: Types of Graph Algorithms — Pathfinding Main Points (Text Explanation): • Pathfinding is about finding a route between two nodes (if one exists), often the shortest route. • Common pathfinding algorithms include: • BFS (Breadth-First Search) for unweighted graphs. • DFS (Depth-First Search) for general graph traversal (not always for shortest path). • Dijkstra’s Algorithm for weighted graphs with non-negative weights. • A* for weighted graphs, using heuristics to speed up search. • Other algorithmic topics include cycle detection, max/min flow, and more. Expanded Explanation: • BFS explores neighbors first and is ideal for unweighted shortest path. • DFS goes deep along a path before backtracking; it’s good for checking connectivity or finding cycles but doesn’t guarantee shortest paths in weighted graphs. • Dijkstra systematically finds the shortest path in graphs with non-negative edge weights. • A* uses heuristics (like Euclidean distance in a map) to guide the search more efficiently. Image Description: • Possibly a diagram showing a sample graph, with BFS exploring nodes level by level, and DFS exploring a path fully before\n",
      "Stored embedding for: backtracking. ⸻ Slide 16: BFS vs. DFS Text Explanation: • Compares how Breadth-First Search (BFS) and Depth-First Search (DFS) traverse a graph. • BFS uses a queue and visits neighbors first. • DFS uses a stack (or recursion) and visits as deep as possible before backtracking. Image Description: • Typically, two small diagrams: • BFS: Nodes are visited in layers (all neighbors of the start node, then neighbors of those neighbors, etc.). • DFS: Follows one branch until it can go no further, then backtracks to explore other branches. ⸻ Slide 17: Shortest Path Text Explanation: • Shows an example of finding the shortest path in a graph. • Could be a weighted or unweighted example, highlighting how a path with the minimum sum of weights (or edges) is found. Expanded Explanation: • In an unweighted graph, BFS can be used to find the shortest path in terms of the number of edges. • In a weighted graph, Dijkstra’s algorithm or A* can be used. Image Description: • A sample graph with edge weights and a highlighted path\n",
      "Stored embedding for: from a “start” node to a “goal” node, illustrating the shortest route. ⸻ Slide 18: Types of Graph Algorithms — Centrality & Community Detection Main Points (Text Explanation): 1. Centrality: Determines which nodes are “more important” or “influential” in a network. • Examples: • Degree Centrality: Nodes with the highest number of direct connections. • Betweenness Centrality: Nodes that lie on many shortest paths. • Closeness Centrality: Nodes that have the smallest average distance to all other nodes. 2. Community Detection: Clustering or partitioning nodes of a graph into groups with stronger intra-group connections than inter- group connections. Expanded Explanation: • Centrality metrics help identify key influencers in social networks or critical junctions in a transportation network. • Community detection algorithms (like Modularity-based methods or Louvain method) can uncover subgraphs that are tightly knit. Image Description: • Possibly a network graph highlighting a particular node as being central or showing a cluster of nodes in a distinct color to represent a community. ⸻ Slide 19: Centrality\n",
      "Stored embedding for: Text Explanation: • Further illustration of a centrality concept, possibly showing how a node with high betweenness centrality might act as a “bridge” between communities. Expanded Explanation: • This slide likely has a diagram highlighting one node in the middle, with many paths going through it. • Emphasizes how centrality can identify bottlenecks or critical communication points in a network. Image Description: • A network with a node that visually appears more “important” (e.g., bigger size or a different color), indicating its high centrality score. ⸻ Slide 20: Some Famous Graph Algorithms Main Points (Text Explanation): 1. Dijkstra’s Algorithm • Single-source shortest path in a graph with non-negative edge weights. • Finds the minimum cost (distance) from one node to all others. 2. A* (A-Star) Algorithm • Similar to Dijkstra’s but uses heuristics (like Euclidean distance) to guide the search, often faster when a good heuristic is available. 3. PageRank • Measures the importance of each node within a graph based on incoming edges. • Originally used by Google to rank webpages in search results. • Nodes with more incoming links from “important” nodes are themselves deemed more important.\n",
      "Stored embedding for: Expanded Explanation: • Dijkstra’s is foundational in many routing applications (e.g., driving directions). • A* is widely used in games and pathfinding where you have a spatial or heuristic-based environment. • PageRank is a centrality-like algorithm focusing on link analysis. Many variations exist for different types of networks. Image Description: • Possibly a comparison chart or a diagram showing a shortest path solution and a conceptual representation of PageRank (arrows with different thicknesses representing link importance). ⸻ Conclusion This expanded outline covers each slide’s main ideas and includes additional clarifications about graph concepts, common algorithms, and typical applications. Where the slides show diagrams, the textual descriptions provide an equivalent explanation of what those diagrams likely represent (e.g., nodes, edges, directions, weights, paths, and algorithmic outcomes). By understanding the Labeled Property Graph model, the different flavors of graphs (connected, weighted, directed, acyclic, etc.), and fundamental graph algorithms (BFS, DFS, Dijkstra, A*, PageRank, centrality measures), one can see how graph databases and graph theory form the backbone of many modern data systems and analytical workflows.\n",
      " -----> Processed Introduction to the Graph Data Model.pdf\n",
      "Stored embedding for: DS 4300 Replicating Data Mark Fontenot, PhD Northeastern University Some material used with permission from Dr. Rachlin, with thanks!\n",
      "Stored embedding for: Distributing Data - Beneﬁts 2 - Scalability / High throughput: Data volume or Read/Write load grows beyond the capacity of a single machine - Fault Tolerance / High Availability: Your application needs to continue working even if one or more machines goes down. - Latency: When you have users in different parts of the world you want to give them fast performance too\n",
      "Stored embedding for: Distributed Data - Challenges - Consistency: Updates must be propagated across the network. - Application Complexity: Responsibility for reading and writing data in a distributed environment often falls to the application. 3\n",
      "Stored embedding for: Vertical Scaling - Shared Memory Architectures - Geographically Centralized server - Some fault tolerance (via hot-swappable components) 4\n",
      "Stored embedding for: Vertical Scaling - Shared Disk Architectures - Machines are connected via a fast network - Contention and the overhead of locking limit scalability (high-write volumes) … BUT ok for Data Warehouse applications (high read volumes) 5\n",
      "Stored embedding for: AWS EC2 Pricing - Oct 2024 6 > $78,000/month https://aws.amazon.com/ec2/pricing/on-demand/\n",
      "Stored embedding for: Horizontal Scaling - Shared Nothing Architectures ●Each node has its own CPU, memory, and disk ●Coordination via application layer using conventional network ●Geographically distributed ●Commodity hardware 7\n",
      "Stored embedding for: Data - Replication vs Partitioning 8 Replicates have same data as Main Partitions have a subset of the data\n",
      "Stored embedding for: Replication 9\n",
      "Stored embedding for: Common Strategies for Replication - Single leader model - Multiple leader model - Leaderless model Distributed databases usually adopt one of these strategies. 10\n",
      "Stored embedding for: Leader-Based Replication - All writes from clients go to the leader - Leader sends replication info to the followers - Followers process the instructions from the leader - Clients can read from either the leader or followers 11\n",
      "Stored embedding for: Leader-Based Replication 12 This write could NOT be sent to one of the followers… only the leader.\n",
      "Stored embedding for: Leader-Based Replication - Very Common Strategy Relational: ● MySQL, ● Oracle, ● SQL Server, ● PostgreSQL NoSQL: ● MongoDB, ● RethinkDB (realtime web apps), ● Espresso (LinkedIn) Messaging Brokers: Kafka, RabbitMQ 13\n",
      "Stored embedding for: How Is Replication Info Transmitted to Followers? 14 Replication Method Description Statement-based Send INSERT, UPDATE, DELETEs to replica. Simple but error-prone due to non-deterministic functions like now(), trigger side-effects, and difﬁculty in handling concurrent transactions. Write-ahead Log (WAL) A byte-level speciﬁc log of every change to the database. Leader and all followers must implement the same storage engine and makes upgrades difﬁcult. Logical (row-based) Log For relational DBs: Inserted rows, modiﬁed rows (before and after), deleted rows. A transaction log will identify all the rows that changed in each transaction and how they changed. Logical logs are decoupled from the storage engine and easier to parse. Trigger-based Changes are logged to a separate table whenever a trigger ﬁres in response to an insert, update, or delete. Flexible because you can have application speciﬁc replication, but also more error prone.\n",
      "Stored embedding for: Synchronous vs Asynchronous Replication Synchronous: Leader waits for a response from the follower Asynchronous: Leader doesn’t wait for conﬁrmation. 15 Synchronous: Asynchronous:\n",
      "Stored embedding for: What Happens When the Leader Fails? Challenges: How do we pick a new Leader Node? ●Consensus strategy – perhaps based on who has the most updates? ●Use a controller node to appoint new leader? AND… how do we conﬁgure clients to start writing to the new leader? 16\n",
      "Stored embedding for: What Happens When the Leader Fails? More Challenges: ● If asynchronous replication is used, new leader may not have all the writes How do we recover the lost writes? Or do we simply discard? ● After (if?) the old leader recovers, how do we avoid having multiple leaders receiving conﬂicting data? (Split brain: no way to resolve conﬂicting requests. ● Leader failure detection. Optimal timeout is tricky. 17\n",
      "Stored embedding for: Replication Lag refers to the time it takes for writes on the leader to be reﬂected on all of the followers. ●Synchronous replication: Replication lag causes writes to be slower and the system to be more brittle as num followers increases. ●Asynchronous replication: We maintain availability but at the cost of delayed or eventual consistency. This delay is called the inconsistency window. Replication Lag 18\n",
      "Stored embedding for: Read-after-Write Consistency Scenario - you’re adding a comment to a Reddit post… after you click Submit and are back at the main post, your comment should show up for you. - Less important for other users to see your comment as immediately. 19\n",
      "Stored embedding for: Implementing Read-After-Write Consistency Method 1: Modiﬁable data (from the client’s perspective) is always read from the leader. 20\n",
      "Stored embedding for: Implementing Read-After-Write Consistency Method 2: Dynamically switch to reading from leader for “recently updated” data. - For example, have a policy that all requests within one minute of last update come from leader. 21\n",
      "Stored embedding for: But… This Can Create Its Own Challenges 22 We created followers so they would be proximal to users. BUT… now we have to route requests to distant leaders when reading modiﬁable data?? :(\n",
      "Stored embedding for: Monotonic Read Consistency Monotonic read anomalies: occur when a user reads values out of order from multiple followers. Monotonic read consistency: ensures that when a user makes multiple reads, they will not read older data after previously reading newer data. 23\n",
      "Stored embedding for: Consistent Preﬁx Reads Reading data out of order can occur if different partitions replicate data at different rates. There is no global write consistency. Consistent Preﬁx Read Guarantee - ensures that if a sequence of writes happens in a certain order, anyone reading those writes will see them appear in the same order. 24 A B How far into the future can you see, Ms. B? About 10 seconds usually, Mr A.\n",
      "Stored embedding for: ?? 25\n",
      " -----> Processed 04 - Data Replication.pdf\n",
      "Stored embedding for: DS 4300 Introduction to the Graph Data Model Mark Fontenot, PhD Northeastern University Material referenced from Graph Algorithms - Practical Examples in Apache Spark and Neo4j by Needham and Hodler (O’Reilly Press, 2019)\n",
      "Stored embedding for: What is a Graph Database - Data model based on the graph data structure - Composed of nodes and edges - edges connect nodes - each is uniquely identiﬁed - each can contain properties (e.g. name, occupation, etc) - supports queries based on graph-oriented operations - traversals - shortest path - lots of others 2\n",
      "Stored embedding for: Where do Graphs Show up? - Social Networks - yes… things like Instagram, - but also… modeling social interactions in ﬁelds like psychology and sociology - The Web - it is just a big graph of “pages” (nodes) connected by hyperlinks (edges) - Chemical and biological data - systems biology, genetics, etc. - interaction relationships in chemistry 3\n",
      "Stored embedding for: Basics of Graphs and Graph Theory 4\n",
      "Stored embedding for: What is a graph? Labeled Property Graph - Composed of a set of node (vertex) objects and relationship (edge) objects - Labels are used to mark a node as part of a group - Properties are attributes (think KV pairs) and can exist on nodes and relationships - Nodes with no associated relationships are OK. Edges not connected to nodes are not permitted. 5\n",
      "Stored embedding for: Example 2 Labels: - person - car 4 relationship types: - Drives - Owns - Lives_with - Married_to Properties 6\n",
      "Stored embedding for: Paths A path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated. 7 1 2 3 6 5 4 Ex: 1 → 2 → 6 → 5 Not a path: 1 → 2 → 6 → 2 → 3\n",
      "Stored embedding for: Flavors of Graphs Connected (vs. Disconnected) – there is a path between any two nodes in the graph Weighted (vs. Unweighted) – edge has a weight property (important for some algorithms) Directed (vs. Undirected) – relationships (edges) deﬁne a start and end node Acyclic (vs. Cyclic) – Graph contains no cycles 8\n",
      "Stored embedding for: Connected vs. Disconnected 9\n",
      "Stored embedding for: Weighted vs. Unweighted 10\n",
      "Stored embedding for: Directed vs. Undirected 11\n",
      "Stored embedding for: Cyclic vs Acyclic 12\n",
      "Stored embedding for: Sparse vs. Dense 13\n",
      "Stored embedding for: Trees 14\n",
      "Stored embedding for: Types of Graph Algorithms - Pathﬁnding - Pathﬁnding - ﬁnding the shortest path between two nodes, if one exists, is probably the most common operation - “shortest” means fewest edges or lowest weight - Average Shortest Path can be used to monitor efﬁciency and resiliency of networks. - Minimum spanning tree, cycle detection, max/min ﬂow… are other types of pathﬁnding 15\n",
      "Stored embedding for: BFS vs DFS 16\n",
      "Stored embedding for: Shortest Path 17\n",
      "Stored embedding for: Types of Graph Algorithms - Centrality & Community Detection - Centrality - determining which nodes are “more important” in a network compared to other nodes - EX: Social Network Inﬂuencers? - Community Detection - evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18\n",
      "Stored embedding for: Centrality 19\n",
      "Stored embedding for: Some Famous Graph Algorithms - Dijkstra’s Algorithm - single-source shortest path algo for positively weighted graphs - A* Algorithm - Similar to Dijkstra’s with added feature of using a heuristic to guide traversal - PageRank - measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20\n",
      "Stored embedding for: Neo4j - A Graph Database System that supports both transactional and analytical processing of graph-based data - Relatively new class of no-sql DBs - Considered schema optional (one can be imposed) - Supports various types of indexing - ACID compliant - Supports distributed computing - Similar: Microsoft CosmoDB, Amazon Neptune 21\n",
      "Stored embedding for: ?? 22\n",
      " -----> Processed 09 - Introduction to Graph Data Model.pdf\n",
      "Stored embedding for: DS 4300 Large Scale Information Storage and Retrieval Foundations Mark Fontenot, PhD Northeastern University\n",
      "Stored embedding for: Searching ●Searching is the most common operation performed by a database system ●In SQL, the SELECT statement is arguably the most versatile / complex. ●Baseline for efﬁciency is Linear Search ○ Start at the beginning of a list and proceed element by element until: ■ You ﬁnd what you’re looking for ■ You get to the last element and haven’t found it 2\n",
      "Stored embedding for: Searching ●Record - A collection of values for attributes of a single entity instance; a row of a table ●Collection - a set of records of the same entity type; a table ○ Trivially, stored in some sequential order like a list ●Search Key - A value for an attribute from the entity type ○ Could be >= 1 attribute 3\n",
      "Stored embedding for: Lists of Records ●If each record takes up x bytes of memory, then for n records, we need n*x bytes of memory. ●Contiguously Allocated List ○ All n*x bytes are allocated as a single “chunk” of memory ●Linked List ○ Each record needs x bytes + additional space for 1 or 2 memory addresses ○ Individual records are linked together in a type of chain using memory addresses 4\n",
      "Stored embedding for: Contiguous vs Linked 5 6 Records Contiguously Allocated - Array front back 6 Records Linked by memory addresses - Linked List Extra storage for a memory address\n",
      "Stored embedding for: Pros and Cons ●Arrays are faster for random access, but slow for inserting anywhere but the end ●Linked Lists are faster for inserting anywhere in the list, but slower for random access 6 Insert after 2nd record records: records: 5 records had to be moved to make space Insert after 2nd record\n",
      "Stored embedding for: Observations: - Arrays - fast for random access - slow for random insertions - Linked Lists - slow for random access - fast for random insertions 7\n",
      "Stored embedding for: Binary Search ● Input: array of values in sorted order, target value ● Output: the location (index) of where target is located or some value indicating target was not found def binary_search(arr, target) left, right = 0, len(arr) - 1 while left <= right: mid = (left + right) // 2 if arr[mid] == target: return mid elif arr[mid] < target: left = mid + 1 else: right = mid - 1 return -1 8 A C G M P R Z target = A mid Since target < arr[mid], we reset right to mid - 1. left right A C G M P R Z target = A mid left right\n",
      "Stored embedding for: Time Complexity ●Linear Search ○ Best case: target is found at the ﬁrst element; only 1 comparison ○ Worst case: target is not in the array; n comparisons ○ Therefore, in the worst case, linear search is O(n) time complexity. ●Binary Search ○ Best case: target is found at mid; 1 comparison (inside the loop) ○ Worst case: target is not in the array; log2 n comparisons ○ Therefore, in the worst case, binary search is O(log2n) time complexity. 9\n",
      "Stored embedding for: Back to Database Searching ● Assume data is stored on disk by column id’s value ● Searching for a speciﬁc id = fast. ● But what if we want to search for a speciﬁc specialVal? ○ Only option is linear scan of that column ● Can’t store data on disk sorted by both id and specialVal (at the same time) ○ data would have to be duplicated → space inefﬁcient 10\n",
      "Stored embedding for: Back to Database Searching ● Assume data is stored on disk by column id’s value ● Searching for a speciﬁc id = fast. ● But what if we want to search for a speciﬁc specialVal? ○ Only option is linear scan of that column ● Can’t store data on disk sorted by both id and specialVal (at the same time) ○ data would have to be duplicated → space inefﬁcient 11 We need an external data structure to support faster searching by specialVal than a linear scan.\n",
      "Stored embedding for: What do we have in our arsenal? 1) An array of tuples (specialVal, rowNumber) sorted by specialVal a) We could use Binary Search to quickly locate a particular specialVal and ﬁnd its corresponding row in the table b) But, every insert into the table would be like inserting into a sorted array - slow… 2) A linked list of tuples (specialVal, rowNumber) sorted by specialVal a) searching for a specialVal would be slow - linear scan required b) But inserting into the table would theoretically be quick to also add to the list. 12\n",
      "Stored embedding for: Something with Fast Insert and Fast Search? - Binary Search Tree - a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent. 13 Image from: https://courses.grainger.illinois.edu/cs225/sp2019/notes/bst/\n",
      "Stored embedding for: To the Board! 14\n",
      " -----> Processed 02 - Foundations.pdf\n",
      "Stored embedding for: DS 4300 Neo4j Mark Fontenot, PhD Northeastern University Material referenced from Graph Algorithms - Practical Examples in Apache Spark and Neo4j by Needham and Hodler (O’Reilly Press, 2019)\n",
      "Stored embedding for: Neo4j - A Graph Database System that supports both transactional and analytical processing of graph-based data - Relatively new class of no-sql DBs - Considered schema optional (one can be imposed) - Supports various types of indexing - ACID compliant - Supports distributed computing - Similar: Microsoft CosmoDB, Amazon Neptune 2\n",
      "Stored embedding for: Neo4j - Query Language and Plugins - Cypher - Neo4j’s graph query language created in 2011 - Goal: SQL-equivalent language for graph databases - Provides a visual way of matching patterns and relationships (nodes)-[:CONNECT_TO]->(otherNodes) - APOC Plugin - Awesome Procedures on Cypher - Add-on library that provides hundreds of procedures and functions - Graph Data Science Plugin - provides efﬁcient implementations of common graph algorithms (like the ones we talked about yesterday) 3\n",
      "Stored embedding for: Neo4j in Docker Compose 4\n",
      "Stored embedding for: Docker Compose 5 ●Supports multi-container management. ●Set-up is declarative - using YAML docker-compose.yaml ﬁle ○ services ○ volumes ○ networks, etc. ●1 command can be used to start, stop, or scale a number of services at one time. ●Provides a consistent method for producing an identical environment (no more “well… it works on my machine!) ●Interaction is mostly via command line\n",
      "Stored embedding for: docker-compose.yaml 6 services: neo4j: container_name: neo4j image: neo4j:latest ports: - 7474:7474 - 7687:7687 environment: - NEO4J_AUTH=neo4j/${NEO4J_PASSWORD} - NEO4J_apoc_export_file_enabled=true - NEO4J_apoc_import_file_enabled=true - NEO4J_apoc_import_file_use__neo4j__config=true - NEO4J_PLUGINS=[\"apoc\", \"graph-data-science\"] volumes: - ./neo4j_db/data:/data - ./neo4j_db/logs:/logs - ./neo4j_db/import:/var/lib/neo4j/import - ./neo4j_db/plugins:/plugins Never put “secrets” in a docker compose ﬁle. Use .env ﬁles.\n",
      "Stored embedding for: .env Files - .env ﬁles - stores a collection of environment variables - good way to keep environment variables for different platforms separate - .env.local - .env.dev - .env.prod 7 NEO4J_PASSWORD=abc123!!! .env file\n",
      "Stored embedding for: Docker Compose Commands ●To test if you have Docker CLI properly installed, run: docker --version ●Major Docker Commands ○ docker compose up ○ docker compose up -d ○ docker compose down ○ docker compose start ○ docker compose stop ○ docker compose build ○ docker compose build --no-cache 8\n",
      "Stored embedding for: localhost:7474 9\n",
      "Stored embedding for: Neo4j Browser 10 https://neo4j.com/docs/browser-manual/current/visual-tour/ localhost:7474 Then login.\n",
      "Stored embedding for: Inserting Data by Creating Nodes CREATE (:User {name: \"Alice\", birthPlace: \"Paris\"}) CREATE (:User {name: \"Bob\", birthPlace: \"London\"}) CREATE (:User {name: \"Carol\", birthPlace: \"London\"}) CREATE (:User {name: \"Dave\", birthPlace: \"London\"}) CREATE (:User {name: \"Eve\", birthPlace: \"Rome\"}) 11\n",
      "Stored embedding for: Adding an Edge with No Variable Names CREATE (:User {name: \"Alice\", birthPlace: \"Paris\"}) CREATE (:User {name: \"Bob\", birthPlace: \"London\"}) MATCH (alice:User {name:”Alice”}) MATCH (bob:User {name: “Bob”}) CREATE (alice)-[:KNOWS {since: “2022-12-01”}]->(bob) 12 Note: Relationships are directed in neo4j.\n",
      "Stored embedding for: Matching Which users were born in London? MATCH (usr:User {birthPlace: “London”}) RETURN usr.name, usr.birthPlace 13\n",
      "Stored embedding for: Download Dataset and Move to Import Folder Clone this repo: https://github.com/PacktPublishing/Graph-Data-Science-with-Neo4j In Chapter02/data of data repo, unzip the netﬂix.zip ﬁle Copy netﬂix_titles.csv into the following folder where you put your docker compose ﬁle neo4j_db/neo4j_db/import 14\n",
      "Stored embedding for: Importing Data 15\n",
      "Stored embedding for: Basic Data Importing LOAD CSV WITH HEADERS FROM 'file:///netflix_titles.csv' AS line CREATE(:Movie { id: line.show_id, title: line.title, releaseYear: line.release_year } ) 16 Type the following into the Cypher Editor in Neo4j Browser\n",
      "Stored embedding for: Loading CSVs - General Syntax LOAD CSV [WITH HEADERS] FROM 'file:///file_in_import_folder.csv' AS line [FIELDTERMINATOR ','] // do stuffs with 'line' 17\n",
      "Stored embedding for: Importing with Directors this Time LOAD CSV WITH HEADERS FROM 'file:///netflix_titles.csv' AS line WITH split(line.director, \",\") as directors_list UNWIND directors_list AS director_name CREATE (:Person {name: trim(director_name)}) But this generates duplicate Person nodes (a director can direct more than 1 movie) 18\n",
      "Stored embedding for: Importing with Directors Merged MATCH (p:Person) DELETE p LOAD CSV WITH HEADERS FROM 'file:///netflix_titles.csv' AS line WITH split(line.director, \",\") as directors_list UNWIND directors_list AS director_name MERGE (:Person {name: director_name}) 19\n",
      "Stored embedding for: Adding Edges LOAD CSV WITH HEADERS FROM 'file:///netflix_titles.csv' AS line MATCH (m:Movie {id: line.show_id}) WITH m, split(line.director, \",\") as directors_list UNWIND directors_list AS director_name MATCH (p:Person {name: director_name}) CREATE (p)-[:DIRECTED]->(m) 20\n",
      "Stored embedding for: Gut Check Let’s check the movie titled Ray: MATCH (m:Movie {title: \"Ray\"})<-[:DIRECTED]-(p:Person) RETURN m, p 21\n",
      "Stored embedding for: ?? 22\n",
      " -----> Processed 10 - Neo4j.pdf\n",
      "Stored embedding for: Replicating Data Distributing Data – Benefits: - **Scalability & High Throughput**: - Distributed databases handle significantly more data and increased read/write loads than a single machine could manage effectively. - **Fault Tolerance & High Availability**: - Ensures continuous operation even if one or more nodes fail, thus increasing the reliability of the overall system. - **Latency Reduction**: - Data replication allows for geographically distributed nodes, enabling users worldwide to experience low-latency performance by accessing data from servers closest to them. --- Distributed Data – Challenges: - **Consistency**: - Data updates must propagate across multiple nodes in a network. Ensuring all nodes remain synchronized in a distributed environment is complex. - **Application Complexity**: - Distributed systems require applications to handle intricacies of managing read and write operations across multiple nodes, often transferring this complexity to the application logic itself. --- Vertical Scaling – Shared Memory Architectures: - Features a centralized server location. - Limited fault tolerance via \"hot-swappable\" components allowing parts to be replaced without downtime. - All CPUs have direct access to a shared memory pool, simplifying data management. --- Vertical Scaling – Shared Disk Architectures: - Multiple machines are interconnected through a fast network, accessing a\n",
      "Stored embedding for: shared disk storage. - While enabling good read scalability, it encounters write scalability limits due to contention and locking mechanisms. - Suitable for data warehouse systems that primarily read rather than write. --- AWS EC2 Pricing – Oct 2024 (Example): - Illustrates the high costs associated with vertical scaling approaches, emphasizing potential monthly costs of over $78,000 for very powerful instance types. - Serves as motivation for considering alternative, more cost-effective scaling methods. --- Horizontal Scaling – Shared Nothing Architectures: - Each node independently manages its CPU, memory, and storage without shared resources. - Application-level coordination manages communication and data synchronization between nodes. - Geographical distribution is common, leveraging commodity hardware to keep costs manageable and scalability high. --- Data Replication vs. Partitioning: - **Replication** involves copying and maintaining database copies across multiple nodes, each replica maintaining an identical dataset. - **Partitioning** (sharding) splits data into different subsets, each stored on separate nodes. Each node manages a unique subset of the data. --- Replication: - Essential for improving availability and fault tolerance. If one node goes offline, other replicas continue serving data seamlessly. ---\n",
      "Stored embedding for: Common Replication Strategies: - **Single Leader**: - One node handles all writes; replication streams updates to follower nodes. - **Multiple Leader**: - Multiple nodes accept writes independently, potentially requiring complex synchronization. - **Leaderless**: - No designated leader; every node can handle read and write operations, with consistency managed via consensus algorithms. --- Leader-Based Replication: - All client writes are directed to the leader node, which then replicates these updates to follower nodes. - Followers process these instructions, maintaining synchronization. - Clients perform read operations from either leader or followers, optimizing load distribution and response times. --- Replication Methods: - **Statement-based**: Replicates SQL commands (INSERT, UPDATE, DELETE). - **Write-ahead Log (WAL)**: Logs every database change as low-level byte streams. - **Logical (row-based)**: Replicates row-level changes explicitly. - **Trigger-based**: Uses database triggers to initiate replication operations. --- Synchronous vs Asynchronous Replication: - **Synchronous**: - Leader waits for follower acknowledgment before completing the write, ensuring high consistency but sacrificing performance and latency. - **Asynchronous**: - Leader completes write immediately without waiting for followers, prioritizing\n",
      "Stored embedding for: speed and performance, but introducing potential lag (\"inconsistency window\"). --- Leader Failure Challenges: - Requires mechanisms to quickly and reliably elect a new leader. - Consensus strategies or controller nodes manage new leader selection. - Complexity arises when handling data consistency and write losses, particularly with asynchronous replication. - Risk of \"split-brain\" conditions, where multiple nodes mistakenly believe they're leaders. --- Replication Lag: - The time delay before writes on the leader propagate to followers. - Increased lag with synchronous replication makes the system slower as followers increase. - Asynchronous replication provides better performance but sacrifices immediate consistency, creating an \"inconsistency window.\" --- Read-after-Write Consistency: - Ensures a user sees their updates immediately after committing changes (common requirement in user-facing applications). Methods: - **Always read from the leader** for modifiable data. - **Dynamic reads**: Temporarily route reads to leader immediately after writes for consistency. --- Challenges of Read-after-Write Consistency: - Routing reads to distant leaders undermines geographic latency advantages originally gained from follower nodes. ---\n",
      "Stored embedding for: Monotonic Read Consistency: - Avoids anomalies where users see older data after previously viewing updated data. - Ensures sequential consistency in multiple reads from distributed followers. --- Consistent Prefix Reads: - Guarantees that sequences of writes seen by users maintain the same order they were originally committed, despite replication delays across distributed systems. - Important to maintain logical consistency from the user's perspective, even if full global consistency isn't achievable.\n",
      " -----> Processed Replicating Data.pdf\n",
      "Stored embedding for: MongoDB Query Language Documentation This document provides an overview of various query operations in MongoDB, including querying documents, embedded documents, arrays, projections, handling null/missing fields, query timeouts, cursor iteration, and snapshot queries. Examples are based on using the PyMongo Python driver and the MongoDB Atlas UI, but the concepts apply broadly to other drivers and interfaces. ⸻ 1. Querying Documents In MongoDB, you can retrieve documents from a collection by specifying a query predicate. An empty predicate ({}) returns all documents. 1.1 Select All Documents cursor = db.inventory.find({}) Equivalent SQL: SELECT * FROM inventory 1.2 Equality Conditions Specify conditions using key-value pairs. cursor = db.inventory.find({\"status\": \"D\"}) Equivalent SQL: SELECT * FROM inventory WHERE status = \"D\" 1.3 Query Operators\n",
      "Stored embedding for: Using $in Operator Select documents where status is either \"A\" or \"D\". cursor = db.inventory.find({\"status\": {\"$in\": [\"A\", \"D\"]}}) Equivalent SQL: SELECT * FROM inventory WHERE status IN (\"A\", \"D\") Combining Conditions with AND MongoDB implicitly uses an AND between multiple field conditions. cursor = db.inventory.find({\"status\": \"A\", \"qty\": {\"$lt\": 30}}) Equivalent SQL: SELECT * FROM inventory WHERE status = \"A\" AND qty < 30 Combining Conditions with OR Use the $or operator to match documents that satisfy at least one condition. cursor = db.inventory.find({ \"$or\": [{\"status\": \"A\"}, {\"qty\": {\"$lt\": 30}}] }) Equivalent SQL: SELECT * FROM inventory WHERE status = \"A\" OR qty < 30 AND as well as OR Conditions\n",
      "Stored embedding for: Mixing both implicit AND and explicit OR conditions: cursor = db.inventory.find({ \"status\": \"A\", \"$or\": [ {\"qty\": {\"$lt\": 30}}, {\"item\": {\"$regex\": \"^p\"}} ] }) Equivalent SQL: SELECT * FROM inventory WHERE status = \"A\" AND (qty < 30 OR item LIKE \"p%\") ⸻ 2. Querying Embedded/Nested Documents MongoDB allows querying of documents embedded within documents. Use dot notation to reference nested fields. 2.1 Query on a Nested Field Equality Match cursor = db.inventory.find({\"size.uom\": \"in\"}) Using Query Operators For example, query nested field h in size with a comparison operator: cursor = db.inventory.find({\"size.h\": {\"$lt\": 15}})\n",
      "Stored embedding for: Compound Conditions on Nested Fields cursor = db.inventory.find({ \"size.h\": {\"$lt\": 15}, \"size.uom\": \"in\", \"status\": \"D\" }) Matching an Entire Embedded Document To match a document exactly (including field order), use an ordered structure like bson.son.SON: from bson.son import SON cursor = db.inventory.find({ \"size\": SON([(\"h\", 14), (\"w\", 21), (\"uom\", \"cm\")]) }) Warning: Comparisons on embedded documents require an exact match—including field order. Reordering the keys will not match. ⸻ 3. Querying Arrays MongoDB provides several methods for querying array fields. 3.1 Matching an Entire Array cursor = db.inventory.find({\"tags\": [\"red\", \"blank\"]}) This finds documents where the tags array is exactly [\"red\", \"blank\"] in that order. 3.2 Matching Array Elements\n",
      "Stored embedding for: Querying for a Single Element cursor = db.inventory.find({\"tags\": \"red\"}) This returns documents where \"red\" is one of the elements in the tags array. Using $all Operator To match arrays that contain all specified elements, regardless of order: cursor = db.inventory.find({\"tags\": {\"$all\": [\"red\", \"blank\"]}}) Querying with Comparison Operators For example, query an array field dim_cm for elements greater than a given value: cursor = db.inventory.find({\"dim_cm\": {\"$gt\": 25}}) 3.3 Compound Conditions on Array Elements Without $elemMatch The following matches documents where any element meets one condition and some (possibly different) element meets another: cursor = db.inventory.find({\"dim_cm\": {\"$gt\": 15, \"$lt\": 20}}) With $elemMatch To ensure a single array element satisfies all conditions: cursor = db.inventory.find({ \"dim_cm\": {\"$elemMatch\": {\"$gt\": 22, \"$lt\": 30}}\n",
      "Stored embedding for: }) Array Index Query You can target an element at a specific index (zero-based indexing): cursor = db.inventory.find({\"dim_cm.1\": {\"$gt\": 25}}) Query by Array Length Use the $size operator to match arrays with a specific number of elements: cursor = db.inventory.find({\"tags\": {\"$size\": 3}}) ⸻ 4. Querying Arrays of Embedded Documents When your array contains documents, you can query on specific fields within those embedded documents. 4.1 Matching an Entire Embedded Document from bson.son import SON cursor = db.inventory.find({ \"instock\": SON([(\"warehouse\", \"A\"), (\"qty\", 5)]) }) Note: The order of fields matters. 4.2 Query Conditions on Fields Within Array Documents Using Dot Notation\n",
      "Stored embedding for: Query for documents where at least one embedded document has qty less than or equal to 20: cursor = db.inventory.find({\"instock.qty\": {\"$lte\": 20}}) Using an Array Index Target the first element of the array: cursor = db.inventory.find({\"instock.0.qty\": {\"$lte\": 20}}) Compound Conditions with $elemMatch To require that one embedded document matches multiple criteria: cursor = db.inventory.find({ \"instock\": {\"$elemMatch\": {\"qty\": 5, \"warehouse\": \"A\"}} }) Another example with range conditions: cursor = db.inventory.find({ \"instock\": {\"$elemMatch\": {\"qty\": {\"$gt\": 10, \"$lte\": 20}}} }) Combining Separate Conditions The following matches documents where some document in the array has qty equal to 5 and another (or the same) document has warehouse equal to “A”: cursor = db.inventory.find({ \"instock.qty\": 5, \"instock.warehouse\": \"A\" })\n",
      "Stored embedding for: ⸻ 5. Projection: Returning Specific Fields By default, MongoDB returns all fields in matching documents. You can specify a projection document to include or exclude specific fields. 5.1 Including Specific Fields To return only the item and status fields (plus the default _id): cursor = db.inventory.find({\"status\": \"A\"}, {\"item\": 1, \"status\": 1}) Equivalent SQL: SELECT _id, item, status FROM inventory WHERE status = \"A\" 5.2 Excluding the _id Field cursor = db.inventory.find({\"status\": \"A\"}, {\"item\": 1, \"status\": 1, \"_id\": 0}) Equivalent SQL: SELECT item, status FROM inventory WHERE status = \"A\" 5.3 Excluding Specific Fields To exclude status and instock fields: cursor = db.inventory.find({\"status\": \"A\"}, {\"status\": 0, \"instock\": 0}) Note: Except for _id, you cannot mix inclusion and exclusion in the\n",
      "Stored embedding for: same projection. 5.4 Projecting Fields in Embedded Documents Return only the uom field within the embedded size document: cursor = db.inventory.find({\"status\": \"A\"}, {\"item\": 1, \"status\": 1, \"size.uom\": 1}) Alternatively, using nested projection syntax: cursor = db.inventory.find({\"status\": \"A\"}, {\"item\": 1, \"status\": 1, \"size\": {\"uom\": 1}}) 5.5 Projection on Arrays Projecting Specific Fields from Array Elements For example, return only the qty field from documents embedded in the instock array: cursor = db.inventory.find({\"status\": \"A\"}, {\"item\": 1, \"status\": 1, \"instock.qty\": 1}) Using Projection Operators for Arrays To return the last element in the instock array using $slice: cursor = db.inventory.find( {\"status\": \"A\"}, {\"item\": 1, \"status\": 1, \"instock\": {\"$slice\": -1}} ) Projection with Aggregation Expressions You can even use aggregation expressions to modify returned\n",
      "Stored embedding for: fields. For example, overriding status and creating new fields: db.inventory.find( { }, { _id: 0, item: 1, status: { $switch: { branches: [ { case: { $eq: [ \"$status\", \"A\" ] }, then: \"Available\" }, { case: { $eq: [ \"$status\", \"D\" ] }, then: \"Discontinued\" } ], default: \"No status found\" } }, area: { $concat: [ { $toString: { $multiply: [ \"$size.h\", \"$size.w\" ] } }, \" \", \"$size.uom\" ] }, reportNumber: { $literal: 1 } } ) This returns documents with computed fields such as area and a new reportNumber. ⸻ 6. Querying for Null or Missing Fields MongoDB treats null values in queries differently depending on the operator used.\n",
      "Stored embedding for: 6.1 Using Equality Filter Using { field: None } (or null in MongoDB shell) returns documents where the field is either null or does not exist. cursor = db.inventory.find({\"item\": None}) 6.2 Non-Equality Filter To query for fields that exist and are not null, use the $ne operator: cursor = db.inventory.find({\"item\": {\"$ne\": None}}) 6.3 Type Check Use $type to match documents where the field is exactly BSON type Null (type 10): cursor = db.inventory.find({\"item\": {\"$type\": 10}}) 6.4 Existence Check To match documents that do not contain a field: cursor = db.inventory.find({\"item\": {\"$exists\": False}}) ⸻ 7. Query Timeouts You can set a timeout to prevent long-running queries from affecting system performance.\n",
      "Stored embedding for: 7.1 Setting a Query Time Limit Use the maxTimeMS() method to specify a query timeout in milliseconds. If the query exceeds this time limit, MongoDB stops the query and no results are returned. Alternatively, you can set a global default timeout using the defaultMaxTimeMS cluster parameter. ⸻ 8. Iterating a Cursor The find() method returns a cursor—not an array of documents. You must iterate the cursor to access documents. 8.1 Manual Iteration Assign the cursor to a variable to prevent automatic iteration (mongosh prints the first 20 documents by default): var myCursor = db.users.find({ type: 2 }); while (myCursor.hasNext()) { printjson(myCursor.next()); } 8.2 Using forEach() var myCursor = db.users.find({ type: 2 }); myCursor.forEach(printjson); 8.3 Converting Cursor to an Array You can convert the cursor to an array using the toArray() method: var documentArray = db.inventory.find({ type: 2 }).toArray();\n",
      "Stored embedding for: var myDocument = documentArray[3]; Note: The toArray() method loads all documents into memory. 8.4 Accessing by Index Some drivers allow you to use array index notation directly on the cursor (this internally calls toArray()): var myDocument = db.users.find({ type: 2 })[1]; ⸻ 9. Cursor Behaviors 9.1 Cursors Opened Within a Session Starting in MongoDB 5.0, cursors created in a session will close when the server session ends, times out, or when the client exhausts the cursor. The default session timeout is 30 minutes. 9.2 Cursors Opened Outside a Session Cursors not associated with a session will automatically close after 10 minutes of inactivity or when fully iterated. To prevent this, use the noCursorTimeout() option: var myCursor = db.users.find().noCursorTimeout(); Remember to close the cursor manually or exhaust it. 9.3 Batch Size and GetMore Operations Query results are returned in batches. The initial batch for find() and aggregate() is 101 documents by default. As you iterate, the cursor issues getMore operations to fetch subsequent batches (up\n",
      "Stored embedding for: to a 16 MiB message size limit). You can check the remaining documents in the current batch with objsLeftInBatch(). ⸻ 10. Snapshot Queries and Read Concern MongoDB supports snapshot queries using the read concern \"snapshot\", which lets you read data as it appeared at a single point in time. 10.1 Use Case for Snapshot Queries Snapshot queries are useful when you need to: • Run multiple related queries that must reflect the same point in time. • Ensure consistency in reporting, especially in systems with ongoing writes. 10.2 Example: Multiple Collections An animal shelter might count adoptable pets from different collections while ensuring consistency: db = client.pets with client.start_session(snapshot=True) as s: adoptablePetsCount = ( db.cats.aggregate( [{\"$match\": {\"adoptable\": True}}, {\"$count\": \"adoptableCatsCount\"}], session=s ).next()[\"adoptableCatsCount\"] ) adoptablePetsCount += ( db.dogs.aggregate( [{\"$match\": {\"adoptable\": True}}, {\"$count\": \"adoptableDogsCount\"}],\n",
      "Stored embedding for: session=s ).next()[\"adoptableDogsCount\"] ) print(adoptablePetsCount) 10.3 Example: Daily Sales Count Ensure that a query counting daily sales does not include new sales occurring during the query execution: db = client.retail with client.start_session(snapshot=True) as s: dailySales = ( db.sales.aggregate( [ { \"$match\": { \"$expr\": { \"$gt\": [ \"$saleDate\", { \"$dateSubtract\": { \"startDate\": \"$$NOW\", \"unit\": \"day\", \"amount\": 1 } } ] } } }, {\"$count\": \"totalDailySales\"} ], session=s ).next()[\"totalDailySales\"] )\n",
      "Stored embedding for: print(dailySales) Note: Sessions using \"snapshot\" read concern must complete within the WiredTiger history retention period (default 300 seconds) unless the retention period is increased. ⸻ Conclusion This guide provides a comprehensive overview of the MongoDB Query Language features, including basic queries, complex operators, array and embedded document queries, projections, and cursor management. MongoDB’s flexible querying capabilities allow you to construct queries that closely resemble SQL while leveraging the document model’s power and flexibility. By understanding these concepts and examples, you can effectively build robust applications that interact with MongoDB data.\n",
      " -----> Processed MongoDB Query Language Documentation.pdf\n",
      "Stored embedding for: Amazon EC2 + EWS Lambda ⸻ Slide 1: Where Can You Store Data? Slide Content (as text): • Instance Store: Temporary, high-speed storage tied to the instance lifecycle • EFS (Elastic File System) Support: Shared file storage • EBS (Elastic Block Storage): Persistent block-level storage • S3: Large data set storage or EC2 backups Expanded Explanation: 1. Instance Store • This is storage physically attached to the host machine running your EC2 instance. • It is extremely fast and ideal for temporary or ephemeral data. • Important Note: If you stop or terminate the EC2 instance, data on the instance store is lost. 2. EFS (Elastic File System) • A fully managed, shared file system that can be mounted on multiple EC2 instances simultaneously. • Ideal for scenarios where you need a common file system for multiple servers or services. 3. EBS (Elastic Block Storage) • Persistent storage volumes that can be attached to an EC2 instance. • Data remains available even if you stop or reboot the instance (but not if you terminate and delete the volume, unless you configure otherwise). • Great for typical server use-cases like a root volume or attached volumes for databases. 4. S3 (Simple Storage Service) • Object-based storage for large amounts of data (files, images, backups, static web content, etc.). • Highly durable and cost-effective. • Typically used for backups, hosting static sites, or data lakes. ⸻ Slide 2 (labeled as “6” in the screenshot): Amazon EC2 & Lambda Slide Content (as text):\n",
      "Stored embedding for: • DS 4300 • Amazon EC2 & Lambda • Mark Fontenot, PhD • Northeastern University Expanded Explanation: • This slide introduces the topic of Amazon EC2 (Elastic Compute Cloud) and AWS Lambda. • EC2 is Amazon’s Infrastructure as a Service (IaaS) offering, allowing you to rent virtual machines (“instances”) in the cloud. • AWS Lambda is a serverless computing service that lets you run code without provisioning or managing servers. ⸻ Slide 3 (labeled “7” in the screenshot): Let’s Spin Up an EC2 Instance Slide Content (as text): • “Let’s Spin Up an EC2 Instance” (title) Expanded Explanation: • This slide indicates the beginning of a tutorial or demonstration on how to create (“spin up”) a new EC2 instance. • Spinning up an instance involves choosing an AMI (Amazon Machine Image), selecting instance type, configuring network settings, etc. • The upcoming slides likely detail the step-by-step process. ⸻ Slide 4 (labeled “8”): Let’s Spin Up an EC2 Instance Slide Content (as text, partially visible): • Screenshots from the AWS Console showing steps to create a new EC2 instance. Expanded Explanation: • Step 1: Choose an AMI – For example, “Ubuntu Server 20.04 LTS”. • Step 2: Choose an Instance Type – e.g., t2.micro for a small, free-tier- eligible instance. • Step 3: Configure Instance Details – VPC settings, subnets, etc. • Step 4: Add Storage – EBS volume size, etc. • Step 5: Add Tags – Optional metadata for better organization. • Step 6: Configure Security Group – Control inbound/outbound traffic with firewall rules.\n",
      "Stored embedding for: • Step 7: Review & Launch – Launch the instance and create or select a key pair for SSH access. ⸻ Slide 5 (labeled “9”): Let’s Spin Up an EC2 Instance Slide Content (as text): • Another screenshot continuing the setup steps. Expanded Explanation: • Likely continues detailing the creation process, focusing on security group configuration or final review. • Security Group best practice: • Open only the ports you need. • Typically, port 22 for SSH (for Linux) or RDP (for Windows) is required. • If you plan to run a web server, open port 80 (HTTP) or 443 (HTTPS). ⸻ Slide 6 (labeled “10”): Ubuntu VM Commands Slide Content (as text): • Initial user is ubuntu • Access super user commands with sudo • Make sure apt-get is up to date • Update the packages installed • sudo apt update; sudo apt upgrade Expanded Explanation: • Once you SSH into the instance, you’ll be logged in as the ubuntu user by default (on an Ubuntu AMI). • To perform administrative tasks (install packages, change configurations), you’ll use sudo. • Key commands: • sudo apt update – Refreshes the package lists for upgrades/ new packages. • sudo apt upgrade – Installs available updates. • This ensures your instance is secure and has the latest security patches. ⸻\n",
      "Stored embedding for: Slide 7 (labeled “11”): MiniConda on EC2 Slide Content (as text): • Make sure you’re logged in to your EC2 instance. • Let’s install MiniConda: • Example command: wget https://repo.anaconda.com/ miniconda/Miniconda3-latest-Linux-x86_64.sh • Then run bash Miniconda3-latest-Linux-x86_64.sh. Expanded Explanation: • MiniConda is a minimal installer for Conda, a package manager that helps you install Python and data-science-related libraries easily. • After installation, you typically need to source your ~/.bashrc or restart your shell so that the conda command is recognized. • You can then create or manage virtual environments on your EC2 instance. ⸻ Slide 8 (labeled “12”): Installing & Using Streamlit Slide Content (as text): • Log out of your EC2 instance and log back in. • streamlit is now available! • In your environment, install Streamlit. • Make a test Streamlit file. • Basic usage: streamlit run my_app.py • cd web Expanded Explanation: • After installing Streamlit with pip or conda install streamlit, you can run it by calling streamlit run <filename.py>. • You might log out and back in to refresh your PATH or environment variables. • Creating a quick test file (e.g., my_app.py) helps confirm that Streamlit is installed correctly. ⸻ Slide 9 (labeled “13”): Basic Streamlit App Slide Content (as text):\n",
      "Stored embedding for: import streamlit as st st.title(\"My Streamlit App\") st.write(\"Hello, world!\") • Save as test.py • nano test.py (to edit) • streamlit run test.py Expanded Explanation: • Streamlit is a Python library for building simple web apps quickly, especially for data visualization and data science demos. • A minimal example includes a title and a simple “Hello, world!” text. • st.title() sets the main heading, while st.write() can display text, data frames, or other content. ⸻ Slide 10 (labeled “14”): Opening Up The Streamlit Port Slide Content (as text): • Screenshot of AWS security group settings. • Possibly showing how to open the port used by Streamlit (default is 8501). Expanded Explanation: • By default, Streamlit runs on localhost:8501. • To make it accessible externally, you can either: 1. Use an SSH tunnel (not covered here), or 2. Open port 8501 in your EC2 security group inbound rules so you can access http://<public-ip>:8501. • Security Group: • Add a rule: • Type: Custom TCP • Port Range: 8501 • Source: Your IP (or 0.0.0.0/0 if you want it accessible to everyone—less secure) ⸻ Slide 11 (labeled “15”): In a Browser Slide Content (as text):\n",
      "Stored embedding for: • Screenshot of the Streamlit app running in the browser, with a title “Welcome to my Streamlit App.” Expanded Explanation: • After you’ve opened the port and started Streamlit on the server (streamlit run test.py), navigate to http://<EC2-Public-IP>:8501. • You should see your basic Streamlit app. • This confirms your setup is correct: instance is running, port is open, and the app is served. ⸻ Slide 12 (labeled “16”): AWS Lambda (This slide seems to be a title or transition slide.) Slide Content (as text): • “AWS Lambda” (title) Expanded Explanation: • Now shifting the topic from EC2 to AWS Lambda. • Lambda is a serverless compute service where you only pay for the time your code is running. • You don’t need to manage any infrastructure; AWS handles the underlying servers. ⸻ Slide 13 (labeled “17”): Lambdas Slide Content (as text): • Lambdas provide serverless computing. • Automatically run code in response to events. • Worry about the code, not the servers. • You only pay for execution time, not idle compute time (different from EC2). Expanded Explanation: • Serverless means you don’t provision or manage servers. Instead, you write functions that AWS executes on-demand. • Events can be triggered by S3 uploads, API Gateway calls, DynamoDB updates, scheduled CloudWatch events, etc. • Pay-as-you-go model: • Billed by the number of requests and the time your code runs.\n",
      "Stored embedding for: • No charges when your code isn’t running. ⸻ Slide 14 (labeled “18”): Lambda Features Slide Content (as text): • Event-driven execution – can be triggered by many different events in AWS. • Supports a large number of runtimes: Python, Java, Node.js, etc. • Highly integrated with other AWS services. • Extremely scalable and scales automatically. Expanded Explanation: • Because Lambda is event-driven, you don’t have to keep an instance running. • Official runtimes include Node.js, Python, Ruby, Java, Go, .NET, etc. You can also use custom runtimes. • Integrations with S3, SNS, SQS, DynamoDB, API Gateway, and more let you build sophisticated serverless architectures. • Lambda automatically handles concurrency. If multiple events occur at once, Lambda will spin up additional executions. ⸻ Slide 15 (labeled “19”): How it Works Slide Content (as text): 1. Add/Upload your code through AWS Management Console (or via CLI, or CI/CD). 2. Configure event source(s). 3. Watch your Lambda run when one of the event sources triggers it. Expanded Explanation: • Step 1: You can write your function code in the console’s built-in editor, upload a ZIP, or reference an S3 bucket. • Step 2: An event source (e.g., S3 bucket event, CloudWatch event) is configured to call your function. • Step 3: Once the event occurs, AWS Lambda automatically executes your function. ⸻ Slide 16 (labeled “20”): Let’s Make One\n",
      "Stored embedding for: (Likely an introduction to a hands-on Lambda creation demo.) Slide Content (as text): • “Let’s Make One” (title) Expanded Explanation: • You will walk through creating a new Lambda function in the AWS Console. • The following slides presumably show each step in the process. ⸻ Slide 17 (labeled “21”): Making a Lambda Slide Content (as text): • Screenshot from the AWS Lambda console, showing the “Create function” button or the initial steps in creation. Expanded Explanation: • On the Lambda main page, click Create function. • You’ll be prompted to choose between: 1. Author from scratch 2. Use a blueprint 3. Use container image 4. Browse serverless app repository • Typically, you’ll “Author from scratch” to get started with a basic function. ⸻ Slide 18 (labeled “22”): Creating a Function Slide Content (as text): • Possibly shows form fields: • Function name • Runtime (Python, Node.js, etc.) • Permissions/Role Expanded Explanation: • Function name: Choose something descriptive, e.g., my-first-lambda. • Runtime: Select the language environment (e.g., Python 3.9). • Permissions: You can create or select an IAM role that gives Lambda the permissions it needs (for example, to log to CloudWatch).\n",
      "Stored embedding for: ⸻ Slide 19 (labeled “23”): Sample Code Slide Content (as text): def lambda_handler(event, context): # Your code here return { 'statusCode': 200, 'body': 'Hello from Lambda!' } Expanded Explanation: • This is a simple Python Lambda handler. • The lambda_handler function is the entry point for the Lambda. • Parameters: • event: data passed in by the triggering service or event. • context: runtime information about the function execution (e.g., request ID, memory limits, etc.). • The returned dictionary typically includes a statusCode and body (especially if used behind an API Gateway). ⸻ Slide 20 (labeled “24”): Edit the code, Deploy the code! Slide Content (as text): • A screenshot showing the Lambda code editor in the console, with a “Deploy” button. Expanded Explanation: • After writing or modifying your code in the console, you must Deploy it to save the changes and make them live. • You can test the code by configuring a test event or by hooking up an actual event source. ⸻ Slide 21 (labeled “25”): Test It Slide Content (as text): • A screenshot showing the “Test” button and a sample event\n",
      "Stored embedding for: configuration. Expanded Explanation: • You can create a test event in the console that mimics the payload from a real event source (e.g., S3, API Gateway). • Once you click Test, Lambda will run your function with the provided event. • The console will display the result, including logs and the return value. ⸻ Final Notes and Summary By following these slides and the expanded explanations, you have a roadmap for: 1. Creating and managing an EC2 instance: • Choosing an AMI (like Ubuntu). • Updating and installing necessary packages (including MiniConda). • Deploying a basic web application (e.g., Streamlit). • Opening ports in the security group for external access. 2. Introduction to AWS Lambda: • Understanding serverless concepts. • Creating a basic Lambda function in Python. • Event-driven execution and the pay-per-execution model. • Testing your Lambda function with custom events. These are foundational AWS concepts—EC2 for on-demand virtual machine instances and Lambda for serverless functions. With these, you can build a wide range of applications in the AWS ecosystem. ⸻ End of Expanded Lecture Notes\n",
      " -----> Processed Amazon EC2 + EWS Lambda.pdf\n",
      "Stored embedding for: ● ● ● ● MongoDB + PyMongo Introduction MongoDB is a popular NoSQL database that stores data in flexible, JSON-like documents. PyMongo is the official Python library for interacting with MongoDB databases, allowing Python applications to easily perform database operations. Presented by: Mark Fontenot, PhD Northeastern University PyMongo Overview PyMongo simplifies interactions between Python and MongoDB instances by abstracting database operations into intuitive Python methods. Installation To install PyMongo: pip install pymongo Connecting to MongoDB Below is an example of how to establish a connection to a local MongoDB instance using PyMongo: from pymongo import MongoClient client = MongoClient('mongodb://user_name:pw@localhost:27017') Explanation: MongoClient is a class imported from the PyMongo library used to establish a connection to a MongoDB database. The connection string (mongodb://) includes a username (user_name), password (pw), hostname (localhost), and port number (27017). Getting a Database and Collection MongoDB organizes data into databases, and each database contains collections (similar to tables in relational databases). from pymongo import MongoClient client = MongoClient('mongodb://user_name:pw@localhost:27017') db = client['ds4300'] # or client.ds4300 collection = db['myCollection'] # or db.myCollection Explanation: db represents a specific database instance. collection is a collection within the database, where documents (records) will be stored.\n",
      "Stored embedding for: ● ● ● ● ● ● 1. 2. 1. ● Inserting a Single Document Here’s how to insert a single JSON-like document into the collection: db = client['ds4300'] collection = db['myCollection'] post = { \"author\": \"Mark\", \"text\": \"MongoDB is Cool!\", \"tags\": [\"mongodb\", \"python\"] } post_id = collection.insert_one(post).inserted_id print(post_id) Explanation: insert_one() inserts a single document into the collection. The inserted document automatically receives a unique _id. The returned inserted_id is useful for referencing this document later. Finding Documents The following example retrieves documents matching specific criteria (e.g., movies released in the year 2000): from bson.json_util import dumps # Find all movies released in 2000 movies_2000 = db.movies.find({\"year\": 2000}) # Pretty-print the results print(dumps(movies_2000, indent=2)) Explanation: find() method queries the database based on specified criteria ({\"year\": 2000}). Results are returned as a cursor that can be iterated through. bson.json_util.dumps helps print MongoDB cursor results in readable JSON format. Setting Up Environment with Jupyter To work with PyMongo in a Jupyter notebook environment: Activate your Conda environment or create a virtual Python environment. Install required libraries: pip install pymongo pip install jupyterlab Download provided notebooks and unzip: A provided zip file (this) contains Jupyter notebooks for practice.\n",
      "Stored embedding for: 1. Launch Jupyter Lab: jupyter lab Navigate to the unzipped folder in the Jupyter interface and open the provided notebooks to begin interacting with MongoDB using PyMongo. This guide covers basic operations for MongoDB interactions using PyMongo, clearly laying out installation, connection, document insertion, querying, and environment setup.\n",
      " -----> Processed MongoDB + PyMongo.pdf\n",
      "Stored embedding for: DS 4300 NoSQL & KV DBs Mark Fontenot, PhD Northeastern University Some material used with permission from Dr. Rachlin, with thanks!\n",
      "Stored embedding for: Distributed DBs and ACID - Pessimistic Concurrency ●ACID transactions ○ Focuses on “data safety” ○ considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions ■ IOW, it assumes that if something can go wrong, it will. ○ Conﬂicts are prevented by locking resources until a transaction is complete (there are both read and write locks) ○ Write Lock Analogy → borrowing a book from a library… If you have it, no one else can. 2 See https://www.freecodecamp.org/news/how-databases-guarantee-isolation for more for a deeper dive.\n",
      "Stored embedding for: Optimistic Concurrency ●Transactions do not obtain locks on data when they read or write ●Optimistic because it assumes conﬂicts are unlikely to occur ○ Even if there is a conﬂict, everything will still be OK. ●But how? ○ Add last update timestamp and version number columns to every table… read them when changing. THEN, check at the end of transaction to see if any other transaction has caused them to be modiﬁed. 3\n",
      "Stored embedding for: Optimistic Concurrency ●Low Conﬂict Systems (backups, analytical dbs, etc.) ○ Read heavy systems ○ the conﬂicts that arise can be handled by rolling back and re-running a transaction that notices a conﬂict. ○ So, optimistic concurrency works well - allows for higher concurrency ●High Conﬂict Systems ○ rolling back and rerunning transactions that encounter a conﬂict → less efﬁcient ○ So, a locking scheme (pessimistic model) might be preferable 4\n",
      "Stored embedding for: NoSQL - “NoSQL” ﬁrst used in 1998 by Carlo Strozzi to describe his relational database system that did not use SQL. - More common, modern meaning is “Not Only SQL” - But, sometimes thought of as non-relational DBs - Idea originally developed, in part, as a response to processing unstructured web-based data. 5 https://www.dataversity.net/a-brief-history-of-non-relational-databases/\n",
      "Stored embedding for: CAP Theorem Review 6 Reference: https://alperenbayramoglu.com/posts/understanding-cap-theorem/ You can have 2, but not 3, of the following: - Consistency*: Every user of the DB has an identical view of the data at any given instant - Availability: In the event of a failure, the database system remains operational - Partition Tolerance: The database can maintain operations in the event of the network’s failing between two segments of the distributed system * Note, the definition of Consistency in CAP is different from that of ACID.\n",
      "Stored embedding for: CAP Theorem Review 7 Reference: https://alperenbayramoglu.com/posts/understanding-cap-theorem/ - Consistency + Availability: System always responds with the latest data and every request gets a response, but may not be able to deal with network partitions - Consistency + Partition Tolerance: If system responds with data from the distrib. system, it is always the latest, else data request is dropped. - Availability + Partition Tolerance: System always sends are responds based on distributed store, but may not be the absolute latest data.\n",
      "Stored embedding for: ACID Alternative for Distrib Systems - BASE ●Basically Available ○Guarantees the availability of the data (per CAP), but response can be “failure”/“unreliable” because the data is in an inconsistent or changing state ○System appears to work most of the time 8\n",
      "Stored embedding for: ACID Alternative for Distrib Systems - BASE ●Soft State - The state of the system could change over time, even w/o input. Changes could be result of eventual consistency. ○Data stores don’t have to be write-consistent ○Replicas don’t have to be mutually consistent 9\n",
      "Stored embedding for: ACID Alternative for Distrib Systems - BASE ●Eventual Consistency - The system will eventually become consistent ○All writes will eventually stop so all nodes/replicas can be updated 10\n",
      "Stored embedding for: First Up → Key-Value Databases 11\n",
      "Stored embedding for: Key Value Stores key = value - Key-value stores are designed around: - simplicity - the data model is extremely simple - comparatively, tables in a RDBMS are very complex. - lends itself to simple CRUD ops and API creation 12\n",
      "Stored embedding for: Key Value Stores key = value - Key-value stores are designed around: - speed - usually deployed as in-memory DB - retrieving a value given its key is typically a O(1) op b/c hash tables or similar data structs used under the hood - no concept of complex queries or joins… they slow things down 13\n",
      "Stored embedding for: Key Value Stores key = value - Key-value stores are designed around: - scalability - Horizontal Scaling is simple - add more nodes - Typically concerned with eventual consistency, meaning in a distributed environment, the only guarantee is that all nodes will eventually converge on the same value. 14\n",
      "Stored embedding for: KV DS Use Cases - EDA/Experimentation Results Store - store intermediate results from data preprocessing and EDA - store experiment or testing (A/B) results w/o prod db - Feature Store - store frequently accessed feature → low-latency retrieval for model training and prediction - Model Monitoring - store key metrics about performance of model, for example, in real-time inferencing. 15\n",
      "Stored embedding for: KV SWE Use Cases - Storing Session Information - everything about the current session can be stored via a single PUT or POST and retrieved with a single GET …. VERY Fast - User Proﬁles & Preferences - User info could be obtained with a single GET operation… language, TZ, product or UI preferences - Shopping Cart Data - Cart data is tied to the user - needs to be available across browsers, machines, sessions - Caching Layer: - In front of a disk-based database 16\n",
      "Stored embedding for: Redis DB - Redis (Remote Directory Server) - Open source, in-memory database - Sometimes called a data structure store - Primarily a KV store, but can be used with other models: Graph, Spatial, Full Text Search, Vector, Time Series - From db-engines.com Ranking of KV Stores: 17\n",
      "Stored embedding for: Redis - It is considered an in-memory database system, but… - Supports durability of data by: a) essentially saving snapshots to disk at speciﬁc intervals or b) append-only ﬁle which is a journal of changes that can be used for roll-forward if there is a failure - Originally developed in 2009 in C++ - Can be very fast … > 100,000 SET ops / second - Rich collection of commands - Does NOT handle complex data. No secondary indexes. Only supports lookup by Key. 18\n",
      "Stored embedding for: Redis Data Types Keys: - usually strings but can be any binary sequence Values: - Strings - Lists (linked lists) - Sets (unique unsorted string elements) - Sorted Sets - Hashes (string → string) - Geospatial data 19\n",
      "Stored embedding for: Setting Up Redis in Docker - In Docker Desktop, search for Redis. - Pull/Run the latest image (see above) - Optional Settings: add 6379 to Ports to expose that port so we can connect to it. - Normally, you would not expose the Redis port for security reasons - If you did this in a prod environment, major security hole. - Notice, we didn’t set a password… 20\n",
      "Stored embedding for: Connecting from DataGrip - File > New > Data Source > Redis - Give the Data Source a Name - Make sure the port is 6379 - Test the connection ✅ 21\n",
      "Stored embedding for: Redis Database and Interaction - Redis provides 16 databases by default - They are numbered 0 to 15 - There is no other name associated - Direct interaction with Redis is through a set of commands related to setting and getting k/v pairs (and variations) - Many language libraries available as well. 22\n",
      "Stored embedding for: Foundation Data Type - String - Sequence of bytes - text, serialized objects, bin arrays - Simplest data type - Maps a string to another string - Use Cases: - caching frequently accessed HTML/CSS/JS fragments - conﬁg settings, user settings info, token management - counting web page/app screen views OR rate limiting 23\n",
      "Stored embedding for: Some Initial Basic Commands - SET /path/to/resource 0 SET user:1 “John Doe” GET /path/to/resource EXISTS user:1 DEL user:1 KEYS user* - SELECT 5 - select a different database 24\n",
      "Stored embedding for: Some Basic Commands - SET someValue 0 INCR someValue #increment by 1 INCRBY someValue 10 #increment by 10 DECR someValue #decrement by 1 DECRBY someValue 5 #decrement by 5 - INCR parses the value as int and increments (or adds to value) - SETNX key value - only sets value to key if key does not already exist 25\n",
      "Stored embedding for: Hash Type 26 - Value of KV entry is a collection of ﬁeld-value pairs - Use Cases: - Can be used to represent basic objects/structures - number of ﬁeld/value pairs per hash is 2^32-1 - practical limit: available system resources (e.g. memory) - Session information management - User/Event tracking (could include TTL) - Active Session Tracking (all sessions under one hash key)\n",
      "Stored embedding for: Hash Commands 27 HSET bike:1 model Demios brand Ergonom price 1971 HGET bike:1 model HGET bike:1 price HGETALL bike:1 HMGET bike:1 model price weight HINCRBY bike:1 price 100 What is returned?\n",
      "Stored embedding for: List Type - Value of KV Pair is linked lists of string values - Use Cases: - implementation of stacks and queues - queue management & message passing queues (producer/consumer model) - logging systems (easy to keep in chronological order) - build social media streams/feeds - message history in a chat application - batch processing by queueing up a set of tasks to be executed sequentially at a later time 28\n",
      "Stored embedding for: Linked Lists Crash Course - Sequential data structure of linked nodes (instead of contiguously allocated memory) - Each node points to the next element of the list (except the last one - points to nil/null) - O(1) to insert new value at front or insert new value at end 29 10 front back nil\n",
      "Stored embedding for: List Commands - Queue Queue-like Ops LPUSH bikes:repairs bike:1 LPUSH bikes:repairs bike:2 RPOP bikes:repairs RPOP biles:repairs 30\n",
      "Stored embedding for: List Commands - Stack Stack-like Ops LPUSH bikes:repairs bike:1 LPUSH bikes:repairs bike:2 LPOP bikes:repairs LPOP biles:repairs 31\n",
      "Stored embedding for: List Commands - Others Other List Ops LLEN mylist LRANGE <key> <start> <stop> LRANGE mylist 0 3 LRANGE mylist 0 0 LRANGE mylist -2 -1 32 LPUSH mylist “one” LPUSH mylist “two” LPUSH mylist “three”\n",
      "Stored embedding for: JSON Type - Full support of the JSON standard - Uses JSONPath syntax for parsing/navigating a JSON document - Internally, stored in binary in a tree-structure → fast access to sub elements 33\n",
      "Stored embedding for: Set Type - Unordered collection of unique strings (members) - Use Cases: - track unique items (IP addresses visiting a site, page, screen) - primitive relation (set of all students in DS4300) - access control lists for users and permission structures - social network friends lists and/or group membership - Supports set operations!! 34\n",
      "Stored embedding for: Set Commands SADD ds4300 “Mark” SADD ds4300 “Sam” SADD cs3200 “Nick” SADD cs3200 “Sam” SISMEMBER ds4300 “Mark” SISMEMBER ds4300 “Nick” SCARD ds4300 35\n",
      "Stored embedding for: Set Commands SADD ds4300 “Mark” SADD ds4300 “Sam” SADD cs3200 “Nick” SADD cs3200 “Sam” SCARD ds4300 SINTER ds4300 cs3200 SDIFF ds4300 cs3200 SREM ds4300 “Mark” SRANDMEMBER ds4300 36\n",
      "Stored embedding for: ?? 37\n",
      " -----> Processed 05 - NoSQL Intro + KV DBs.pdf\n",
      "Stored embedding for: DS 4300 Document Databases & MongoDB Mark Fontenot, PhD Northeastern University Some material used with permission from Dr. Rachlin, with thanks!\n",
      "Stored embedding for: Document Database A Document Database is a non-relational database that stores data as structured documents, usually in JSON. They are designed to be simple, ﬂexible, and scalable. 2\n",
      "Stored embedding for: What is JSON? ● JSON (JavaScript Object Notation) ○ a lightweight data-interchange format ○ It is easy for humans to read and write. ○ It is easy for machines to parse and generate. ● JSON is built on two structures: ○ A collection of name/value pairs. In various languages, this is operationalized as an object, record, struct, dictionary, hash table, keyed list, or associative array. ○ An ordered list of values. In most languages, this is operationalized as an array, vector, list, or sequence. ● These are two universal data structures supported by virtually all modern programming languages ○ Thus, JSON makes a great data interchange format. 3\n",
      "Stored embedding for: JSON Syntax 4 https://www.json.org/json-en.html\n",
      "Stored embedding for: Binary JSON? BSON - BSON → Binary JSON - binary-encoded serialization of a JSON-like document structure - supports extended types not part of basic JSON (e.g. Date, BinaryData, etc) - Lightweight - keep space overhead to a minimum - Traversable - designed to be easily traversed, which is vitally important to a document DB - Efﬁcient - encoding and decoding must be efﬁcient - Supported by many modern programming languages 5\n",
      "Stored embedding for: XML (eXtensible Markup Language) ●Precursor to JSON as data exchange format ●XML + CSS → web pages that separated content and formatting ●Structurally similar to HTML, but tag set is extensible 6\n",
      "Stored embedding for: XML-Related Tools/Technologies - Xpath - a syntax for retrieving speciﬁc elements from an XML doc - Xquery - a query language for interrogating XML documents; the SQL of XML - DTD - Document Type Deﬁnition - a language for describing the allowed structure of an XML document - XSLT - eXtensible Stylesheet Language Transformation - tool to transform XML into other formats, including non-XML formats such as HTML. 7\n",
      "Stored embedding for: Why Document Databases? - Document databases address the impedance mismatch problem between object persistence in OO systems and how relational DBs structure data. - OO Programming → Inheritance and Composition of types. - How do we save a complex object to a relational database? We basically have to deconstruct it. - The structure of a document is self-describing. - They are well-aligned with apps that use JSON/XML as a transport layer 8\n",
      "Stored embedding for: MongoDB 9\n",
      "Stored embedding for: MongoDB - Started in 2007 after Doubleclick was acquired by Google, and 3 of its veterans realized the limitations of relational databases for serving > 400,000 ads per second - MongoDB was short for Humongous Database - MongoDB Atlas released in 2016 → documentdb as a service 10 https://www.mongodb.com/company/our-story\n",
      "Stored embedding for: MongoDB Structure 11 Database Collection A Collection B Collection C Document 1 Document 2 Document 3 Document 1 Document 2 Document 3 Document 1 Document 2 Document 3\n",
      "Stored embedding for: MongoDB Documents - No predeﬁned schema for documents is needed - Every document in a collection could have different data/schema 12\n",
      "Stored embedding for: Relational vs Mongo/Document DB 13 RDBMS MongoDB Database Database Table/View Collection Row Document Column Field Index Index Join Embedded Document Foreign Key Reference\n",
      "Stored embedding for: MongoDB Features - Rich Query Support - robust support for all CRUD ops - Indexing - supports primary and secondary indices on document ﬁelds - Replication - supports replica sets with automatic failover - Load balancing built in 14\n",
      "Stored embedding for: MongoDB Versions ●MongoDB Atlas ○ Fully managed MongoDB service in the cloud (DBaaS) ●MongoDB Enterprise ○ Subscription-based, self-managed version of MongoDB ●MongoDB Community ○ source-available, free-to-use, self-managed 15\n",
      "Stored embedding for: Interacting with MongoDB ●mongosh → MongoDB Shell ○ CLI tool for interacting with a MongoDB instance ●MongoDB Compass ○ free, open-source GUI to work with a MongoDB database ●DataGrip and other 3rd Party Tools ●Every major language has a library to interface with MongoDB ○ PyMongo (Python), Mongoose (JavaScript/node), … 16\n",
      "Stored embedding for: Mongodb Community Edition in Docker - Create a container - Map host:container port 27017 - Give initial username and password for superuser 17 E D\n",
      "Stored embedding for: MongoDB Compass - GUI Tool for interacting with MongoDB instance - Download and install from > here <. 18\n",
      "Stored embedding for: Load MFlix Sample Data Set - In Compass, create a new Database named mﬂix - Download mﬂix sample dataset and unzip it - Import JSON ﬁles for users, theaters, movies, and comments into new collections in the mﬂix database 19\n",
      "Stored embedding for: Creating a Database and Collection 20 mﬂix users To Create a new DB: To Create a new Collection:\n",
      "Stored embedding for: mongosh - Mongo Shell - ﬁnd(...) is like SELECT 21 collection.find({ ____ }, { ____ }) ﬁlters projections\n",
      "Stored embedding for: mongosh - ﬁnd() - SELECT * FROM users; 22 use mflix db.users.find()\n",
      "Stored embedding for: mongosh - ﬁnd() - SELECT * FROM users WHERE name = “Davos Seaworth”; 23 db.users.find({\"name\": \"Davos Seaworth\"}) ﬁlter\n",
      "Stored embedding for: mongosh - ﬁnd() - SELECT * FROM movies WHERE rated in (\"PG\", \"PG-13\") 24 db.movies.find({rated: {$in:[ \"PG\", \"PG-13\" ]}})\n",
      "Stored embedding for: mongosh - ﬁnd() - Return movies which were released in Mexico and have an IMDB rating of at least 7 25 db.movies.find( { \"countries\": \"Mexico\", \"imdb.rating\": { $gte: 7 } } )\n",
      "Stored embedding for: mongosh - ﬁnd() - Return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of Drama 26 db.movies.find( { “year”: 2010, $or: [ { \"awards.wins\": { $gte: 5 } }, { “genres”: \"Drama\" } ] })\n",
      "Stored embedding for: Comparison Operators 27\n",
      "Stored embedding for: mongosh - countDocuments() - How many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of Drama 28 db.movies.countDocuments( { “year”: 2010, $or: [ { \"awards.wins\": { $gte: 5 } }, { “genres”: \"Drama\" } ] })\n",
      "Stored embedding for: mongosh - project - Return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of Drama 29 db.movies.countDocuments( { “year”: 2010, $or: [ { \"awards.wins\": { $gte: 5 } }, { “genres”: \"Drama\" } ] }, {“name”: 1, “_id”: 0} ) 1 = return; 0 = don’t return\n",
      "Stored embedding for: PyMongo 30\n",
      "Stored embedding for: PyMongo ●PyMongo is a Python library for interfacing with MongoDB instances 31 from pymongo import MongoClient client = MongoClient( ‘mongodb://user_name:pw@localhost:27017’ )\n",
      "Stored embedding for: Getting a Database and Collection 32 from pymongo import MongoClient client = MongoClient( ‘mongodb://user_name:pw@localhost:27017’ ) db = client[‘ds4300’] collection = db[‘myCollection’]\n",
      "Stored embedding for: Inserting a Single Document 33 db = client[‘ds4300’] collection = db[‘myCollection’] post = { “author”: “Mark”, “text”: “MongoDB is Cool!”, “tags”: [“mongodb”, “python”] } post_id = collection.insert_one(post).inserted_id print(post_id)\n",
      "Stored embedding for: Count Documents in Collection - SELECT count(*) FROM collection 34 demodb.collection.count_documents({})\n",
      "Stored embedding for: ?? 35\n",
      " -----> Processed 07 - Document DBs and Mongo.pdf\n",
      "Stored embedding for: DS 4300 Redis in Docker Setup Mark Fontenot, PhD Northeastern University\n",
      "Stored embedding for: Pre-Requisites 2 - You have installed Docker Desktop - You have installed Jetbrains DataGrip\n",
      "Stored embedding for: Step 1 - Find the Redis Image - Open Docker Desktop - Use the Built In search to ﬁnd the Redis Image - Click Run 3\n",
      "Stored embedding for: Step 2 - Conﬁgure & Run the Container - Give the new container a name - Enter 6379 in Host Port ﬁeld - Click Run - Give Docker some time to download and start Redis 4\n",
      "Stored embedding for: Step 3 - Set up Data Source in DataGrip - Start DataGrip - Create a new Redis Data Source - You can use the + in the Database Explorer OR - You can use New from the File Menu 5\n",
      "Stored embedding for: Step 4 - Conﬁgure the Data Source - Give the data source a name - Install Drivers if needed (message above Test Connection) - Test the Connection to Redis - Click OK if connection test was successful 6 There will be a message to install drivers above Test Connection if they aren’t already installed\n",
      " -----> Processed 06 - Redis in Docker.pdf\n",
      "Stored embedding for: DS 4300 AWS Introduction Mark Fontenot, PhD Northeastern University\n",
      "Stored embedding for: Amazon Web Services ●Leading Cloud Platform with over 200 different services available ●Globally available via its massive networks of regions and availability zones with their massive data centers ●Based on a pay-as-you-use cost model. ○ Theoretically cheaper than renting rackspace/servers in a data center… Theoretically. 2\n",
      "Stored embedding for: History of AWS ●Originally launched in 2006 with only 2 services: S3 & EC2. ●By 2010, services had expanded to include SimpleDB, Elastic Block Store, Relational Database Service, DynamoDB, CloudWatch, Simple Workﬂow, CloudFront, Availability Zones, and others. ●Amazon had competitions with big prizes to spur the adoption of AWS in its early days ●They’ve continuously innovated, always introducing new services for ops, dev, analytics, etc… (200+ services now) 3\n",
      "Stored embedding for: AWS Service Categories 4\n",
      "Stored embedding for: Cloud Models ●IaaS (more) - Infrastructure as a Service ○ Contains the basic services that are needed to build an IT infrastructure ●PaaS (more) - Platform as a Service ○ Remove the need for having to manage infrastructure ○ You can get right to deploying your app ●SaaS (more) - Software as a Service ○ Provide full software apps that are run and managed by another party/vendor 5\n",
      "Stored embedding for: Cloud Models 6 https://bluexp.netapp.com/iaas\n",
      "Stored embedding for: The Shared Responsibility Model - AWS - AWS Responsibilities (Security OF the cloud): - Security of physical infrastructure (infra) and network - keep the data centers secure, control access to them - maintain power availability, HVAC, etc. - monitor and maintain physical networking equipment and global infra/connectivity - Hypervisor & Host OSs - manage the virtualization layer used in AWS compute services - maintaining underlying host OSs for other services - Maintaining managed services - keep infra up to date and functional - maintain server software (patching, etc) 7\n",
      "Stored embedding for: The Shared Responsibility Model - Client - Client Responsibilities (Security IN the cloud): - Control of Data/Content - client controls how its data is classiﬁed, encrypted, and shared - implement and enforce appropriate data-handling policies - Access Management & IAM - properly conﬁgure IAM users, roles, and policies. - enforce the Principle of Least Privilege - Manage self-hosted Apps and associated OSs - Ensure network security to its VPC - Handle compliance and governance policies and procedures 8\n",
      "Stored embedding for: The AWS Global Infrastructure - Regions - distinct geographical areas - us-east-1, us-west 1, etc - Availability Zones (AZs) - each region has multiple AZs - roughly equiv to isolated data centers - Edge Locations - locations for CDN and other types of caching services - allows content to be closer to end user. 9\n",
      "Stored embedding for: 10 https://aws.amazon.com/about-aws/global-infrastructure/\n",
      "Stored embedding for: Compute Services 11 https://aws.amazon.com/products/compute/ - VM-based: - EC2 & EC2 Spot - Elastic Cloud Compute - Container-based: - ECS - Elastic Container Service - ECR - Elastic Container Registry - EKS - Elastic Kubernetes Service - Fargate - Serverless container service - Serverless: AWS Lambda\n",
      "Stored embedding for: Storage Services 12 https://aws.amazon.com/products/storage/ ● Amazon S3 - Simple Storage Service ○ Object storage in buckets; highly scalable; different storage classes ● Amazon EFS - Elastic File System ○ Simple, serverless, elastic, “set-and-forget” ﬁle system ● Amazon EBS - Elastic Block Storage ○ High-Performance block storage service ● Amazon File Cache ○ High-speed cache for datasets stored anywhere ● AWS Backup ○ Fully managed, policy-based service to automate data protection and compliance of apps on AWS\n",
      "Stored embedding for: Database Services ●Relational - Amazon RDS, Amazon Aurora ●Key-Value - Amazon DynamoDB ●In-Memory - Amazon MemoryDB, Amazon ElastiCache ●Document - Amazon DocumentDB (Compat with MongoDB) ●Graph - Amazon Neptune 13\n",
      "Stored embedding for: Analytics Services ●Amazon Athena - Analyze petabyte scale data where it lives (S3, for example) ●Amazon EMR - Elastic MapReduce - Access Apache Spark, Hive, Presto, etc. ●AWS Glue - Discover, prepare, and integrate all your data ●Amazon Redshift - Data warehousing service ●Amazon Kinesis - real-time data streaming ●Amazon QuickSight - cloud-native BI/reporting tool 14\n",
      "Stored embedding for: ML and AI Services - Amazon SageMaker - fully-managed ML platform, including Jupyter NBs - build, train, deploy ML models - AWS AI Services w/ Pre-trained Models - Amazon Comprehend - NLP - Amazon Rekognition - Image/Video analysis - Amazon Textract - Text extraction - Amazon Translate - Machine translation 15\n",
      "Stored embedding for: Important Services for Data Analytics/Engineering - EC2 and Lambda - Amazon S3 - Amazon RDS and DynamoDB - AWS Glue - Amazon Athena - Amazon EMR - Amazon Redshift 16\n",
      "Stored embedding for: AWS Free Tier ●Allows you to gain hands-on experience with a subset of the services for 12 months (service limitations apply as well) ○ Amazon EC2 - 750 hours/month (speciﬁc OSs and Instance Sizes) ○ Amazon S3 - 5GB (20K GETs, 2K Puts) ○ Amazon RDS - 750 hours/month of DB use (within certain limits) ○ ….. So many free services 17\n",
      "Stored embedding for: ?? 18\n",
      " -----> Processed 11 - AWS Intro.pdf\n",
      "Stored embedding for: B-Trees Explained in Detail Overview: B-trees optimize search tree structures for storage efficiency and locality, particularly beneficial for database and memory systems where cache or disk access is expensive. Unlike binary search trees (BSTs), which store a single key per node, B-trees store multiple keys, dramatically improving locality and reducing the number of reads required for operations. Key Concepts: - **Locality**: Exploiting memory and cache efficiency by grouping multiple keys and pointers. - **Branching Factor (Order)**: B-trees have a high branching factor (m > 2), meaning each node can have up to m children, significantly reducing tree height and enhancing search efficiency. - **Nodes**: There are two types of nodes: - **Non-leaf (Internal) nodes**: Contain keys and pointers to children but not actual data. - **Leaf nodes**: Contain the actual data records and keys. B-tree Properties: 1. **Uniform Path Length**: Every path from root to leaf is the same length. 2. **Key-Child Relationship**: If a node has n children, it contains exactly n−1 keys. 3. **Minimum Node Fill**: All nodes (except possibly the root) are at least half full. 4. **Ordered Subtrees**: Elements in subtrees fall between parent keys on either side of the subtree pointer, generalizing the binary search tree invariant. 5. **Root Constraint**: The root node must have at least two children if it’s not a leaf. Example from Provided Image (Order-5 B-tree): - The first image illustrates a B-tree of order 5 (each internal node can have up to 5 children). Here, nodes have multiple keys: Structure Breakdown: - Root Node: - Contains key '20', with two pointers: one to the left subtree and one to the right subtree. - Left Internal Node: - Keys: 11 and 15 - Three child pointers: - Leftmost pointer points to leaf node [3, 5, 7]. - Middle pointer points to leaf node [11, 12].\n",
      "Stored embedding for: leaf node [11, 12].\n",
      "Stored embedding for: - Right pointer points to leaf node [15, 16, 19]. - Right Internal Node: - Keys: 25, 30, 33, and 37 - Five child pointers: - [20, 22, 23], [25], [30, 31, 32], [33, 35], [37]. Insertion Operations (Illustrated in the second image): Step 1: Insert (13) - Insert key '13' into the leaf node [11, 12], resulting in [11, 12, 13]. No splitting is required since there's room. Step 2: Insert (14) - Leaf node [11, 12, 13] becomes full; inserting '14' causes splitting: - Left node: [11, 12] - Right node: [13, 14] - Update the parent node by adding '13' as a new separating key. Step 3: Insert (24) - The insertion of '24' into leaf node [20, 22, 23] causes overflow, splitting into: - Left node: [20, 22] - Right node: [23, 24] - Parent node becomes full after adding key '23', causing further split at the internal node level, promoting '30' to the root. After these operations, the tree height increases, maintaining B-tree invariants. Deletion Operations (General description): - Deletion is opposite insertion; removing keys from leaves and restructuring nodes to maintain minimum fill. - If nodes become underfilled (below half), sibling nodes are adjusted or merged, potentially reducing tree height. Benefits of B-trees: - Reduced number of disk reads due to high branching factor. - Improved cache utilization by matching node size to cache lines or disk blocks. Practical Use: - Databases and file systems frequently use B-trees due to their balanced nature and excellent performance in data retrieval and modification operations. Further Reading: - Aho, Hopcroft, and Ullman, *Data Structures and Algorithms*, Chapter 11 for in-\n",
      "Stored embedding for: depth theoretical and practical understanding.\n",
      " -----> Processed Notes on B-Trees.pdf\n",
      "Stored embedding for: DS 4300 Large Scale Information Storage and Retrieval B+ Tree Walkthrough Mark Fontenot, PhD Northeastern University\n",
      "Stored embedding for: 2 B+ Tree : m = 4 Insert: 42, 21, 63, 89 ●Initially, the ﬁrst node is a leaf node AND root node. ●21, 42, … represent keys of some set of K:V pairs ●Leaf nodes store keys and data, although data not shown ●Inserting another key will cause the node to split.\n",
      "Stored embedding for: 3 B+ Tree : m = 4 Insert: 35 ● Leaf node needs to split to accommodate 35. New leaf node allocated to the right of existing node ● 5/2 values stay in original node; remaining values moved to new node ● Smallest value from new leaf node (42) is copied up to the parent, which needs to be created in this case. It will be an internal node.\n",
      "Stored embedding for: 4 B+ Tree : m = 4 Insert: 10, 27, 96 ● The insert process starts at the root node. The keys of the root node are searched to ﬁnd out which child node we need to descend to. ○ EX: 10. Since 10 < 42, we follow the pointer to the left of 42 ● Note - none of these new values cause a node to split\n",
      "Stored embedding for: 5 B+ Tree : m = 4 Insert: 30 ● Starting at root, we descend to the left-most child (we’ll call curr). ○ curr is a leaf node. Thus, we insert 30 into curr. ○ BUT curr is full. So we have to split. ○ Create a new node to the right of curr, temporarily called newNode. ○ Insert newNode into the doubly linked list of leaf nodes.\n",
      "Stored embedding for: 6 B+ Tree : m = 4 Insert: 30 cont’d. ● re-distribute the keys ● copy the smallest key (27 in this case) from newNode to parent; rearrange keys and pointers in parent node. ● Parent of newNode is also root. So, nothing else to do.\n",
      "Stored embedding for: 7 B+ Tree : m = 4 Fast forward to this state of the tree… ● Observation: The root node is full. ○ The next insertion that splits a leaf will cause the root to split, and thus the tree will get 1 level deeper.\n",
      "Stored embedding for: 8 B+ Tree : m = 4 Insert 37. Step 1\n",
      "Stored embedding for: 9 B+ Tree : m = 4 Insert 37. Step 2. ● When splitting an internal node, we move the middle element to the parent (instead of copying it). ● In this particular tree, that means we have to create a new internal node which is also now the root.\n",
      " -----> Processed 04-B+Tree Walkthrough.pdf\n",
      "Stored embedding for: ● ● ● ● ● ● ● ● ● ● ○ ○ ● ● ● ○ ○ ● ● ● ○ ○ ● B+ Tree Walkthrough Mark Fontenot Introduction to B+ Trees A B+ tree is a balanced tree data structure commonly used in databases and file systems to efficiently store and retrieve data. In a B+ tree, all data values are stored in the leaf nodes, while internal nodes only contain keys and pointers for navigation. Key Features: Nodes contain keys and pointers. Data is stored in leaf nodes. Internal nodes act solely as index structures. Efficiently supports insertions, deletions, and searches. Example Walkthrough B+ Tree (Order m = 4) We start by inserting the keys: Insert: 42, 21, 63, 89 Initially, a single node acts as both the root and leaf node. Keys are inserted in sorted order: [21, 42, 63, 89]. Each leaf node stores keys and associated data (data is omitted here for simplicity). Any further insertion into this node would require a split, as the node is at maximum capacity. Insert: 35 The leaf node is full; we must split it. A new leaf node is created to the right, and keys are redistributed: Original node: [21, 35] New node: [42, 63, 89] The smallest value from the new node (42) is copied up to create a new internal parent node. The structure now has an internal root node and two leaf nodes. Insert: 10, 27, 96 Begin at root (42). Each key directs us to a leaf node: 10 < 42; insert into left leaf: [10, 21, 27, 35]. 96 > 42; insert into right leaf: [42, 63, 89, 96]. No splits required, as both leaf nodes accommodate the insertions. Insert: 30 Navigate to the left leaf node, which is full ([10, 21, 27, 35]). Node splits, creating a new leaf node: Original node: [10, 21] New node: [27, 30, 35] Update leaf pointers to maintain a doubly linked list.\n",
      "Stored embedding for: 35]). Node splits, creating a new leaf node: Original node: [10, 21] New node: [27, 30, 35] Update leaf pointers to maintain a doubly linked list.\n",
      "Stored embedding for: ● ● ● ● ○ ○ ● ● ● ○ ○ ○ ● Smallest key of the new node (27) is copied up to the internal parent node. Internal root node now contains [27, 42]. Advanced Insertion Example (Fast Forward) The tree has grown, and the root node is at full capacity ([10, 27, 42, 63]). Additional insertions will deepen the tree structure. Insert: 37 (Step 1) Navigate to correct leaf node, which is full ([27, 30, 35, 38]). Node splits: Original node: [27, 30] New node: [35, 37, 38] Smallest key (35) must be copied to the parent node. Parent node ([10, 27, 42, 63]) is also full, requiring further handling. Insert: 37 (Step 2 – Splitting Internal Node) Internal node splits by moving middle key (35) upwards: Left node: [10, 27] Right node: [42, 63] New root created with key [35]. Tree height increases by one. Final Structure The tree is now deeper, balancing insertions efficiently across nodes. This walkthrough illustrates how a B+ tree maintains balance through splits and re- distributions, ensuring operations remain efficient even as data scales.\n",
      " -----> Processed B+ Tree Walkthrough Mark Fontenot.pdf\n",
      "Stored embedding for: DS 4300 Large Scale Information Storage and Retrieval Mark Fontenot, PhD Northeastern University\n",
      "Stored embedding for: Hi! 󰗞 ●Mark Fontenot, PhD ○ Ofﬁce: 353 Meserve Hall ○ Ofﬁce Hours: ■ M & Th 1:30 - 3:00 pm (If those times don’t work, just DM me on Slack to set up an alternate time!) ○ Usually, very available on Slack… so just DM me. ●m.fontenot@northeastern.edu 2\n",
      "Stored embedding for: Teaching Assistants 3 Iker Acosta Venegas Dallon Archibald Nathan Cheung Aryan Jain Abhishek Kumar Eddy Liu Sevinch Noori Junxiang Lin\n",
      "Stored embedding for: Where do I ﬁnd … ? ●Course materials (Notes, Assignments, etc): https://markfontenot.net/teaching/ds4300/25s-ds4300/ ●Assignment submissions (and grades): GradeScope ●Q & A Platform is CampusWire ●Quick DMs and Announcements will be on Slack 4\n",
      "Stored embedding for: What’s this class about? ●By the end of this class, you should ○ Understand the efﬁciency-related concepts (including limitations) of RDBMSs ○ Understand data replication and distribution effects on typical DB usage scenarios ○ Understand the use cases for and data models of various NoSQL database systems, including storing and retrieving data. Data models include document-based, key-value stores, graph based among others. ○ Access and implement data engineering and big-data-related AWS services 5\n",
      "Stored embedding for: Course Deliverables and Evaluation 6\n",
      "Stored embedding for: Assignments ●Homeworks and Practicals ○ Usually due Tuesday Nights at 11:59 unless otherwise stated ○ 3% Bonus for submitting 48 hours early. (No… you can’t get > 3% for submitting >48 hours early) ○ No Late Submissions accepted! ■ But… life happens… So everyone gets 1 free, no-questions-asked 48 hour extension. ■ DM Dr. Fontenot on Slack sometime before the original deadline requesting to use your extension. 7\n",
      "Stored embedding for: Assignments ●Submissions will be via GradeScope and/or GitHub (unless directed otherwise) ○ Only submit PDFs unless otherwise instructed. ○ If only submitting a PDF, be sure to associate questions in gradescope with the correct page in your PDF. ■ Failure to do so may result in a grade of 0 on the assignment. ●All regrade requests must be submitted within 48 hours of grades being released on GradeScope. No Exceptions. 8\n",
      "Stored embedding for: Midterm Monday, March 17 Mark it in your calendars now! 9\n",
      "Stored embedding for: Final Grade Breakdown ●Homeworks (5) 30% ●Practicals (2) 20% ●Midterm 20% ●Semester Project 30% 10\n",
      "Stored embedding for: Reference Materials Primary Resources 11 O’Reilly Playlist Other books are in the playlist. I will add additional materials to the playlist or webpage as the semester progresses.\n",
      "Stored embedding for: Tentative List of Topics ●Thinking about data storage and retrieval at the data structures level ●How far can we get with the relational model? ●NoSQL Databases ○ Document Databases (Mongo) ○ Graph Databases (Neo4j) ○ Key/Value Databases ○ Maybe Vector Databases ●Data Distribution and Replication ●Distributed SQL DBs & Apache Spark/SparkSQL ●Big Data Tools and Services on AWS 12\n",
      "Stored embedding for: Tools You Will Need to Install on your Laptop ●Docker Desktop ●Anaconda or Miniconda Python ○ You’re welcome to use another distro, but you’re responsible for ﬁxing it if something doesn’t work (dependency conﬂicts, etc.) ●A Database Access tool like Datagrip or DBeaver ●VS Code set up for Python Development ○ See > here < for more info about VSCode, Python, and Anaconda ●Ability to interact with git and GitHub through terminal or GUI app. 13\n",
      "Stored embedding for: Topics to Review over the Next Few Days - Shell/cmd Prompt/PowerShell CLI - Windows - if you want a Unix terminal: WSL2 or zsh on Windows - navigating the ﬁle system - running commands like pip, conda, python, etc - command line args - Docker & Docker Compose - Basics of Dockerﬁles and docker-compose.yaml ﬁles - port mapping - setting up volumes & mapping between host and guest OS 14\n",
      "Stored embedding for: Is your Python Rusty or Haven’t Done a ton with it? - Python Crash Course by Net Ninja on YT - On O’Reilly (See Python section of class playlist): - Python - Object-Oriented Programming Video Course by Simon Sez IT - E. Matthes - Python Crash Course, 3rd Edition - No Starch Press (not related to the YT video playlist listed above) 15\n",
      "Stored embedding for: Expectations ●Conduct yourself respectfully ●Don’t distract your classmates from learning ●Don’t cheat!! ○ Do your own work unless group assignment ○ Discussing problems is encouraged, but you must formulate your own solutions ○ See Syllabus for details! 16\n",
      "Stored embedding for: Let’s GOOO! 17\n",
      " -----> Processed 01 - Introduction & Getting Started.pdf\n",
      "Stored embedding for: DS 4300 Moving Beyond the Relational Model Mark Fontenot, PhD Northeastern University\n",
      "Stored embedding for: Beneﬁts of the Relational Model - (Mostly) Standard Data Model and Query Language - ACID Compliance (more on this in a second) - Atomicity, Consistency, Isolation, Durability - Works well will highly structured data - Can handle large amounts of data - Well understood, lots of tooling, lots of experience 2\n",
      "Stored embedding for: Relational Database Performance Many ways that a RDBMS increases efﬁciency: - indexing (the topic we focused on) - directly controlling storage - column oriented storage vs row oriented storage - query optimization - caching/prefetching - materialized views - precompiled stored procedures - data replication and partitioning 3\n",
      "Stored embedding for: Transaction Processing - Transaction - a sequence of one or more of the CRUD operations performed as a single, logical unit of work - Either the entire sequence succeeds (COMMIT) - OR the entire sequence fails (ROLLBACK or ABORT) - Help ensure - Data Integrity - Error Recovery - Concurrency Control - Reliable Data Storage - Simpliﬁed Error Handling 4\n",
      "Stored embedding for: ACID Properties - Atomicity - transaction is treated as an atomic unit - it is fully executed or no parts of it are executed - Consistency - a transaction takes a database from one consistent state to another consistent state - consistent state - all data meets integrity constraints 5\n",
      "Stored embedding for: ACID Properties - Isolation - Two transactions T1 and T2 are being executed at the same time but cannot affect each other - If both T1 and T2 are reading the data - no problem - If T1 is reading the same data that T2 may be writing, can result in: - Dirty Read - Non-repeatable Read - Phantom Reads 6\n",
      "Stored embedding for: Isolation: Dirty Read 7 Figure from: https://www.mybluelinux.com/relational-databases-explained/ Dirty Read - a transaction T1 is able to read a row that has been modiﬁed by another transaction T2 that hasn’t yet executed a COMMIT\n",
      "Stored embedding for: Isolation: Non-Repeatable Read 8 Figure from: https://www.mybluelinux.com/relational-databases-explained/ Non-repeatable Read - two queries in a single transaction T1 execute a SELECT but get different values because another transaction T2 has changed data and COMMITTED\n",
      "Stored embedding for: Isolation: Phantom Reads 9 Figure from: https://www.mybluelinux.com/relational-databases-explained/ Phantom Reads - when a transaction T1 is running and another transaction T2 adds or deletes rows from the set T1 is using\n",
      "Stored embedding for: Example Transaction - Transfer $$ 10 DELIMITER // CREATE PROCEDURE transfer( IN sender_id INT, IN receiver_id INT, IN amount DECIMAL(10,2) ) BEGIN DECLARE rollback_message VARCHAR(255) DEFAULT 'Transaction rolled back: Insufficient funds'; DECLARE commit_message VARCHAR(255) DEFAULT 'Transaction committed successfully'; -- Start the transaction START TRANSACTION; -- Attempt to debit money from account 1 UPDATE accounts SET balance = balance - amount WHERE account_id = sender_id; -- Attempt to credit money to account 2 UPDATE accounts SET balance = balance + amount WHERE account_id = receiver_id; -- Continued Next Slide\n",
      "Stored embedding for: Example Transaction - Transfer $$ 11 -- Continued from previous slide -- Check if there are sufficient funds in account 1 -- Simulate a condition where there are insufficient funds IF (SELECT balance FROM accounts WHERE account_id = sender_id) < 0 THEN -- Roll back the transaction if there are insufficient funds ROLLBACK; SIGNAL SQLSTATE '45000' -- 45000 is unhandled, user-defined error SET MESSAGE_TEXT = rollback_message; ELSE -- Log the transactions if there are sufficient funds INSERT INTO transactions (account_id, amount, transaction_type) VALUES (sender_id, -amount, 'WITHDRAWAL'); INSERT INTO transactions (account_id, amount, transaction_type) VALUES (receiver_id, amount, 'DEPOSIT'); -- Commit the transaction COMMIT; SELECT commit_message AS 'Result'; END IF; END // DELIMITER ;\n",
      "Stored embedding for: ACID Properties - Durability - Once a transaction is completed and committed successfully, its changes are permanent. - Even in the event of a system failure, committed transactions are preserved - For more info on Transactions, see: - Kleppmann Book Chapter 7 12\n",
      "Stored embedding for: But … Relational Databases may not be the solution to all problems… - sometimes, schemas evolve over time - not all apps may need the full strength of ACID compliance - joins can be expensive - a lot of data is semi-structured or unstructured (JSON, XML, etc) - Horizontal scaling presents challenges - some apps need something more performant (real time, low latency systems) 13\n",
      "Stored embedding for: Scalability - Up or Out? Conventional Wisdom: Scale vertically (up, with bigger, more powerful systems) until the demands of high-availability make it necessary to scale out with some type of distributed computing model But why? Scaling up is easier - no need to really modify your architecture. But there are practical and ﬁnancial limits However: There are modern systems that make horizontal scaling less problematic. 14\n",
      "Stored embedding for: So what? Distributed Data when Scaling Out A distributed system is “a collection of independent computers that appear to its users as one computer.” -Andrew Tennenbaum Characteristics of Distributed Systems: - computers operate concurrently - computers fail independently - no shared global clock 15\n",
      "Stored embedding for: Distributed Storage - 2 Directions 16 Single Main Node\n",
      "Stored embedding for: Distributed Data Stores - Data is stored on > 1 node, typically replicated - i.e. each block of data is available on N nodes - Distributed databases can be relational or non-relational - MySQL and PostgreSQL support replication and sharding - CockroachDB - new player on the scene - Many NoSQL systems support one or both models - But remember: Network partitioning is inevitable! - network failures, system failures - Overall system needs to be Partition Tolerant - System can keep running even w/ network partition 17\n",
      "Stored embedding for: The CAP Theorem 18\n",
      "Stored embedding for: The CAP Theorem 19 The CAP Theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees: - Consistency - Every read receives the most recent write or error thrown - Availability - Every request receives a (non-error) response - but no guarantee that the response contains the most recent write - Partition Tolerance - The system can continue to operate despite arbitrary network issues.\n",
      "Stored embedding for: CAP Theorem - Database View 20 Reference: https://alperenbayramoglu.com/posts/understanding-cap-theorem/ - Consistency*: Every user of the DB has an identical view of the data at any given instant - Availability: In the event of a failure, the database remains operational - Partition Tolerance: The database can maintain operations in the event of the network’s failing between two segments of the distributed system * Note, the definition of Consistency in CAP is different from that of ACID.\n",
      "Stored embedding for: CAP Theorem - Database View 21 Reference: https://alperenbayramoglu.com/posts/understanding-cap-theorem/ - Consistency + Availability: System always responds with the latest data and every request gets a response, but may not be able to deal with network issues - Consistency + Partition Tolerance: If system responds with data from a distributed store, it is always the latest, else data request is dropped. - Availability + Partition Tolerance: System always sends are responds based on distributed store, but may not be the absolute latest data.\n",
      "Stored embedding for: CAP in Reality What it is really saying: - If you cannot limit the number of faults, requests can be directed to any server, and you insist on serving every request, then you cannot possibly be consistent. But it is interpreted as: - You must always give up something: consistency, availability, or tolerance to failure. 22\n",
      "Stored embedding for: ?? 23\n",
      " -----> Processed 03 - Moving Beyond the Relational Model.pdf\n",
      "Stored embedding for: 3/16/25, 9:37 PM ICS 46 Spring 2022, Notes and Examples: AVL Trees Page 1 of 7 https://ics.uci.edu/~thornton/ics46/Notes/AVLTrees/ ICS 46 Spring 2022 | News | Course Reference | Schedule | Project Guide | Notes and Examples | Reinforcement Exercises | Grade Calculator | About Alex ICS 46 Spring 2022 Notes and Examples: AVL Trees Why we must care about binary search tree balancing We've seen previously that the performance characteristics of binary search trees can vary rather wildly, and that they're mainly dependent on the shape of the tree, with the height of the tree being the key determining factor. By definition, binary search trees restrict what keys are allowed to present in which nodes — smaller keys have to be in left subtrees and larger keys in right subtrees — but they specify no restriction on the tree's shape, meaning that both of these are perfectly legal binary search trees containing the keys 1, 2, 3, 4, 5, 6, and 7. Yet, while both of these are legal, one is better than the other, because the height of the first tree (called a perfect binary tree) is smaller than the height of the second (called a degenerate tree). These two shapes represent the two extremes — the best and worst possible shapes for a binary search tree containing seven keys. Of course, when all you have is a very small number of keys like this, any shape will do. But as the number of keys grows, the distinction between these two tree shapes becomes increasingly vital. What's more, the degenerate shape isn't even necessarily a rare edge case: It's what you get when you start with an empty tree and add keys that are already in order, which is a surprisingly common scenario in real-world programs. For example, one very obvious algorithm for generating unique integer keys — when all you care about is that they're unique — is to generate them sequentially. What's so bad about a degenerate tree, anyway? Just looking at a picture of a degenerate tree, your intuition should already be telling you that something is amiss. In particular, if you tilt your head 45 degrees to the right, they look just like linked lists; that perception is no accident, as they behave like them, too (except that they're more complicated, to boot!). From a more analytical perspective, there are three results that should give us\n",
      "Stored embedding for: obvious algorithm for generating unique integer keys — when all you care about is that they're unique — is to generate them sequentially. What's so bad about a degenerate tree, anyway? Just looking at a picture of a degenerate tree, your intuition should already be telling you that something is amiss. In particular, if you tilt your head 45 degrees to the right, they look just like linked lists; that perception is no accident, as they behave like them, too (except that they're more complicated, to boot!). From a more analytical perspective, there are three results that should give us pause: Every time you perform a lookup in a degenerate binary search tree, it will take O(n) time, because it's possible that you'll have to reach every node in the tree before you're done. As n grows, this is a heavy burden to bear. If you implement your lookup recursively, you might also be using O(n) memory, too, as you might end up with as many as n frames on your run-time stack — one for every recursive call. There are ways to mitigate this — for example, some kinds of carefully-written recursion (in some programming languages, including C++) can avoid run-time stack growth as you recurse — but it's still a sign of potential trouble. The time it will take you to build the degenerate tree will also be prohibitive. If you start with an empty binary search tree and add keys to it in order, how long does it take to do it? The first key you add will go directly to the root. You could think of this as taking a single step: creating the node. The second key you add will require you to look at the root node, then take one step to the right. You could think of this as taking two steps. Each subsequent key you add will require one more step than the one before it. The total number of steps it would take to add n keys would be determined by the sum 1 + 2 + 3 + ... + n. This sum, which we'll see several times throughout this course, is equal to n(n + 1) / 2. So, the total number of steps to build the entire tree would be Θ(n2). Overall, when n gets large, the tree would be hideously expensive to build, and then every subsequent\n",
      "Stored embedding for: You could think of this as taking two steps. Each subsequent key you add will require one more step than the one before it. The total number of steps it would take to add n keys would be determined by the sum 1 + 2 + 3 + ... + n. This sum, which we'll see several times throughout this course, is equal to n(n + 1) / 2. So, the total number of steps to build the entire tree would be Θ(n2). Overall, when n gets large, the tree would be hideously expensive to build, and then every subsequent search would be painful, as well. So this, in general, is a situation we need to be sure to avoid, or else we should probably consider a data structure other than a binary search tree; the worst case is simply too much of a burden to bear if n might get large. But if we can find a way to control the tree's shape more carefully, to force it to remain more balanced, we'll be fine. The question, of course, is how to do it, and, as importantly, whether we can do it while keeping the cost low enough that it doesn't outweigh the benefit.\n",
      "Stored embedding for: 3/16/25, 9:37 PM ICS 46 Spring 2022, Notes and Examples: AVL Trees Page 2 of 7 https://ics.uci.edu/~thornton/ics46/Notes/AVLTrees/ Aiming for perfection The best goal for us to shoot for would be to maintain perfection. In other words, every time we insert a key into our binary search tree, it would ideally still be a perfect binary tree, in which case we'd know that the height of the tree would always be Θ(log n), with a commensurate effect on performance. However, when we consider this goal, a problem emerges almost immediately. The following are all perfect binary trees, by definition: The perfect binary trees pictured above have 1, 3, 7, and 15 nodes respectively, and are the only possible perfect shapes for binary trees with that number of nodes. The problem, though, lies in the fact that there is no valid perfect binary tree with 2 nodes, or with 4, 5, 6, 8, 9, 10, 11, 12, 13, or 14 nodes. So, generally, it's impossible for us to guarantee that a binary search tree will always be \"perfect,\" by our definition, because there's simply no way to represent most numbers of keys. So, first thing's first: We'll need to relax our definition of \"perfection\" to accommodate every possible number of keys we might want to store. Complete binary trees A somewhat more relaxed notion of \"perfection\" is something called a complete binary tree, which is defined as follows. A complete binary tree of height h is a binary tree where: If h = 0, its left and right subtrees are empty. If h > 0, one of two things is true: The left subtree is a perfect binary tree of height h − 1 and the right subtree is a complete binary tree of height h − 1 The left subtree is a complete binary tree of height h − 1 and the right subtree is a perfect binary tree of height h − 2 That can be a bit of a mind-bending definition, but it actually leads to a conceptually simple result: On every level of a complete binary tree, every node that could possibly be present will be, except the last level might be missing nodes, but if it is missing nodes, the nodes that are there will be as far to the left as possible. The following are all complete binary trees: Furthermore, these are the only possible complete\n",
      "Stored embedding for: a complete binary tree of height h − 1 and the right subtree is a perfect binary tree of height h − 2 That can be a bit of a mind-bending definition, but it actually leads to a conceptually simple result: On every level of a complete binary tree, every node that could possibly be present will be, except the last level might be missing nodes, but if it is missing nodes, the nodes that are there will be as far to the left as possible. The following are all complete binary trees: Furthermore, these are the only possible complete binary trees with these numbers of nodes in them; any other arrangement of, say, 6 keys besides the one shown above would violate the definition. We've seen that the height of a perfect binary tree is Θ(log n). It's not a stretch to see that the height a complete binary tree will be Θ(log n), as well, and we'll accept that via our intuition for now and proceed. All in all, a complete binary tree would be a great goal for us to attain: If we could keep the shape of our binary search trees complete, we would always have binary search trees with height Θ(log n). The cost of maintaining completeness The trouble, of course, is that we need an algorithm for maintaining completeness. And before we go to the trouble of trying to figure one out, we should consider whether it's even worth our time. What can we deduce about the cost of maintaining completeness, even if we haven't figured out an algorithm yet? One example demonstrates a very big problem. Suppose we had the binary search tree on the left — which is complete, by our definition — and we wanted to insert the key 1 into it. If so, we would need an algorithm that would transform the tree on the left into the tree on the right.\n",
      "Stored embedding for: it. If so, we would need an algorithm that would transform the tree on the left into the tree on the right.\n",
      "Stored embedding for: 3/16/25, 9:37 PM ICS 46 Spring 2022, Notes and Examples: AVL Trees Page 3 of 7 https://ics.uci.edu/~thornton/ics46/Notes/AVLTrees/ The tree on the right is certainly complete, so this would be the outcome we'd want. But consider what it would take to do it. Every key in the tree had to move! So, no matter what algorithm we used, we would still have to move every key. If there are n keys in the tree, that would take Ω(n) time — moving n keys takes at least linear time, even if you have the best possible algorithm for moving them; the work still has to get done. So, in the worst case, maintaining completeness after a single insertion requires Ω(n) time. Unfortunately, this is more time than we ought to be spending on maintaining balance. This means we'll need to come up with a compromise; as is often the case when we learn or design algorithms, our willingness to tolerate an imperfect result that's still \"good enough\" for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result. So what would a \"good enough\" result be? What is a \"good\" balance condition Our overall goal is for lookups, insertions, and removals from a binary search tree to require O(log n) time in every case, rather than letting them degrade to a worst-case behavior of O(n). To do that, we need to decide on a balance condition, which is to say that we need to understand what shape is considered well- enough balanced for our purposes, even if not perfect. A \"good\" balance condition has two properties: The height of a binary search tree meeting the condition is Θ(log n). It takes O(log n) time to re-balance the tree on insertions and removals. In other words, it guarantees that the height of the tree is still logarithmic, which will give us logarithmic-time lookups, and the time spent re-balancing won't exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height. The cost won't outweigh the benefit. Coming up with a balance condition like this on our own is a tall task, but we can stand on the shoulders of the giants who came before us, with the definition above helping to guide us toward an understanding of whether we've found what we're looking for. A compromise:\n",
      "Stored embedding for: other words, it guarantees that the height of the tree is still logarithmic, which will give us logarithmic-time lookups, and the time spent re-balancing won't exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height. The cost won't outweigh the benefit. Coming up with a balance condition like this on our own is a tall task, but we can stand on the shoulders of the giants who came before us, with the definition above helping to guide us toward an understanding of whether we've found what we're looking for. A compromise: AVL trees There are a few well-known approaches for maintaining binary search trees in a state of near-balance that meets our notion of a \"good\" balance condition. One of them is called an AVL tree, which we'll explore here. Others, which are outside the scope of this course, include red-black trees (which meet our definition of \"good\") and splay trees (which don't always meet our definition of \"good\", but do meet it on an amortized basis), but we'll stick with the one solution to the problem for now. AVL trees AVL trees are what you might called \"nearly balanced\" binary search trees. While they certainly aren't as perfectly-balanced as possible, they nonetheless achieve the goals we've decided on: maintaining logarithmic height at no more than logarithmic cost. So, what makes a binary search tree \"nearly balanced\" enough to be considered an AVL tree? The core concept is embodied by something called the AVL property. We say that a node in a binary search tree has the AVL property if the heights of its left and right subtrees differ by no more than 1. In other words, we tolerate a certain amount of imbalance — heights of subtrees can be slightly different, but no more than that — in hopes that we can more efficiently maintain it. Since we're going to be comparing heights of subtrees, there's one piece of background we need to consider. Recall that the height of a tree is the length of its longest path. By definition, the height of a tree with just a root node (and empty subtrees) would then be zero. But what about a tree that's totally empty? To maintain a clear pattern, relative to other tree heights, we'll say that the height of an empty tree is -1. This means that a\n",
      "Stored embedding for: different, but no more than that — in hopes that we can more efficiently maintain it. Since we're going to be comparing heights of subtrees, there's one piece of background we need to consider. Recall that the height of a tree is the length of its longest path. By definition, the height of a tree with just a root node (and empty subtrees) would then be zero. But what about a tree that's totally empty? To maintain a clear pattern, relative to other tree heights, we'll say that the height of an empty tree is -1. This means that a node with, say, a childless left child and no right child would still be considered balanced. This leads us, finally, to the definition of an AVL tree: An AVL tree is a binary search tree in which all nodes have the AVL property. Below are a few binary trees, two of which are AVL and two of which are not.\n",
      "Stored embedding for: 3/16/25, 9:37 PM ICS 46 Spring 2022, Notes and Examples: AVL Trees Page 4 of 7 https://ics.uci.edu/~thornton/ics46/Notes/AVLTrees/ The thing to keep in mind about AVL is that it's not a matter of squinting at a tree and deciding whether it \"looks\" balanced. There's a precise definition, and the two trees above that don't meet that definition fail to meet it because they each have at least one node (marked in the diagrams by a dashed square) that doesn't have the AVL property. AVL trees, by definition, are required to meet the balance condition after every operation; every time you insert or remove a key, every node in the tree should have the AVL property. To meet that requirement, we need to restructure the tree periodically, essentially detecting and correcting imbalance whenever and wherever it happens. To do that, we need to rearrange the tree in ways that improve its shape without losing the essential ordering property of a binary search tree: smaller keys toward the left, larger ones toward the right. Rotations Re-balancing of AVL trees is achieved using what are called rotations, which, when used at the proper times, efficiently improve the shape of the tree by altering a handful of pointers. There are a few kinds of rotations; we should first understand how they work, then focus our attention on when to use them. The first kind of rotation is called an LL rotation, which takes the tree on the left and turns it into the tree on the right. The circle with A and B written in them are each a single node containing a single key; the triangles with T1, T2, and T3 written in them are arbitrary subtrees, which may be empty or may contain any number of nodes (but which are, themselves, binary search trees). It's important to remember that both of these trees — before and after — are binary search trees; the rotation doesn't harm the ordering of the keys in nodes, because the subtrees T1, T2, and T3 maintain the appropriate positions relative to the keys A and B: All keys in T1 are smaller than A. All keys in T2 are larger than A and smaller than B. All keys in T3 are larger than B. Performing this rotation would be a simple matter of adjusting a few pointers — notably, a constant number of pointers, no matter how many\n",
      "Stored embedding for: trees). It's important to remember that both of these trees — before and after — are binary search trees; the rotation doesn't harm the ordering of the keys in nodes, because the subtrees T1, T2, and T3 maintain the appropriate positions relative to the keys A and B: All keys in T1 are smaller than A. All keys in T2 are larger than A and smaller than B. All keys in T3 are larger than B. Performing this rotation would be a simple matter of adjusting a few pointers — notably, a constant number of pointers, no matter how many nodes are in the tree, which means that this rotation would run in Θ(1) time: B's parent would now point to A where it used to point to B A's right child would now be B instead of the root of T2 B's left child would now be the root of T2 instead of A A second kind of rotation is an RR rotation, which makes a similar adjustment. Note that an RR rotation is the mirror image of an LL rotation. A third kind of rotation is an LR rotation, which makes an adjustment that's slightly more complicated.\n",
      "Stored embedding for: 3/16/25, 9:37 PM ICS 46 Spring 2022, Notes and Examples: AVL Trees Page 5 of 7 https://ics.uci.edu/~thornton/ics46/Notes/AVLTrees/ An LR rotation requires five pointer updates instead of three, but this is still a constant number of changes and runs in Θ(1) time. Finally, there is an RL rotation, which is the mirror image of an LR rotation. Once we understand the mechanics of how rotations work, we're one step closer to understanding AVL trees. But these rotations aren't arbitrary; they're used specifically to correct imbalances that are detected after insertions or removals. An insertion algorithm Inserting a key into an AVL tree starts out the same way as insertion into a binary search tree: Perform a lookup. If you find the key already in the tree, you're done, because keys in a binary search tree must be unique. When the lookup terminates without the key being found, add a new node in the appropriate leaf position where the lookup ended. The problem is that adding the new node introduced the possibility of an imbalance. For example, suppose we started with this AVL tree: and then we inserted the key 35 into it. A binary search tree insertion would give us this as a result:\n",
      "Stored embedding for: 3/16/25, 9:37 PM ICS 46 Spring 2022, Notes and Examples: AVL Trees Page 6 of 7 https://ics.uci.edu/~thornton/ics46/Notes/AVLTrees/ But this resulting tree is not an AVL tree, because the node containing the key 40 does not have the AVL property, because the difference in the heights of its subtrees is 2. (Its left subtree has height 1, its right subtree — which is empty — has height -1.) What can we do about it? The answer lies in the following algorithm, which we perform after the normal insertion process: Work your way back up the tree from the position where you just added a node. (This could be quite simple if the insertion was done recursively.) Compare the heights of the left and right subtrees of each node. When they differ by more than 1, choose a rotation that will fix the imbalance. Note that comparing the heights of the left and right subtrees would be quite expensive if you didn't already know what they were. The solution to this problem is for each node to store its height (i.e., the height of the subtree rooted there). This can be cheaply updated after every insertion or removal as you unwind the recursion. The rotation is chosen considering the two links along the path below the node where the imbalance is, heading back down toward where you inserted a node. (If you were wondering where the names LL, RR, LR, and RL come from, this is the answer to that mystery.) If the two links are both to the left, perform an LL rotation rooted where the imbalance is. If the two links are both to the right, perform an RR rotation rooted where the imbalance is. If the first link is to the left and the second is to the right, perform an LR rotation rooted where the imbalance is. If the first link is to the right and the second is to the left, perform an RL rotation rooted where the imbalance is. It can be shown that any one of these rotations — LL, RR, LR, or RL — will correct any imbalance brought on by inserting a key. In this case, we'd perform an LR rotation — the first two links leading from 40 down toward 35 are a Left and a Right — rooted at 40, which would correct the imbalance, and the tree would be rearranged\n",
      "Stored embedding for: perform an LR rotation rooted where the imbalance is. If the first link is to the right and the second is to the left, perform an RL rotation rooted where the imbalance is. It can be shown that any one of these rotations — LL, RR, LR, or RL — will correct any imbalance brought on by inserting a key. In this case, we'd perform an LR rotation — the first two links leading from 40 down toward 35 are a Left and a Right — rooted at 40, which would correct the imbalance, and the tree would be rearranged to look like this: Compare this to the diagram describing an LR rotation: The node containing 40 is C The node containing 30 is A The node containing 35 is B The (empty) left subtree of the node containing 30 is T1 The (empty) left subtree of the node containing 35 is T2 The (empty) right subtree of the node containing 35 is T3 The (empty) right subtree of the node containing 40 is T4 After the rotation, we see what we'd expect: The node B, which in our example contained 35, is now the root of the newly-rotated subtree The node A, which in our example contained 30, is now the left child of the root of the newly-rotated subtree The node C, which in our example contained 40, is now the right child of the root of the newly-rotated subtree The four subtrees T1, T2, T3, and T4 were all empty, so they are still empty. Note, too, that the tree is more balanced after the rotation than it was before. This is no accident; a single rotation (LL, RR, LR, or RL) is all that's necessary to correct an imbalance introduced by the insertion algorithm. A removal algorithm Removals are somewhat similar to insertions, in the sense that you would start with the usual binary search tree removal algorithm, then find and correct imbalances while the recursion unwinds. The key difference is that removals can require more than one rotation to correct imbalances, but will still only require rotations on the path back up to the root from where the removal occurred — so, generally, O(log n) rotations. Asymptotic analysis The key question here is What is the height of an AVL tree with n nodes? If the answer is Θ(log n), then we can be certain\n",
      "Stored embedding for: algorithm Removals are somewhat similar to insertions, in the sense that you would start with the usual binary search tree removal algorithm, then find and correct imbalances while the recursion unwinds. The key difference is that removals can require more than one rotation to correct imbalances, but will still only require rotations on the path back up to the root from where the removal occurred — so, generally, O(log n) rotations. Asymptotic analysis The key question here is What is the height of an AVL tree with n nodes? If the answer is Θ(log n), then we can be certain that lookups, insertions, and removals will take O(log n) time. How can we be so sure? Lookups would be O(log n) because they're the same as they are in a binary search tree that doesn't have the AVL property. If the height of the tree is Θ(log n), lookups will run in O(log n) time. Insertions and removals, despite being slightly more complicated in an AVL tree, do their work by traversing a single path in the tree — potentially all the way down to a leaf position, then all the way back up. If the length of the longest path — that's what the height of a tree is! — is Θ(log n), then we know that none of these paths is longer than that, so insertions and removals will take O(log n) time.\n",
      "Stored embedding for: 3/16/25, 9:37 PM ICS 46 Spring 2022, Notes and Examples: AVL Trees Page 7 of 7 https://ics.uci.edu/~thornton/ics46/Notes/AVLTrees/ So we're left with that key question. What is the height of an AVL tree with n nodes? (If you're not curious, you can feel free to just assume this; if you want to know more, keep reading.) What is the height of an AVL tree with n nodes? (Optional) The answer revolves around noting how many nodes, at minimum, could be in a binary search tree of height n and still have it be an AVL tree. It turns out AVL trees of height n ≥ 2 that have the minimum number of nodes in them all share a similar property: The AVL tree with height h ≥ 2 with the minimum number of nodes consists of a root node with two subtrees, one of which is an AVL tree with height h − 1 with the minimum number of nodes, the other of which is an AVL tree with height h − 2 with the minimum number of nodes. Given that observation, we can write a recurrence that describes the number of nodes, at minimum, in an AVL tree of height h. M(0) = 1 When height is 0, minimum number of nodes is 1 (a root node with no children) M(1) = 2 When height is 1, minimum number of nodes is 2 (a root node with one child and not the other) M(h) = 1 + M(h - 1) + M(h - 2) While the repeated substitution technique we learned previously isn't a good way to try to solve this particular recurrence, we can prove something interesting quite easily. We know for sure that AVL trees with larger heights have a bigger minimum number of nodes than AVL trees with smaller heights — that's fairly self-explanatory — which means that we can be sure that 1 + M(h − 1) ≥ M(h − 2). Given that, we can conclude the following: M(h) ≥ 2M(h - 2) We can then use the repeated substitution technique to determine a lower bound for this recurrence: M(h) ≥ 2M(h - 2) ≥ 2(2M(h - 4)) ≥ 4M(h - 4) ≥ 4(2M(h - 6)) ≥ 8M(h - 6) ... ≥ 2jM(h - 2j) We could prove this by induction on j, but we'll accept it on faith let j = h/2 ≥ 2h/2M(h -\n",
      "Stored embedding for: with smaller heights — that's fairly self-explanatory — which means that we can be sure that 1 + M(h − 1) ≥ M(h − 2). Given that, we can conclude the following: M(h) ≥ 2M(h - 2) We can then use the repeated substitution technique to determine a lower bound for this recurrence: M(h) ≥ 2M(h - 2) ≥ 2(2M(h - 4)) ≥ 4M(h - 4) ≥ 4(2M(h - 6)) ≥ 8M(h - 6) ... ≥ 2jM(h - 2j) We could prove this by induction on j, but we'll accept it on faith let j = h/2 ≥ 2h/2M(h - h) ≥ 2h/2M(0) M(h) ≥ 2h/2 So, we've shown that the minimum number of nodes that can be present in an AVL tree of height h is at least 2h/2. In reality, it's actually more than that, but this gives us something useful to work with; we can use this result to figure out what we're really interested in, which is the opposite: what is the height of an AVL tree with n nodes? M(h) ≥ 2h/2 log2M(h) ≥ h/2 2 log2M(h) ≥ h Finally, we see that, for AVL trees of height h with the minimum number of nodes, the height is no more than 2 log2n, where n is the number of nodes in the tree. For AVL trees with more than the minimum number of nodes, the relationship between the number of nodes and the height is even better, though, for reasons we've seen previously, we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic. So, ultimately, we see that the height of an AVL tree with n nodes is Θ(log n). (In reality, it turns out that the bound is lower than 2 log2n; it's something more akin to about 1.44 log2n, even for AVL trees with the minimum number of nodes, though the proof of that is more involved and doesn't change the asymptotic result.)\n",
      "Stored embedding for: log2n; it's something more akin to about 1.44 log2n, even for AVL trees with the minimum number of nodes, though the proof of that is more involved and doesn't change the asymptotic result.)\n",
      " -----> Processed Notes and Examples AVL Trees.pdf\n",
      "Stored embedding for: Neo4j Slide Visual Description (cover image) • The slide features a simple title layout with a white background. • Text reads: “Neo4j” in large font. • The instructor’s name and course code (“DS 4300”) appear near the top or center. • No additional graphics, just text. Overview / Context This slide introduces the day’s topic: working with the Neo4j graph database. It shows the course number (DS 4300), the instructor (Mark Fontenot, PhD), and the specific project or lesson (Market Forecast Project, Day 11). ⸻ Slide 2: Neo4j Key Points • A Graph Database System that supports both transactional and analytical processing. • Graph-based data model (nodes, relationships, properties). • ACID transactions (so constraints can be imposed). • ACID stands for Atomicity, Consistency, Isolation, Durability. • Possibly the best-known graph DB in industry. • Other similar technologies: ArangoDB, Amazon Neptune, etc. Explanation / Expansion Neo4j is a popular choice for applications where relationships between data points are as critical as the data itself. Because it is ACID-compliant, Neo4j can be used in production environments requiring strict consistency. Its graph-based nature makes it ideal for queries that are difficult to model in a traditional relational database (e.g., social networks, recommendation engines, knowledge graphs). Slide Visual Description • White background with bullet points summarizing Neo4j’s key features. • Title “Neo4j” in bold. • Bulleted list of features.\n",
      "Stored embedding for: ⸻ Slide 3: Neo4j – Query Language and Plugins Key Points • Cypher: Neo4j’s query language, created in 2011. • APOC: “Awesome Procedures On Cypher.” • A library of user-defined procedures and functions. • Example usage: CALL apoc.help('csv') • Allows advanced transformations and extended functionality. • APOC is optional (plugin you can install). Explanation / Expansion • Cypher is the SQL-like language for Neo4j but is more intuitive for graph operations. • APOC extends Cypher by providing a wide range of helper procedures. For instance, you can easily import/export data, manipulate data structures, or generate random data for testing. Slide Visual Description • Title “Neo4j – Query Language and Plugins” at the top. • Bullet points describing Cypher and APOC. • Possibly a screenshot snippet or reference to APOC commands (like CALL apoc.help('csv')). ⸻ Slide 4: Neo4j in Docker Compose Key Point This slide likely serves as a transition to the idea of running Neo4j inside Docker containers. Running databases in containers is common for portability and easy environment setup. Explanation / Expansion • You can run Neo4j in Docker as a single container. • To manage multiple services (e.g., a web app + Neo4j), Docker Compose is typically used. • Docker Compose can define services, networks, and volumes in one YAML file. Slide Visual Description\n",
      "Stored embedding for: • Title: “Neo4j in Docker Compose.” • The slide may show a brief list or a single statement that Docker Compose will be used to spin up a Neo4j container. ⸻ Slide 5: Docker Compose Key Points • Supports multi-container management: You can define multiple services in one file. • File is declarative: Typically docker-compose.yaml or docker- compose.yml. • Cross-platform: Works on Windows, macOS, Linux. • Commands: docker-compose up -d, docker-compose down, etc. Explanation / Expansion • Docker Compose allows you to describe an application’s services (e.g., a database, a web front-end, a backend API) in a single YAML file. • By running docker-compose up -d, all containers defined in the file are created and started. • Docker Compose is particularly useful in development environments to replicate a production setup on a local machine. Slide Visual Description • A bullet list explaining what Docker Compose is and why it’s used. • Possibly an example snippet showing the structure of a docker- compose.yaml. ⸻ Slide 6: docker-compose.yaml Key Points / Example Snippet A typical docker-compose.yaml might include: version: '3.8' services: neo4j: image: neo4j:latest container_name: neo4j env_file: - .env\n",
      "Stored embedding for: ports: - \"7474:7474\" - \"7687:7687\" volumes: - neo4j_data:/data environment: - NEO4J_AUTH=neo4j/your_password volumes: neo4j_data: Explanation / Expansion • version: The Compose file format version. • services: Each service is a container. Here, only one named neo4j. • env_file: References an external .env file that holds environment variables. • ports: Maps container ports to host ports (7474 for Neo4j’s web interface, 7687 for the Bolt protocol). • volumes: Persists data outside the container’s ephemeral filesystem. Slide Visual Description • A screenshot or text snippet of the YAML configuration. • Possibly highlights the env_file line or the port mapping lines. ⸻ Slide 7: .env Files Key Points • .env files store a collection of environment variables. • Good way to keep environment-specific variables separate (e.g., passwords, hostnames, etc.). • Example snippet: NEO4J_AUTH=neo4j/somepassword Explanation / Expansion • .env files allow you to avoid hardcoding sensitive or environment- specific details into the docker-compose.yaml. • The Compose file references these variables with syntax like $ {VARIABLE_NAME} or via env_file:. Slide Visual Description • The slide shows a small snippet of .env content: • Possibly NEO4J_AUTH=neo4j=abc123!!! as an example.\n",
      "Stored embedding for: ⸻ Slide 8: Docker Compose Commands Key Points • To test if you have Docker CLI properly installed, run: docker --version docker-compose --version • Common Docker Compose commands: • docker-compose up -d • docker-compose down • docker-compose build --no-cache Explanation / Expansion • docker-compose up -d: Builds (if needed) and starts containers in the background (detached mode). • docker-compose down: Stops and removes containers and networks defined in the Compose file. • docker-compose build --no-cache: Rebuilds images without using any cached layers. Slide Visual Description • Bullet list of commands, with a black terminal screenshot or code block style. • Possibly a note that you should see a version output if Docker is installed correctly. ⸻ Slide 9: localhost:7474 Key Point Once Neo4j is running in Docker, you can access the Neo4j Browser at http://localhost:7474. Explanation / Expansion • Port 7474 is the default for the Neo4j web interface. • Another port (7687) is used by the Bolt protocol (used by drivers and some query tools).\n",
      "Stored embedding for: Slide Visual Description • Likely a screenshot of a browser window pointing to localhost:7474, prompting for a username and password. ⸻ Slide 10: Neo4j Browser (Login Screen and Interface) Title/Text “localhost:7474 Then login.” Screenshot Description • The screenshot shows the Neo4j Browser interface. • There is a login prompt, typically requiring a username (neo4j) and a password (the one set in NEO4J_AUTH). Additional Explanation • After logging in, you will see the Neo4j Browser, which has: • A Sidebar (left) with Database, Favorites, Guides. • A Query editor or “Run query” bar at the top. • Result frames below the query editor showing node- relationship visualizations and data tables. • Ability to Export results, Collapse frames, or Save as a Favorite. ⸻ Slide 11: Inserting Data by Creating Nodes Cypher Examples CREATE (john:User {name: \"John\", birthPlace: \"Dallas\"}) CREATE (eve:User {name: \"Eve\", birthPlace: \"Paris\"}) Explanation / Expansion • CREATE is used to add nodes (and relationships) to the graph. • (john:User ...) means: “Create a node labeled User and assign it to the variable john.” • {name: \"John\", birthPlace: \"Dallas\"} is a property map for that node. Slide Visual Description • Shows sample Cypher queries for creating two users, John and Eve, with\n",
      "Stored embedding for: properties. ⸻ Slide 12: Adding an Edge with No Variable Names Cypher Example CREATE (:User {name: \"Bill\", birthPlace: \"Houston\"}) CREATE (:User {name: \"Alice\", birthPlace: \"2023-01-23\"}) CREATE (:User {name: \"Eve\", birthPlace: \"Paris\"}) Then MATCH (a:User {name: \"Bill\"}), (b:User {name: \"Alice\"}) CREATE (a)-[:KNOWS]->(b) Explanation / Expansion • We can create nodes without assigning them to variables by omitting the variable inside parentheses. • The second query uses MATCH to find two existing nodes and creates a relationship between them. • The :KNOWS is the relationship type. Slide Visual Description • Possibly code snippets in bullet points. • Explanation that relationships are always directed in Neo4j. ⸻ Slide 13: Matching: Which Users Were Born in London? Cypher Example MATCH (usr:User {birthPlace: \"London\"}) RETURN usr.name, usr.birthPlace Explanation / Expansion • MATCH is used to find nodes or relationships in the graph that match a pattern. • Here, we match any User node that has the property birthPlace = \"London\". • RETURN outputs the user’s name and birthplace.\n",
      "Stored embedding for: Slide Visual Description • A short snippet of the Cypher query. • Possibly shows the result table with any matching users. ⸻ Slide 14: Download Dataset and Move to Import Folder Instructions • Link to a dataset: https://data.films… (example link) • Move the CSV files into the import folder in your Neo4j instance or Docker volume. Explanation / Expansion • Neo4j can import CSV files located in its import directory. • If you’re using Docker, you might map a local directory to /var/lib/neo4j/ import so that you can place CSV files there. Slide Visual Description • Possibly a screenshot showing the user copying or moving files into an import folder. • Example CSV files might include netflix_titles.csv, imdb_titles.csv, etc. ⸻ Slide 15: Importing Data Key Point • You can use LOAD CSV in Cypher to import data into Neo4j. Explanation / Expansion • This is a built-in Neo4j command that allows reading CSV files, optionally specifying headers, field terminators, and so on. • Must be mindful of path references (e.g., file:///filename.csv) and whether you have read/write permissions. Slide Visual Description • Title: “Importing Data” • Possibly a bullet list describing the steps to use LOAD CSV. ⸻\n",
      "Stored embedding for: Slide 16: Basic Data Importing Cypher Example LOAD CSV WITH HEADERS FROM 'file:///netflix_titles.csv' AS line CREATE (m:Movie { title: line.title, director: line.director }) Explanation / Expansion • LOAD CSV WITH HEADERS interprets the first row of the CSV as column names. • Each row is accessible via the line variable (e.g., line.title). • CREATE (m:Movie {...}) creates a node labeled Movie with properties from the CSV columns. Slide Visual Description • Shows code snippet for a basic import from netflix_titles.csv. ⸻ Slide 17: Loading CSVs – General Syntax General Syntax LOAD CSV [WITH|WITHOUT] HEADERS FROM 'file:///some_file.csv' AS line FIELDTERMINATOR ';' [do stuff with line] Explanation / Expansion • WITH HEADERS or WITHOUT HEADERS depends on your CSV. • FIELDTERMINATOR is optional if your CSV uses a non-default delimiter (e.g., semicolon). • Inside the query, you can manipulate each row (line) to create nodes, relationships, or update existing data. Slide Visual Description • A bullet list or code snippet showing the general structure of a LOAD CSV statement.\n",
      "Stored embedding for: ⸻ Slide 18: Importing with Directors This Time Cypher Example LOAD CSV WITH HEADERS FROM 'file:///netflix_titles.csv' AS line MATCH (p:Person {name: line.director}) MERGE (m:Movie {title: line.title}) CREATE (p)-[:DIRECTED]->(m) Explanation / Expansion • Demonstrates how you can match or merge existing persons (:Person) and create relationships to newly merged movies. • MERGE acts like an “upsert” — if the node doesn’t exist, it will be created; if it does exist, it is matched. Slide Visual Description • Code snippet focusing on the creation of DIRECTED relationships. • Possibly a bullet or highlight on MERGE vs. CREATE. ⸻ Slide 19: Matching and Removing Directors Merged Cypher Example MATCH (p:Person) DELETE p (Hypothetical example if you needed to remove incorrectly merged data.) Or: MATCH (p:Person)-[r:DIRECTED]->(m:Movie) DELETE r (Removes relationships but keeps the nodes.) Explanation / Expansion • MATCH + DELETE is how you remove nodes or relationships in Neo4j.\n",
      "Stored embedding for: • If a node has relationships, you cannot delete the node unless you also delete or detach those relationships (DETACH DELETE p). Slide Visual Description • Possibly a snippet showing the commands to remove data from the graph. • Could be used for cleaning up mistakes before re-importing. ⸻ Slide 20: Adding Edges Cypher Example LOAD CSV WITH HEADERS FROM 'file:///some_titles.csv' AS line MATCH (m:Movie {title: line.title}) MATCH (p:Person {name: line.director_name}) CREATE (p)-[:DIRECTED]->(m) Explanation / Expansion • You can do multiple MATCH clauses to find existing nodes (movies and people) and then create relationships between them. • This step is often done after the nodes themselves have been created/ merged in prior steps. Slide Visual Description • Another code snippet with bullet points. • Emphasizes that relationships are created after we have the needed nodes in the graph. ⸻ Slide 21: Gut Check Query Example MATCH (m:Movie {title: \"Ray\"})<-[:DIRECTED]-(p:Person) RETURN m, p Explanation / Expansion • This is a test query to see if the movie Ray and its director(s) were successfully imported.\n",
      "Stored embedding for: • m, p in the return statement will show the movie node and the related person node in both table and graph view. Screenshot Description • The Neo4j Browser screenshot likely shows a node labeled “Ray” connected via a DIRECTED relationship to a person node labeled with the director’s name (e.g., “Taylor Hackford”). ⸻ Additional Notes and Best Practices 1. Data Modeling • Plan your labels (e.g., :Movie, :Person, :User) carefully. • Decide which properties each node should contain and which relationships to create. 2. Indexes and Constraints • For better performance, especially on large datasets, consider creating indexes or constraints (e.g., CREATE CONSTRAINT FOR (m:Movie) REQUIRE m.title IS UNIQUE). 3. Docker Tips • Always ensure your volumes are mapped properly so data persists. • .env is a best practice for storing credentials. • Use docker-compose logs -f neo4j to view logs in real time if something goes wrong. 4. Security • Don’t leave default credentials in production environments. • Limit external access to Neo4j’s ports if not needed. 5. Learning Resources • The Neo4j Documentation covers more advanced topics like indexing, constraints, performance tuning, APOC usage, etc. • The Neo4j Browser Guide is helpful for new users. ⸻ Conclusion This write-up provides an expanded overview of the slides regarding Neo4j basics, how to run Neo4j in Docker Compose, how to use .env files, and how to import data with Cypher commands such as LOAD CSV. The screenshots have been described in text so you can fully understand the setup, commands, and the Neo4j\n",
      "Stored embedding for: Browser interface. With this information, you should be able to: • Spin up a Neo4j container via Docker Compose, • Log into the Neo4j Browser at localhost:7474, • Create nodes and relationships in Cypher, • Import CSV data into Neo4j, and • Verify your data by running test queries.\n",
      " -----> Processed Neo4j.pdf\n",
      "Stored embedding for: DS 4300 Redis + Python Mark Fontenot, PhD Northeastern University\n",
      "Stored embedding for: Redis-py 2 - Redis-py is the standard client for Python. - Maintained by the Redis Company itself - GitHub Repo: redis/redis-py - In your 4300 Conda Environment: pip install redis\n",
      "Stored embedding for: Connecting to the Server - For your Docker deployment, host could be localhost or 127.0.0.1 - Port is the port mapping given when you created the container (probably the default 6379) - db is the database 0-15 you want to connect to - decode_responses → data comes back from server as bytes. Setting this true converter them (decodes) to strings. 3 import redis redis_client = redis.Redis(host=’localhost’, port=6379, db=2, decode_responses=True)\n",
      "Stored embedding for: Redis Command List - Full List > here < - Use Filter to get to command for the particular data structure you’re targeting (list, hash, set, etc.) - Redis.py Documentation > here < - The next slides are not meant to be an exhaustive list of commands, only some highlights. Check the documentation for a complete list. 4\n",
      "Stored embedding for: String Commands # r represents the Redis client object r.set(‘clickCount:/abc’, 0) val = r.get(‘clickCount:/abc’) r.incr(‘clickCount:/abc’) ret_val = r.get(‘clickCount:/abc’) print(f’click count = {ret_val}’) 5\n",
      "Stored embedding for: String Commands - 2 # r represents the Redis client object redis_client.mset({'key1': 'val1', 'key2': 'val2', 'key3': 'val3'}) print(redis_client.mget('key1', 'key2', 'key3')) # returns as list [‘val1’, ‘val2’, ‘val3’] 6\n",
      "Stored embedding for: String Commands - 3 - set(), mset(), setex(), msetnx(), setnx() - get(), mget(), getex(), getdel() - incr(), decr(), incrby(), decrby() - strlen(), append() 7\n",
      "Stored embedding for: List Commands - 1 # create list: key = ‘names’ # values = [‘mark’, ‘sam’, ‘nick’] redis_client.rpush('names', 'mark', 'sam', 'nick') # prints [‘mark’, ‘sam’, ‘nick’] print(redis_client.lrange('names', 0, -1)) 8\n",
      "Stored embedding for: List Commands - 2 - lpush(), lpop(), lset(), lrem() - rpush(), rpop() - lrange(), llen(), lpos() - Other commands include moving elements between lists, popping from multiple lists at the same time, etc. 9\n",
      "Stored embedding for: Hash Commands - 1 redis_client.hset('user-session:123', mapping={'first': 'Sam', 'last': 'Uelle', 'company': 'Redis', 'age': 30 }) # prints: #{'name': 'Sam', 'surname': 'Uelle', 'company': 'Redis', 'age': '30'} print(redis_client.hgetall('user-session:123')) 10\n",
      "Stored embedding for: Hash Commands - 2 - hset(), hget(), hgetall() - hkeys() - hdel(), hexists(), hlen(), hstrlen() 11\n",
      "Stored embedding for: Redis Pipelines - Helps avoid multiple related calls to the server → less network overhead 12 r = redis.Redis(decode_responses=True) pipe = r.pipeline() for i in range(5): pipe.set(f\"seat:{i}\", f\"#{i}\") set_5_result = pipe.execute() print(set_5_result) # >>> [True, True, True, True, True] pipe = r.pipeline() # \"Chain\" pipeline commands together. get_3_result = pipe.get(\"seat:0\").get(\"seat:3\").get(\"seat:4\").execute() print(get_3_result) # >>> ['#0', '#3', '#4']\n",
      "Stored embedding for: Redis in Context 13\n",
      "Stored embedding for: Redis in ML - Simpliﬁed Example 14 Source: https://www.featureform.com/post/feature-stores-explained-the-three-common-architectures\n",
      "Stored embedding for: Redis in DS/ML 15 Source: https://madewithml.com/courses/mlops/feature-store/\n",
      " -----> Processed 06 - Redis + Python.pdf\n",
      "Stored embedding for: Chapter 12: Binary Search Trees A binary search tree is a binary tree with a special property called the BST-property, which is given as follows: ⋆ For all nodes x and y, if y belongs to the left subtree of x, then the key at y is less than the key at x, and if y belongs to the right subtree of x, then the key at y is greater than the key at x. We will assume that the keys of a BST are pairwise distinct. Each node has the following attributes: • p, left, and right, which are pointers to the parent, the left child, and the right child, respectively, and • key, which is key stored at the node. 1\n",
      "Stored embedding for: An example 4 2 3 6 5 12 9 8 11 15 19 20 7 2\n",
      "Stored embedding for: Traversal of the Nodes in a BST By “traversal” we mean visiting all the nodes in a graph. Traversal strategies can be speciﬁed by the ordering of the three objects to visit: the current node, the left subtree, and the right subtree. We assume the the left subtree always comes before the right subtree. Then there are three strategies. 1. Inorder. The ordering is: the left subtree, the current node, the right subtree. 2. Preorder. The ordering is: the current node, the left subtree, the right subtree. 3. Postorder. The ordering is: the left subtree, the right subtree, the current node. 3\n",
      "Stored embedding for: Inorder Traversal Pseudocode This recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree. While doing traversal it prints out the key of each node that is visited. Inorder-Walk(x) 1: if x = nil then return 2: Inorder-Walk(left[x]) 3: Print key[x] 4: Inorder-Walk(right[x]) We can write a similar pseudocode for preorder and postorder. 4\n",
      "Stored embedding for: preorder postorder inorder 2 1 3 1 1 2 2 3 3 4 2 3 6 5 12 9 8 11 15 19 20 7 What is the outcome of inorder traversal on this BST? How about postorder traversal and preorder traversal? 5\n",
      "Stored embedding for: Inorder traversal gives: 2, 3, 4, 5, 6, 7, 8 , 9, 11, 12, 15, 19, 20. Preorder traversal gives: 7, 4, 2, 3, 6, 5, 12, 9, 8, 11, 19, 15, 20. Postorder traversal gives: 3, 2, 5, 6, 4, 8, 11, 9, 15, 20, 19, 12, 7. So, inorder travel on a BST ﬁnds the keys in nondecreasing order! 6\n",
      "Stored embedding for: Operations on BST 1. Searching for a key We assume that a key and the subtree in which the key is searched for are given as an input. We’ll take the full advantage of the BST-property. Suppose we are at a node. If the node has the key that is being searched for, then the search is over. Otherwise, the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for. If the former is the case, then by the BST property, all the keys in th left subtree are strictly less than the key that is searched for. That means that we do not need to search in the left subtree. Thus, we will examine only the right subtree. If the latter is the case, by symmetry we will examine only the right subtree. 7\n",
      "Stored embedding for: Algorithm Here k is the key that is searched for and x is the start node. BST-Search(x, k) 1: y ←x 2: while y ̸= nil do 3: if key[y] = k then return y 4: else if key[y] < k then y ←right[y] 5: else y ←left[y] 6: return (“NOT FOUND”) 8\n",
      "Stored embedding for: An Example search for 8 7 4 2 6 9 13 11 NIL What is the running time of search? 9\n",
      "Stored embedding for: 2. The Maximum and the Minimum To ﬁnd the minimum identify the leftmost node, i.e. the farthest node you can reach by following only left branches. To ﬁnd the maximum identify the rightmost node, i.e. the farthest node you can reach by following only right branches. BST-Minimum(x) 1: if x = nil then return (“Empty Tree”) 2: y ←x 3: while left[y] ̸= nil do y ←left[y] 4: return (key[y]) BST-Maximum(x) 1: if x = nil then return (“Empty Tree”) 2: y ←x 3: while right[y] ̸= nil do y ←right[y] 4: return (key[y]) 10\n",
      "Stored embedding for: 3. Insertion Suppose that we need to insert a node z such that k = key[z]. Using binary search we ﬁnd a nil such that replacing it by z does not break the BST-property. 11\n",
      "Stored embedding for: BST-Insert(x, z, k) 1: if x = nil then return “Error” 2: y ←x 3: while true do { 4: if key[y] < k 5: then z ←left[y] 6: else z ←right[y] 7: if z = nil break 8: } 9: if key[y] > k then left[y] ←z 10: else right[p[y]] ←z 12\n",
      "Stored embedding for: 4. The Successor and The Predecessor The successor (respectively, the predecessor) of a key k in a search tree is the smallest (respectively, the largest) key that belongs to the tree and that is strictly greater than (respectively, less than) k. The idea for ﬁnding the successor of a given node x. • If x has the right child, then the successor is the minimum in the right subtree of x. • Otherwise, the successor is the parent of the farthest node that can be reached from x by following only right branches backward. 13\n",
      "Stored embedding for: An Example 4 2 3 6 5 12 9 8 11 15 19 20 7 25 23 14\n",
      "Stored embedding for: Algorithm BST-Successor(x) 1: if right[x] ̸= nil then 2: { y ←right[x] 3: while left[y] ̸= nil do y ←left[y] 4: return (y) } 5: else 6: { y ←x 7: while right[p[x]] = x do y ←p[x] 8: if p[x] ̸= nil then return (p[x]) 9: else return (“NO SUCCESSOR”) } 15\n",
      "Stored embedding for: The predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged. For which node is the successor undeﬁned? What is the running time of the successor algorithm? 16\n",
      "Stored embedding for: 5. Deletion Suppose we want to delete a node z. 1. If z has no children, then we will just replace z by nil. 2. If z has only one child, then we will promote the unique child to z’s place. 3. If z has two children, then we will identify z’s successor. Call it y. The successor y either is a leaf or has only the right child. Promote y to z’s place. Treat the loss of y using one of the above two solutions. 17\n",
      "Stored embedding for: 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 9 13 11 8 4 2 3 5 6 1 3 2 4 7 9 13 11 8 5 6 9 13 11 8 7 10 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 13 11 9 10 18\n",
      "Stored embedding for: Algorithm This algorithm deletes z from BST T. BST-Delete(T, z) 1: if left[z] = nil or right[z] = nil 2: then y ←z 3: else y ←BST-Successor(z) 4: ✄y is the node that’s actually removed. 5: ✄Here y does not have two children. 6: if left[y] ̸= nil 7: then x ←left[y] 8: else x ←right[y] 9: ✄x is the node that’s moving to y’s position. 10: if x ̸= nil then p[x] ←p[y] 11: ✄p[x] is reset If x isn’t NIL. 12: ✄Resetting is unnecessary if x is NIL. 19\n",
      "Stored embedding for: Algorithm (cont’d) 13: if p[y] = nil then root[T] ←x 14: ✄If y is the root, then x becomes the root. 15: ✄Otherwise, do the following. 16: else if y = left[p[y]] 17: then left[p[y]] ←x 18: ✄If y is the left child of its parent, then 19: ✄Set the parent’s left child to x. 20: else right[p[y]] ←x 21: ✄If y is the right child of its parent, then 22: ✄Set the parent’s right child to x. 23: if y ̸= z then 24: { key[z] ←key[y] 25: Move other data from y to z } 27: return (y) 20\n",
      "Stored embedding for: Summary of Eﬃciency Analysis Theorem A On a binary search tree of height h, Search, Minimum, Maximum, Successor, Predecessor, Insert, and Delete can be made to run in O(h) time. 21\n",
      "Stored embedding for: Randomly built BST Suppose that we insert n distinct keys into an initially empty tree. Assuming that the n! permutations are equally likely to occur, what is the average height of the tree? To study this question we consider the process of constructing a tree T by inserting in order randomly selected n distinct keys to an initially empty tree. Here the actually values of the keys do not matter. What matters is the position of the inserted key in the n keys. 22\n",
      "Stored embedding for: The Process of Construction So, we will view the process as follows: A key x from the keys is selected uniformly at random and is inserted to the tree. Then all the other keys are inserted. Here all the keys greater than x go into the right subtree of x and all the keys smaller than x go into the left subtree. Thus, the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree. 23\n",
      "Stored embedding for: Random Variables n = number of keys Xn = height of the tree of n keys Yn = 2Xn. We want an upper bound on E[Yn]. For n ≥2, we have E[Yn] = 1 n ⎛ ⎝ n # i=1 2E[max{Yi−1, Yn−i}] ⎞ ⎠. E[max{Yi−1, Yn−i}] ≤ E[Yi−1 + Yn−i] ≤ E[Yi−1] + E[Yn−i] Collecting terms: E[Yn] ≤4 n n−1 # i=1 E[Yi]. 24\n",
      "Stored embedding for: Analysis We claim that for all n ≥1 E[Yn] ≤1 4 &n+3 3 ' . We prove this by induction on n. Base case: E[Y1] = 20 = 1. Induction step: We have E[Yn] ≤4 n n−1 # i=1 E[Yi] Using the fact that n−1 # i=0 &i + 3 3 ' = &n + 3 4 ' E[Yn] ≤4 n · 1 4 · &n + 3 4 ' E[Yn] ≤1 4 · &n + 3 3 ' 25\n",
      "Stored embedding for: Jensen’s inequality A function f is convex if for all x and y, x < y, and for all λ, 0 ≤λ ≤1, f(λx + (1 −λ)y) ≤λf(x) + (1 −λ)f(y) Jensen’s inequality states that for all random variables X and for all convex function f f(E[X]) ≤E[f(X)]. Let this X be Xn and f(x) = 2x. Then E[f(X)] = E[Yn]. So, we have 2E[Xn] ≤1 4 &n + 3 3 ' . The right-hand side is at most (n + 3)3. By taking the log of both sides, we have E[Xn] = O(log n). Thus the average height of a randomly build BST is O(log n). 26\n",
      " -----> Processed C12-bst.pdf\n",
      "Stored embedding for: ### Chapter 12: Indexing – B-Trees and B+ Trees ## 12.6 B-Trees ### Overview B-trees are balanced tree data structures commonly attributed to R. Bayer and E. McCreight (1972). They significantly improved large-file storage and retrieval, largely replacing methods other than hashing by 1979. Their strength lies in efficiently supporting insertion, deletion, and key range searches. ### Properties and Advantages of B-Trees - **Shallow Structure:** Height-balanced with all leaves at the same level. - **High Branching Factor:** Limits the number of disk accesses, enhancing performance. - **Localized Updates:** Operations only affect nodes along the path from the root to a leaf. - **Efficient Range Searches:** Related records are grouped together. - **Space Efficiency:** Nodes must maintain a minimum occupancy, ensuring efficient space utilization. ### Shape Properties of B-Trees A B-tree of order `m` adheres to: - Root is either a leaf or has at least two children. - Internal nodes (except root) have between ⌈m/2⌉ and m children. - Leaves are uniformly at the same level. B-trees generalize 2-3 trees (B-trees of order three), and node sizes typically match disk blocks for optimized I/O operations. ### Searching in B-Trees - Begin at root; use binary search. - Follow pointers according to comparison until a leaf is reached or key is found. ### Insertion in B-Trees - Locate the appropriate leaf node. - Insert directly if space is available. - If full, split the node and promote the middle key. - Propagate splits upwards if necessary, maintaining balance. ## 12.6.1.1 B+ Trees ### Motivation and Concept B+ trees improve B-trees by storing records exclusively in leaf nodes, using internal nodes only for guidance. B+ trees are particularly efficient for range\n",
      "Stored embedding for: queries due to linked leaf nodes. ### Structure and Properties of B+ Trees - Internal nodes contain keys and pointers. - Leaf nodes store actual records or keys linked to external records. - Leaf nodes form a doubly linked list. ### Operations in B+ Trees - **Search:** Always ends at leaf nodes. Internal nodes only direct searches. - **Insertion:** Similar to B-tree insertion; leaf nodes are split evenly, and the smallest key from the right node is promoted. - **Deletion:** Remove the record; if underflow occurs, redistribute from siblings or merge nodes. ### Efficiency and Utilization - B+ trees maintain at least 50% node occupancy. - Enhanced variants like B* trees optimize storage further, keeping nodes about two-thirds full through more sophisticated splitting/merging strategies. ## 12.6.1.2 B-Tree Analysis ### Efficiency - Operations (search, insertion, deletion) in B-trees and variants (B+, B*) have complexity Θ(log n), where n is the number of records. - High branching factors make trees shallow, reducing disk accesses. ### Practical Example A B+ tree of order 100: - Height 1: Up to 100 records. - Height 2: Up to 10,000 records. - Height 3: Up to 1,000,000 records. - Height 4: Up to 100,000,000 records. ### Disk Management - Top levels can be kept in main memory to minimize disk fetches. - A buffer pool (using an LRU policy or similar) manages nodes in memory, further optimizing performance. ## Conclusion B-trees and B+ trees are robust data structures, essential for database management, providing balanced and efficient operations tailored for disk storage systems.\n",
      " -----> Processed B-Trees and B+ Trees.pdf\n",
      "Stored embedding for: DS 4300 Amazon EC2 & Lambda Mark Fontenot, PhD Northeastern University Based in part on material from Gareth Eagar’s Data Engineering with AWS, Packt Publishing\n",
      "Stored embedding for: EC2 2\n",
      "Stored embedding for: EC2 ●EC2 → Elastic Cloud Compute ●Scalable Virtual Computing in the Cloud ●Many (Many!!) instance types available ●Pay-as-you-go model for pricing ●Multiple different Operating Systems 3\n",
      "Stored embedding for: Features of EC2 ●Elasticity - easily (and programmatically) scale instances up or down as needed ●You can use one of the standard AMIs OR provide your own AMI if pre-conﬁg is needed ●Easily integrates with many other services such as S3, RDS, etc. 4 AMI = Amazon Machine Image\n",
      "Stored embedding for: EC2 Lifecycle ●Launch - when starting an instance for the ﬁrst time with a chosen conﬁguration ●Start/Stop - Temporarily suspend usage without deleting the instance ●Terminate - Permanently delete the instance ●Reboot - Restart an instance without sling the data on the root volume 5\n",
      "Stored embedding for: Where Can You Store Data? - Instance Store: Temporary, high-speed storage tied to the instance lifecycle - EFS (Elastic File System) Support - Shared ﬁle storage - EBS (Elastic Block Storage) - Persistent block-level storage - S3 - large data set storage or EC2 backups even 6\n",
      "Stored embedding for: Common EC2 Use Cases ●Web Hosting - Run a website/web server and associated apps ●Data Processing - It’s a VM… you can do anything to data possible with a programming language. ●Machine Learning - Train models using GPU instances ●Disaster Recovery - Backup critical workloads or infrastructure in the cloud 7\n",
      "Stored embedding for: Let’s Spin Up an EC2 Instance 8\n",
      "Stored embedding for: Let’s Spin Up an EC2 Instance 9\n",
      "Stored embedding for: Let’s Spin Up an EC2 Instance 10\n",
      "Stored embedding for: Ubuntu VM Commands - Initial user is ubuntu - Access super user commands with sudo - Package manager is apt - kind of like Homebrew or Choco - Update the packages installed - sudo apt update; sudo apt upgrade 11\n",
      "Stored embedding for: MiniConda on EC2 Make sure you’re logged in to your EC2 instance ● Let’s install MiniConda ○ curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh ○ bash ./Miniconda3-latest-Linux-x86_64.sh 12\n",
      "Stored embedding for: Installing & Using Streamlit ●Log out of your EC2 instance and log back in ●Make sure pip is now available: ○ pip --version ●Install Streamlit and sklearn ○ pip install streamlit scikit-learn ●Make a directory for a small web app ○ mkdir web ○ cd web 13\n",
      "Stored embedding for: Basic Streamlit App ● nano test.py ● Add code on left ● ctrl-x to save and exit ● streamlit run test.py 14 import streamlit as st def main(): st.title(\"Welcome to my Streamlit App\") st.write(\"## Data Sets\") st.write(\"\"\" - data set 01 - data set 02 - data set 03 \"\"\") st.write(\"\\n\") st.write(\"## Goodbye!\") if __name__ == \"__main__\": main()\n",
      "Stored embedding for: Opening Up The Streamlit Port 15\n",
      "Stored embedding for: In a Browser 16\n",
      "Stored embedding for: AWS Lambda 17\n",
      "Stored embedding for: Lambdas ●Lambdas provide serverless computing ●Automatically run code in response to events. ●Relieves you from having to manage servers - only worry about the code ●You only pay for execution time, not for idle compute time (different from EC2) 18\n",
      "Stored embedding for: Lambda Features ●Event-driven execution - can be triggered by many different events in AWS ●Supports a large number of runtimes… Python, Java, Node.js, etc ●HIGHLY integrated with other AWS services ●Extremely scalable and can rapidly adjust to demands 19\n",
      "Stored embedding for: How it Works ●Add/upload your code through AWS MGMT console ●Conﬁgure event source(s) ●Watch your Lambda run when one of the event sources ﬁres an event 20\n",
      "Stored embedding for: Let’s Make One 21\n",
      "Stored embedding for: Making a Lambda 22\n",
      "Stored embedding for: Creating a Function 23\n",
      "Stored embedding for: Sample Code ●Edit the code ●Deploy the code! 24\n",
      "Stored embedding for: Test It 25\n",
      "Stored embedding for: ?? 26\n",
      " -----> Processed 12 - EC2 & Lambda.pdf\n",
      "Stored embedding for: CS 3200 The Relational Model of Data Mark Fontenot, PhD Northeastern University These notes are based upon and adapted via the generosity of Prof. Nate Derbinsky.\n",
      "Stored embedding for: Reminders - Make sure you can see the course on Gradescope, Campuswire, and Slack - Mini HW00 - Early EC Deadline: Sunday @ 11:59 pm - Regular Deadline: Tuesday @ 11:59 pm 2\n",
      "Stored embedding for: Applications and Data ● Many applications are data intensive compared to compute intensive. ● Many apps need to… ○ store data for itself or another app to use in the future (databases) ○ remember the results of prior expensive operations (caches) ○ allow users to search for data eﬃciently (indexes) ○ send messages with data to another application to handle (stream processing) ○ periodically process a large accumulation of data (batch processing) From Kleppmann, Designing Data Intensive Applications, O’Reilly, 2017. 3\n",
      "Stored embedding for: Stores of data? 4\n",
      "Stored embedding for: What is a database? ●Structured collection of related data ○usually related to something in the real world ○usually created… ■ for a speciﬁc group of users ■ to help these users perform some kind of tasks ■ to hopefully complete those tasks with some performance, redundancy, concurrency, and/or security considerations in mind 5\n",
      "Stored embedding for: Notion ●Intended users? ●Intended tasks? ●Considerations of ○Performance? ○Concurrency? ○Redundancy? ○Security? 6\n",
      "Stored embedding for: Database Management Systems (DBMS) ●Software that allow the creation and maintenance of databases ○Support the encoding of some type of structure for the data ○Persists the data ○Support adding new data and updating existing data ○Protects against failures and unauthorized access 7\n",
      "Stored embedding for: Some Categories of DBMSs Document-Oriented Databases - Organizes and queries data based on the concept of a “document” often in JSON - usually considered semi-structured Graph Databases - Organizes data by nodes, edges, labels - Query about paths between nodes and node relationships 8\n",
      "Stored embedding for: Some Categories of DBMSs Key-Value Databases - Everything is a key/value pair - Based on associative array Spatial Databases - Stores data related to 2D/3D locations - Query example: are 2 cars about to collide? 9\n",
      "Stored embedding for: Some Categories of DBMSs Vector Databases - unit of storage is a vector represent high-dimensional data - highly performant similarity searches - used extensively in LLMs 10\n",
      "Stored embedding for: The category for this course… ●Relational Database Management Systems ○Based on storing data in tables and connections between those tables ○Original concept developed in early 70s by EF Codd and colleagues 11\n",
      "Stored embedding for: Relational Model of Data: Overview 12\n",
      "Stored embedding for: The Relational Database: Relation/Table Image borrowed from this Medium post. Students Relation/Table Name Attributes/Columns Rows/ Tuples 13 Relation - the core construct in a relational database; collection of tuples with each tuple having values for a ﬁxed number of attributes/ﬁelds. Relation Schema - represents the attributes and their data types for a particular relation Relation Instance - represents the state of the data in the relation at a particular point in time. Row/Tuple - values for each relation’s attribute for one element of a relation instance ID Name Phone Dorm Age GPA 1123141 Mark 555-1234 1 19 3.21 2323411 Kim 555-9876 2 25 3.53 17642352 Sam 555-6758 1 19 3.25 Primary Key\n",
      "Stored embedding for: The Relational Database: Constraints Image borrowed from this Medium post. cannot be null Must be a valid dorm id in the dorm relation Example Constraints: 14 Constraint - conditions that must hold on all valid relation instances Types: - Key Constraints - Entity Integrity Constraints - Referential Integrity constraints ID Name Phone Dorm Age GPA 1123141 Mark 555-1234 1 19 3.21 2323411 Kim 555-9876 2 25 3.53 17642352 Sam 555-6758 1 19 3.25 Student\n",
      "Stored embedding for: The Relational Database: Relationships Name ID Phone Dorm Age GPA Mark 1123141 555-1234 1 19 3.21 Kim 2323411 555-9876 2 25 3.53 Sam 17642352 555-6758 1 19 3.25 Dorm Name 1 555 Huntington 2 Baker ID Class 1123141 COMP355 2323411 COMP355 17642352 MATH650 1123141 MATH650 2323411 BIOL110 Student Dorm Class Some values in one table are related (by design) to values in another table 15 Referential Integrity Constraint Examples\n",
      "Stored embedding for: The Relational Database: Queries Name ID Phone Dorm Age GPA Mark 1123141 555-1234 1 19 3.21 Kim 2323411 555-9876 2 25 3.53 Sam 17642352 555-6758 1 19 3.25 Dorm Name 1 555 Huntington 2 Baker ID Class 1123141 COMP355 2323411 COMP355 17642352 MATH650 1123141 MATH650 2323411 BIOL110 Student Dorm Class Questions (Queries): “Provide a list of student names and IDs with GPA between 3.0 and 3.5.” 16\n",
      "Stored embedding for: The Relational Database: Queries Name ID Phone Dorm Age GPA Mark 1123141 555-1234 1 19 3.21 Kim 2323411 555-9876 2 25 3.53 Sam 17642352 555-6758 1 19 3.25 Dorm Name 1 555 Huntington 2 Baker ID Class 1123141 COMP355 2323411 COMP355 17642352 MATH650 1123141 MATH650 2323411 BIOL110 Student Dorm Class 17 Questions (Queries): “Which students live in Baker Dorm?”\n",
      "Stored embedding for: Databases in the Context of a Software System Database Tier Application Tier Client Tier 18\n",
      "Stored embedding for: The Relational Model of Data: Digging In 19\n",
      "Stored embedding for: History 101 Codd, Edgar F. \"A relational model of data for large shared data banks.\" Communications of the ACM 13.6 (1970): 377-387. 20 “Future users of large data banks must be protected from having to know how the data is organized in the machine (the internal representation)…”\n",
      "Stored embedding for: History 101 ●The relational model provides a formal mathematical basis for the structure of and interaction with a relational database ○ Based on set theory and ﬁrst-order predicate logic ○ The formal basis allows for robust scientiﬁc development of the model. ●The (eternal) struggle of theory vs. practice… ○ Most modern RDBMSs don’t strictly adhere to the purest mathematical formalisms in the relational model. 21\n",
      "Stored embedding for: A Little Set Theory Review… ●What’s a set? ●What is the cardinality of a set? (denoted |S|) 22 A set is a collection of unique objects. The things (objects) inside a set are called elements. cardinality → the number of unique elements in the set. What are some examples of sets? ?\n",
      "Stored embedding for: A Little Set Theory Review… ●What is the Union of sets A and B (A ⋃ B)? ●What is the intersection of sets A and B (A ∩ B)? ●What is the set diﬀerence of A and B (A - B)? 23 A = {1, 3, 5} B = {3, 4, 5, 6} AUB = {1, 3, 5, 4, 6} A⋂B = {3, 5} A-B = {1}\n",
      "Stored embedding for: A Little Set Theory Review… ●What is the cartesian product of two sets? A = {123, 435} B = {Chul, Kev, Sal} ●What is the cartesian product of A and B (aka A x B)? 24 AXB = { (123, Chul), (123, Kev), (123, Sal), (435, Chul), (435, Kev), (435, Sal) } The cartesian product of two sets A and B is the set of all ordered pairs (a, b) where a ∈ A and b ∈ B.\n",
      "Stored embedding for: What’s a Relational Database? ●A Relational Database consists of ○ a collection of relations, and ○ a collection of constraints. ●A relational database is in a valid/consistent state if it satisﬁes all constraints (else, invalid/inconsistent state). 25\n",
      "Stored embedding for: Relations (Some Review) ●A relation consists of ○ its schema → a description of the structure of the relation (relation schema) ○ its state → the current data that is populated in the relation (relation instance) ●The schema of a relation includes ○ the name of the relation ○ an list of n attributes each with an associated domain (what values that attribute can take on). ●Notation: REL_NAME(Attrib1:Dom1, Attrib2:Dom2…) 26\n",
      "Stored embedding for: More formally… ●Let A1, A2, …, An be names of attributes of relation R with associated domains D1, D2, …, Dn, then R(A1:D1, A2:D2,...An:Dn) is a relation schema and n, the degree of R, represents the number of attributes of R. ●Then, an instance of Relation R is a subset of the cartesian product of the domains of the attributes of R. 27\n",
      "Stored embedding for: Relations - Example - Assume we have the following domains: - names → {‘Jared’, ‘Sakshi’} - id_nums → {all 9 digit positive integers starting with 00} - majors → {‘CS’, ‘DS’, ‘CY’} - Deﬁning the TA relation schema: TA(name: names, id: id_nums, major: majors) 28 Is the following a valid instance of TA? { (‘Jared’, 001928374, ‘CS’) (‘Sakshi’, 001122334, ‘DS’) } ?\n",
      "Stored embedding for: Relations - Example - Assume we have the following domains: - names → {‘Jared’, ‘Sakshi’} - id_nums → {all 9 digit positive integers starting with 00} - majors → {‘CS’, ‘DS’, ‘CY’} - Deﬁning the TA relation schema: TA(name: names, id: id_nums, major: majors) 29 Is the following a valid instance of TA? { (‘Sakshi’, 001928374, ‘CS’) (‘Sakshi’, 001122334, ‘CY’) } ?\n",
      "Stored embedding for: Relations - Example - Assume we have the following domains: - names → {‘Jared’, ‘Sakshi’} - id_nums → {all 9 digit positive integers starting with 00} - majors → {‘CS’, ‘DS’, ‘CY’} - Deﬁning the TA relation schema: TA(name: names, id: id_nums, major: majors) 30 Is the following a valid instance of TA? { (‘Sakshi’, 001928374, ‘CS’) (‘Dylan’, 001122334, ‘DS’) } ?\n",
      "Stored embedding for: Relations - Example - Assume we have the following domains: - names → {‘Jared’, ‘Sakshi’} - id_nums → {all 9 digit positive integers starting with 00} - majors → {‘CS’, ‘DS’, ‘CY’} - Deﬁning the TA relation schema: TA(name: names, id: id_nums, major: majors) 31 Is the following a valid instance of TA? { (‘Sakshi’, 001928374, ‘CS’) (‘Jared’, 001122334) } ?\n",
      "Stored embedding for: Relation Instance ●A relation instance is a set of tuples (rows) from a relation at a particular point in time. ●Each tuple (row) is an ordered sequence of values, one for each attribute (possibly null) ○ usually enclosed in < and > 32 Name ID Phone Dorm Age GPA Mark 1123141 555-1234 1 19 3.21 Kim 2323411 555-9876 2 25 3.53 Sam 17642352 555-6758 1 19 3.25 Student 1 Tuple\n",
      "Stored embedding for: Null Value ● Null is a special value that may exist in the domain of an attribute ● Could mean diﬀerent things ○ value unknown ○ value unavailable right now ○ attribute doesn’t apply to this tuple ● Does NOT mean: ○ zero (0) ○ the empty string (‘’) ● (NULL != NULL) Comparing two values of NULL does NOT return true 33\n",
      "Stored embedding for: Value of an Attr in a Tuple ●Values should be atomic ○ Say NO to composite attributes (ex: address that includes city, state and zip) ○ Say NO to multi-valued attributes (ex: all email addresses for 1 person) 34 Name ID Address Phone Dorm Age GPA Mark 1123141 121 Anystreet Boston MA 02212 555-1234 555-1876 1 19 3.21 Kim 2323411 235 Huntington Boston MA 02215 555-9876 2 25 3.53 Student\n",
      "Stored embedding for: Super and Candidate Keys - key - a subset of attributes of a relation used to uniquely identify each tuple - A super key of a relation R is a subset of the attributes of R such that no two distinct tuples in any possible relation instance will have the same values for the subset of attributes. - may not be minimal - could contain attributes that aren’t needed for unique determination - A candidate key of relation r is a minimal super key. - A relation may have more than one candidate keys. 35\n",
      "Stored embedding for: Keys - Superkey Some Possible Superkeys: (customerNumber, customerName) (customerNumber, salesRepEmployeeNumber) (customerNumber) Not Minimal Minimal Customers 36\n",
      "Stored embedding for: Keys - Candidate Keys Some Possible Superkeys: (customerNumber, customerName) (customerNumber, salesRepEmployeeNumber) (customerNumber) Not Candidate Keys Candidate Key Customers 37\n",
      "Stored embedding for: The Primary Key - The primary key (PK) of relation R is chosen from the set of candidate keys - If a relation has only 1 candidate key, it becomes the PK. - If a relation has > 1 candidate key, the database designer chooses one based on business requirements - Every relation must have a PK - Entity Integrity Constraint → PK values must be unique and may NOT be null - (Usually) the PK is underlined in a relation schema or table 38\n",
      "Stored embedding for: Keys - Primary Key Some Possible Superkeys: (customerNumber, customerName) (customerNumber, salesRepEmployeeNumber) (customerNumber) Chosen as Primary Key If there are 2+ Candidate Keys, DB Designer will choose one as the Primary key Customers 39\n",
      "Stored embedding for: Foreign Keys ● Foreign Key (FK) - An attribute ai in one relation RC (the child relation) refers to/references the PK aj in another relation RP (parent relation) such that all values of ai must either be NULL or contain a value from aj. ● Self-Referential Relation → RC and RP are the same relation ● Foreign Key == Referential Integrity Constraint ● Foreign Keys are the operationalization of relationships in a relational database 40\n",
      "Stored embedding for: Foreign Key Example 41 Students cID sID Grade 332 234 B 332 332 A 535 336 C 221 134 A 535 332 C 221 332 A 221 336 B Grades sID Name Year 134 Chul 1 234 Kev 3 332 Sam 2 336 Ashwin 2 Parent Relation Child Relation (the one being referenced) (the one referencing another relation) Primary Keys Foreign Key Note: in this example, Grades.sID cannot contain null values because it is part of the PK of Grades Relation.\n",
      "Stored embedding for: FKs - but why??? Customers Notice: empLast, empFirst, and empEmail attributes contain duplicated/repeated data. Opens up the possibility of insert or update anomalies. So, put them in a separate table along with empNum and refer back to the new table (becomes the parent table) 42\n",
      "Stored embedding for: Relational Algebra 43\n",
      "Stored embedding for: Relational Algebra ●Relational Algebra (RA) : a procedural query language for relations that allow us to retrieve information from a relational database ○ A query is an operation or set of operations applied to one or more relation instances ○ RA is closed, meaning the result/output of each query is another relation ○ In RA, order of operations matters ●Not a full-ﬂedged (turing complete) programming language 44\n",
      "Stored embedding for: Quick Aside: Predicate Functions - Predicate functions - functions that return true or false. - We will use predicate functions (or just “predicates”) to determine if tuples in a relation instance should be returned by a query or not. - in the form attr <op> attr OR attr <op> <constant> - <op> can be any standard relational operator (=, !=, <, >, <=, etc) - predicates can be composed with ^ (and), v (or), ㄱ (not). empID firstName 333 Bob 143 Sam Employees Examples: - empID = 143 - empID > 400 - firstName = ‘Bob’ ^ empID = 333 45\n",
      "Stored embedding for: The First 8 Basic Operations of RA 1. Select σ (Tuple Filtering) 2. Project π (Attribute Filtering) 3. Rename ρ 4. Cartesian Product ✕ 5. Join ⨝ 6. Intersection ⋂ 7. Set-diﬀerence – 8. Union U 46 Highest Precedence Lowest Precedence You can always use ( ) to change the order of operations. Often times, ( ) make things more clear.\n",
      "Stored embedding for: Relational Algebra: Select Operator ●Select - return a relation containing tuples from relation R that satisfy predicate pred. Notation: ●Think of it as a horizontal subset (subset of tuples/rows) of a relation instance 47\n",
      "Stored embedding for: Relational Algebra: Select Operator 48 Dept Class Enrollment CS 3200 40 CS 2500 643 CS 1800 680 DS 2000 412 Enrollments Dept Class Enrollment CS 2500 643 CS 1800 680 Enrollments Result:\n",
      "Stored embedding for: Relational Algebra: Select Operator 49 Dept Class Taught_by CS 3200 CS CS 2500 CS CS 1800 Math DS 2000 Math Enrollments Dept Class Taught_by CS 3200 CS Enrollments Result:\n",
      "Stored embedding for: Relational Algebra: Project Operator Project - returns a relation with a subset of attributes (A1… Ak) from R. Notation: Duplicate tuples will be removed from the resulting relation (because relations are sets). 50\n",
      "Stored embedding for: Relational Algebra: Project Operator 51 Dept Class Enrollment CS 3200 40 CS 2500 643 CS 1800 680 DS 2000 412 Enrollments Dept Class CS 3200 CS 2500 CS 1800 DS 2000 Enrollments Dept CS DS Enrollments Result: Result:\n",
      "Stored embedding for: Relational Algebra: Cartesian Product Same operation from set theory. 52 Attr_1 Attr_2 123 abc 456 def R Attr_1 Attr_3 123 CS 789 DS 111 Cyber S R.Attr_1 R.Attr_2 S.Attr_1 S.Attr_3 123 abc 123 CS 123 abc 789 DS 123 abc 111 Cyber 456 def 123 CS 456 def 789 DS 456 def 111 Cyber Note: we can always use Relation.Attribute notation to resolve naming collisions. Relations can’t have two attributes with the same name\n",
      "Stored embedding for: Union, Intersection & Difference ●Essentially the same as what we know from set theory. ●One small diﬀerence: relations must be schema compatible ○ same number of attributes ○ attributes’ domains must be compatible 53\n",
      "Stored embedding for: More Complex RA Expressions 54 - Simple RA expression can be composed into more complex expressions - Remember: output of each RA operation is another relation\n",
      "Stored embedding for: Relational Algebra: Temporary Relation Names For more complex RA queries, you can have: ● one long query expression ● an ordered list of smaller expressions, the result of each is given a temporary name with the ← operator Example: ● TEMP_NAME ← TEMP_NAME can then be used as a relation in subsequent steps of the same query. ● Be careful about attribute naming collisions 55\n",
      "Stored embedding for: Relational Algebra: Rename Operator Rename Operator (rho) – Allows us to “rename” a relation, the attributes of a relation, or both. - If only name is provided, the relation is being renamed (all attributes retain their original name.) - List of attributes in parentheses means renaming attributes, but not relation (assume attributes originally (employeeID, lastName, ﬁrstName) - Rename both. 56\n",
      "Stored embedding for: Developing Relational Algebra Expressions 57\n",
      "Stored embedding for: Writing RA Queries - Sometimes we need to evaluate an RA query against a database instance - Result is usually another relation instance/set of tuples/table - Other times we need to convert the narrative form of a query into a RA query - Example: “Provide a list of all info from the Employee relation where the empID is less than 400.” - Answer: 58\n",
      "Stored embedding for: Writing RA Queries 59 Stu_ID Name Year 134 Chul 1 234 Kev 3 332 Sam 2 336 Ashwin 2 Cou_ID Dept Number Prof 332 CS 3345 Lawrimore 221 CS 2341 Fontenot 535 MATH 2339 Norris Students Courses C_ID S_ID Grade 332 234 B 332 332 A 535 336 C 221 134 A 535 332 C 221 332 A 221 336 B Grades Next few examples use this database schema.\n",
      "Stored embedding for: Writing RA Queries 60 Stu_ID Name Year 134 Chul 1 234 Kev 3 332 Sam 2 336 Ashwin 2 Cou_ID Dept Number Prof 332 CS 3345 Lawrimore 221 CS 2341 Fontenot 535 MATH 2339 Norris Students Courses C_ID S_ID Grade 332 234 B 332 332 A 535 336 C 221 134 A 535 332 C 221 332 A 221 336 B Grades Write a RA query that returns the names of all students.\n",
      "Stored embedding for: Writing RA Queries 61 Stu_ID Name Year 134 Chul 1 234 Kev 3 332 Sam 2 336 Ashwin 2 Cou_ID Dept Number Prof 332 CS 3345 Lawrimore 221 CS 2341 Fontenot 535 MATH 2339 Norris Students Courses C_ID S_ID Grade 332 234 B 332 332 A 535 336 C 221 134 A 535 332 C 221 332 A 221 336 B Grades Provide a list of all course numbers taught by the CS Department.\n",
      "Stored embedding for: Writing RA Queries 62 Stu_ID Name Year 134 Chul 1 234 Kev 3 332 Sam 2 336 Ashwin 2 Cou_ID Dept Number Prof 332 CS 3345 Lawrimore 221 CS 2341 Fontenot 535 MATH 2339 Norris Students Courses C_ID S_ID Grade 332 234 B 332 332 A 535 336 C 221 134 A 535 332 C 221 332 A 221 336 B Grades List all 2nd year student names.\n",
      "Stored embedding for: Writing RA Queries 63 Stu_ID Name Year 134 Chul 1 234 Kev 3 332 Sam 2 336 Ashwin 2 Cou_ID Dept Number Prof 332 CS 3345 Lawrimore 221 CS 2341 Fontenot 535 MATH 2339 Norris Students Courses C_ID S_ID Grade 332 234 B 332 332 A 535 336 C 221 134 A 535 332 C 221 332 A 221 336 B Grades What course or courses (dept and number) are taught by Professor Norris?\n",
      "Stored embedding for: Writing RA Queries 64 Stu_ID Name Year 134 Chul 1 234 Kev 3 332 Sam 2 336 Ashwin 2 Cou_ID Dept Number Prof 332 CS 3345 Lawrimore 221 CS 2341 Fontenot 535 MATH 2339 Norris Students Courses C_ID S_ID Grade 332 234 B 332 332 A 535 336 C 221 134 A 535 332 C 221 332 A 221 336 B Grades List the letter grades that Sam has earned (don’t need to include course info). or\n",
      "Stored embedding for: Joining Data from Two Relations 65\n",
      "Stored embedding for: Relational Algebra: The Join Operator ●Join - allows us to combine data from two relations. ○ Subset of the cartesian product of the two argument relations based on explicit or implicit join predicate. ●2 Versions: ○ Natural Join - Join relations on attributes with the same name ○ Theta Join (or condition join or simply Join) - Join relations with explicitly supplied join predicate 66\n",
      "Stored embedding for: Natural Join ●Given: A(R, S, T, U, V) and B(S, V, W, X) ●Query: ●Notice: A and B both have attributes named S & V. ●Result: ○ Schema of resulting relation is where attr(R) returns a set containing the attributes of R - so, attributes used in the implicit join condition are not duplicated in the result ○ contains any tuple from A x B where values for attributes S and V are equal ○ If A & B have no common attribute names, result is A X B. 67\n",
      "Stored embedding for: Example 68 Students cID sID Grade 332 234 B 332 332 A 535 336 C 221 134 A 535 332 C 221 332 A 221 336 B Grades sID Name Year 134 Chul 1 234 Kev 3 332 Sam 2 336 Ashwin 2 Query: cID sID Grade Name Year 332 234 B Kev 3 332 332 A Sam 2 535 336 C Ashwin 2 221 134 A Chul 1 535 332 C Sam 2 221 332 A Sam 2 221 336 B Ashwin 2 Result:\n",
      "Stored embedding for: theta-Join (or Join… or Condition(al) Join) ● Operator has an explicit join predicate (condition) (doesn’t rely upon attribute names) ● Result is a subset of the cartesian product where provided predicate holds true ● Assume: S(sID, name) and G(stu-ID, course, semester, grade) ● Query: ● Result: ○ Relation with schema (sID, name, stu-ID, course, semester, grade) ○ all tuples where S.sID = G.stu-ID ● Note: join condition can use other operators besides =. 69\n",
      "Stored embedding for: theta-Join (or Join… or Condition(al) Join) A B 1 Cat 2 Dog 3 Bird C D 1 Meow 3 Chirp S T A B 1 Cat 3 Bird C D 1 Meow 3 Chirp 70\n",
      "Stored embedding for: New: Entity Relationship Diagram 71 Northwind Traders Data Model - sample database model that has been around for years; originally developed by Microsoft - represents data model for fake Northwind Traders company, an importer/exporter of specialty foods\n",
      "Stored embedding for: New: ER Diagram 72 Relation Name Attributes Primary Key Attributes Relationships\n",
      "Stored embedding for: Relations in Northwind ●Suppliers: Suppliers and vendors of Northwind ●Customers: Customers who buy products from Northwind ●Employees: Employee details of Northwind traders ●Products: Product information ●Shippers: The details of the shippers who ship the products from the traders to the end-customers ●Orders and Order_Details: Sales Order transactions taking place between the customers & the company ●Categories: The categories a product can fall into 73\n",
      "Stored embedding for: For each of the following, compose a relational algebra query that satisﬁes the query prompt. Practice Time! 74\n",
      "Stored embedding for: Query Time! 75 1. For each supplier, provide its name, contact person, and phone number.\n",
      "Stored embedding for: Query Time! 76 2. Which products are supplied by FoodsRUs? Please include the product name and unit price.\n",
      "Stored embedding for: Query Time! 77 3. Provide a list of all product names ordered by World Market (a customer’s name).\n",
      "Stored embedding for: Query Time! 78 4. Which customers has employee Sam Johnson worked with? Provide the complete customer information. (CustomerID)\n",
      "Stored embedding for: Further Reading ●Harrington Ch5 (OReilly) ●Foundations of Computer Science - Ch 8 - The Relational Model 79\n",
      " -----> Processed 02 - The Relational Model and Rel Algebra - Instructor.pdf\n",
      "Stored embedding for: Redis and Python Integration Overview This document provides detailed information on how to use Redis with Python, specifically using Redis-py, including installation, connection setup, common commands, and example use cases in Machine Learning contexts. ⸻ Introduction to Redis-py Redis-py is the standard Redis client for Python, maintained directly by the Redis company, ensuring up-to-date compatibility and reliable functionality. • GitHub Repository: redis/redis-py • Installation (for Conda Environment): pip install redis ⸻ Connecting to the Redis Server in Python Below is a simple and clear example demonstrating how to set up a connection to a Redis server using Python: import redis redis_client = redis.Redis( host='localhost', # or '127.0.0.1' for Docker/local deployment port=6379, # default Redis port db=2, # Redis databases range from 0-15 decode_responses=True # automatically decode bytes to strings ) Explanation of parameters: • host: Hostname/IP address of Redis server. • port: Redis server port (default is 6379).\n",
      "Stored embedding for: • db: Redis supports multiple logical databases, numbered 0 through 15. • decode_responses=True: Converts byte responses to Python strings automatically, simplifying data handling. ⸻ Redis Commands (Python Examples) Redis commands correspond to the different types of data structures Redis supports: strings, lists, hashes, sets, etc. String Commands Setting and Retrieving Values: # Basic usage example: redis_client.set('clickCount:/abc', 0) redis_client.incr('clickCount:/abc') value = redis_client.get('clickCount:/abc') print(f'Click count = {value}') Setting and Retrieving Multiple Values: # Multiple key-value pairs redis_client.mset({'key1': 'val1', 'key2': 'val2', 'key3': 'val3'}) values = redis_client.mget('key1', 'key2', 'key3') print(values) # Outputs: ['val1', 'val2', 'val3'] Common String Commands: • Set Commands: set(), mset(), setex(), msetnx(), setnx() • Get Commands: get(), mget(), getex(), getdel() • Numeric Increment/Decrement: incr(), decr(), incrby(), decrby() • String Length and Append: strlen(), append() ⸻ List Commands\n",
      "Stored embedding for: Lists in Redis are ordered collections of strings. Basic Example: # Adding elements to a Redis list: redis_client.rpush('names', 'mark', 'sam', 'nick') # Retrieving all elements from the list: print(redis_client.lrange('names', 0, -1)) # Outputs: ['mark', 'sam', 'nick'] Common List Commands: • Left-side operations: lpush(), lpop(), lset(), lrem() • Right-side operations: rpush(), rpop() • List queries: lrange(), llen(), lpos() • Advanced operations: Moving elements between lists, simultaneous popping from multiple lists, etc. ⸻ Hash Commands Redis hashes allow storage and retrieval of structured key-value data. Basic Example: redis_client.hset('user-session:123', mapping={ 'first': 'Sam', 'last': 'Uelle', 'company': 'Redis', 'age': 30 }) user_data = redis_client.hgetall('user-session:123') print(user_data) # Outputs: {'first': 'Sam', 'last': 'Uelle', 'company': 'Redis', 'age': '30'} Common Hash Commands: • Setting/Getting: hset(), hget(), hgetall() • Key Operations: hkeys() • Existence and Deletion: hdel(), hexists() • Information retrieval: hlen(), hstrlen()\n",
      "Stored embedding for: ⸻ Redis Pipelines Pipelines are used to batch multiple commands to reduce network latency, thus enhancing performance significantly. r = redis×Redis(decode_responses=True) pipe = r×pipeline() # Batch setting values: for i in range(5): pipe.set(f\"seat:{i}\", f\"#{i}\") results = pipe.execute() print(results) # Outputs: [True, True, True, True, True] # Batch retrieval: pipe = r×pipeline() pipe.get(\"seat:0\").get(\"seat:3\").get(\"seat:4\") retrieved_values = pipe.execute() print(retrieved_values) # Outputs: ['#0', '#3', '#4'] ⸻ Redis in Machine Learning & Data Science Context Redis is increasingly being used in ML contexts due to its high performance and flexibility, primarily as a feature store. Simplified ML Example (Feature Store): Data Source: Raw data stored in Amazon S3 as CSV files. • Example Data: user,price,date Alex,$10,1 Jan 2022 Alex,$20,1 Feb 2022 Alex,$30,1 Mar 2022\n",
      "Stored embedding for: Melody,$5,1 Jan 2022 Melody,$15,1 Feb 2022 • Data Transformations using Spark or similar tools: SELECT user, avg(price) FROM raw_data GROUP BY user • Result: • Inference (real-time) data stored in Redis: user | avg(price) Alex | $20 Melody| $10 • Historical (training) data stored back in Amazon S3. ML Workflow Integration: • Data flows through batch processing (e.g., Pandas, Spark, DBT). • Processed data moves into an Offline Store (e.g., Snowflake). • Features are registered and defined centrally in a Registry. • Redis acts as an Online Feature Store providing extremely low latency (typically <10ms) for real-time inference serving. • Models access data from the online store to serve predictions. Benefit: Redis provides immediate, high-speed data retrieval suitable for model serving scenarios. ⸻ Summary of Redis Advantages: • High performance with extremely low latency. • Data structure versatility (strings, lists, hashes, etc.). • Easy integration with Python and ML workflows. • Suitable for real-time feature serving in production environments. Redis has become an essential tool in modern data and machine learning pipelines due to its efficiency, simplicity, and speed. ⸻\n",
      " -----> Processed Redis and Python Integration.pdf\n",
      "Stored embedding for: Document Databases & MongoDB Detailed Description (Cover Image as Text): • The background typically features a clean layout with the Northeastern University logo or branding (depending on how the slide was designed). • The text is centered, reading “Document Databases & MongoDB” in a larger font. • The subtitle “DS 5300” indicates the course code. • Below, the instructor’s name, “Mark Fontanot, PhD,” is listed, along with “Northeastern University.” This cover page sets the stage for a presentation about modern document databases—particularly MongoDB—and how they differ from or complement traditional relational databases. ⸻ Slide 2: Introduction to Document Databases Key Points: • A Document Database is a type of database that stores data as semi- structured documents, usually in JSON format. • These databases are designed to be simpler, flexible, and scalable. Expanded Explanation: • Semi-structured means that the data does not strictly follow a fixed schema like relational tables do. Instead, each “document” can contain varying sets of fields and nested structures. • JSON (JavaScript Object Notation) is a lightweight data-interchange format that is human-readable and widely used in modern web applications. • Document databases allow you to store data in a way that often more closely matches the objects in your application’s code, leading to less “impedance mismatch” compared to traditional relational models. • “Simpler” often refers to fewer steps when mapping objects from code to database documents, while “flexible” and “scalable” refer to the ease of horizontal scaling and handling evolving data structures. ⸻ Slide 3: What Is JSON? Key Points: • JSON (JavaScript Object Notation) is a lightweight, text-based data interchange format.\n",
      "Stored embedding for: • It is language-independent but uses conventions familiar to programmers of the C-family of languages (C, C++, Java, JavaScript, etc.). • JSON is easy for humans to read and write, and easy for machines to parse and generate. • It is widely used by modern applications, web APIs, and configurations. Expanded Explanation: • JSON typically represents data in key-value pairs. • Example structure: { \"name\": \"Alice\", \"age\": 30, \"skills\": [\"Python\", \"Databases\", \"Machine Learning\"] } • JSON has become a de facto standard for RESTful APIs, configuration files, and data storage in NoSQL/document-oriented databases. ⸻ Slide 4: JSON Syntax Key Points: • Objects are delimited by curly braces { }. • Arrays are delimited by square brackets [ ]. • Key-value pairs use the format \"key\": value, and strings must be in double quotes. • Values can be strings, numbers, objects, arrays, booleans, or null. Expanded Explanation: • Proper formatting and strict rules around quotes and commas are important for valid JSON. • Missing commas, incorrect bracket usage, or single quotes instead of double quotes can break JSON. • JSON data can nest objects within objects and arrays, enabling complex hierarchies. ⸻ Slide 5: Binary JSON (BSON) Key Points:\n",
      "Stored embedding for: • BSON stands for Binary JSON. • MongoDB uses BSON internally as its storage format. • BSON extends the JSON model with additional data types (e.g., for dates, binary data) and is more efficient for machine processing. Expanded Explanation: • BSON is designed to be lightweight and traversable. It encodes length information at the beginning of objects and arrays, making it easier and faster to parse. • Although MongoDB “speaks” JSON to external clients, the actual storage on disk is in BSON, which supports features like 32-bit/64-bit integers, floating-point numbers, timestamps, etc. ⸻ Slide 6: XML (Extensible Markup Language) Key Points: • XML was a precursor to JSON as an exchange format. • XML separates data from presentation and uses tags for structure. • It is highly extensible, but can be verbose compared to JSON. Expanded Explanation: • XML stands for Extensible Markup Language and was widely used for data exchange in the early days of web services (SOAP, RSS, etc.). • While still used in many enterprise environments, XML has largely been supplanted by JSON in many modern REST APIs due to JSON’s simpler syntax. • XML’s flexibility lies in the ability to define custom tags, but this can also lead to more complex parsing logic. ⸻ Slide 7: XML-Related Tools/Technologies Key Points: • XPath: A query language for selecting nodes from an XML document. • XQuery: A functional query language that can query and transform XML data. • XSLT: A language for transforming XML documents into other formats. Expanded Explanation: • These tools can filter, query, and restructure XML data. • While powerful, these technologies can be more cumbersome compared to the simpler query capabilities of JSON-based document databases.\n",
      "Stored embedding for: ⸻ Slide 8: Why Document Databases? Key Points: • Document databases are useful when data does not fit well into a tabular schema or when the schema is constantly evolving. • They can handle large volumes of data and support high throughput (e.g., thousands of requests per second). • They are often easier to scale horizontally across multiple servers (sharding). Expanded Explanation: • In modern web applications, data often changes shape rapidly. For instance, a user profile might gain new fields as features evolve. Document databases make these schema changes straightforward. • High scalability and flexible schemas make document databases a good choice for real-time analytics, mobile apps, content management systems, and more. ⸻ Slide 9: MongoDB – Introduction Key Points: • MongoDB is one of the most popular document-oriented databases. • Known for storing data in JSON-like documents (internally BSON). • Provides high availability, horizontal scaling, and robust query capabilities. Expanded Explanation: • MongoDB was developed by a company formerly known as 10gen; the database quickly became a leader in the NoSQL movement. • It has a flexible schema design, meaning each document in a collection can have a unique structure. ⸻ Slide 10: MongoDB Background Key Points: • Started in 2007 after DoubleClick was acquired by Google; engineers realized relational databases were hitting limitations with massive ad-serving\n",
      "Stored embedding for: workloads (~400,000 ads per second). • The name “Mongo” is short for “Humongous,” reflecting its design for very large data volumes. • MongoDB Atlas was released in 2016, offering a cloud-hosted DBaaS (Database as a Service). Expanded Explanation: • MongoDB’s founders sought a more scalable solution than traditional relational databases could provide at the time. • MongoDB, Inc. (formerly 10gen) offers both a community edition (open source) and enterprise versions with additional features. • The success of Atlas shows how important managed cloud services have become, as it simplifies deployment and scaling. ⸻ Slide 11: MongoDB Structure Key Points: • A MongoDB database consists of one or more collections. • A collection is analogous to a table in relational databases, but without a fixed schema. • Each collection contains multiple documents (analogous to rows). Expanded Explanation: • Database > Collection > Document is the hierarchy. • There is no enforced schema at the collection level, meaning each document can have different fields. • This flexibility allows for faster development cycles and easier updates to data models. ⸻ Slide 12: MongoDB Documents Key Points: • There is no predefined schema for documents. • Every document in a collection can have different data or schema. • Documents are typically stored as BSON, but displayed as JSON when retrieved. Expanded Explanation: • A single collection could store user profiles that differ in the number of fields. For example, some users might have a “hobbies” field, while others do\n",
      "Stored embedding for: not. • The lack of a predefined schema can be a double-edged sword: it offers flexibility but requires careful handling of inconsistent data. ⸻ Slide 13: Relational vs. Mongo/Document DB Key Points: • Relational databases store data in tables with rows and columns, enforcing a strict schema. • Document databases store data in collections of flexible JSON-like documents. • Each approach has advantages: relational excels at complex queries with joins and transactional integrity, while document databases excel at flexibility, speed, and horizontal scalability. Expanded Explanation: • Relational: Best for scenarios requiring strong consistency, complex joins, and ACID transactions (banking, financial records). • Document: Best for fast-evolving applications, content management, analytics, or real-time data feeds where structure is not always consistent. ⸻ Slide 14: MongoDB Features Key Points: • Rich Query Support: Full CRUD (Create, Read, Update, Delete) operations. • Indexes: Supports primary and secondary indexes on any field. • Replication: Provides high availability with replica sets. • Sharding: Enables horizontal scaling. • Automatic Failover: In replica sets, one node can automatically take over if the primary fails. Expanded Explanation: • Queries can be as simple or as complex as needed, including aggregation pipelines that transform and analyze data in place. • MongoDB’s replication model allows for distributed clusters that remain online even if one node goes down. • Sharding splits data across multiple machines, balancing the load. ⸻\n",
      "Stored embedding for: Slide 15: MongoDB Versions Key Points: • MongoDB Atlas: Managed MongoDB service in the cloud (DBaaS). • MongoDB Enterprise: Commercial edition with advanced security and features. • MongoDB Community: Free, self-managed version of MongoDB. Expanded Explanation: • Atlas simplifies deployment, scaling, and maintenance by handling backups, monitoring, and upgrades automatically. • Enterprise edition includes features like encrypted storage engines, LDAP integration, and advanced auditing. • Community edition is open source and can be installed on-premises or on your own servers in the cloud. ⸻ Slide 16: Interacting with MongoDB Key Points: • mongosh (MongoDB Shell) provides a command-line interface. • MongoDB Compass is a GUI tool for visually interacting with MongoDB. • Other drivers/tools: PyMongo (Python), Mongoose (Node.js), DataGrip (JetBrains), etc. Expanded Explanation: • The shell is useful for quick commands, maintenance, and troubleshooting. • MongoDB Compass helps visualize collections, indexes, and run queries in a more user-friendly environment. • Drivers in various programming languages allow developers to integrate MongoDB directly into applications. ⸻ Slide 17: MongoDB Community Edition in Docker Key Points: • You can run MongoDB in a container using Docker. • Create a container, specify ports and credentials, and map local storage. • Docker images help maintain consistent development environments.\n",
      "Stored embedding for: Expanded Explanation: • For example, using a command like: docker run -d -p 27017:27017 --name mongodb -v /localpath:/data/db mongo:latest This starts a MongoDB container, maps port 27017, and mounts a local directory for data persistence. • Docker simplifies setup and teardown of development environments. ⸻ Slide 18: MongoDB Compass Key Points: • A GUI tool for interacting with MongoDB. • Allows you to visualize databases, collections, and indexes. • You can perform CRUD operations and run queries without using the shell. Expanded Explanation: • MongoDB Compass is ideal for beginners or those who prefer a graphical interface. • It provides schema analysis, data validation, and aggregation pipeline builders. ⸻ Slide 19: Load Mflix Sample Data Set Key Points: • In Compass, create a new database named mflix (a sample movie database). • Download the mflix sample JSON or use built-in sample data sets. • Import JSON files into new collections in the mflix database. Expanded Explanation: • MongoDB often provides sample datasets (e.g., sample_mflix, sample_airbnb, sample_supplies) for educational or demonstration purposes. • Once loaded, you can explore the data in Compass or the shell, run queries, and practice.\n",
      "Stored embedding for: ⸻ Slide 20: Creating a Database and Collection Key Points: • In MongoDB, you don’t explicitly create a database until you insert data. • You can switch to a database in the shell using use <dbName>. • You can then insert a document into a collection, and MongoDB will create both the database and collection if they do not already exist. Example: use myDatabase db.myCollection.insertOne({ name: \"Test\", value: 123 }) Expanded Explanation: • This dynamic creation is different from relational databases, where you must explicitly define schemas. • MongoDB’s flexibility reduces initial overhead but requires you to track your structure carefully. ⸻ Slide 21: mongosh – Mongo Shell: find() Key Points: • find() is analogous to SELECT in SQL. • Basic usage: collection.find( { /* filters */ }, { /* projections */ } ) • Filters define which documents to retrieve, projections define which fields to include or exclude. Expanded Explanation: • If you call collection.find() with no arguments, it returns all documents in the collection. • Filters use JSON-like syntax to specify conditions, e.g., { name: \"Alice\" }. • Projections let you control which fields are returned, e.g., { name: 1, _id:\n",
      "Stored embedding for: 0 }. ⸻ Slide 22: SQL vs. Mongo Shell – Basic “SELECT * FROM users” SQL: SELECT * FROM users; MongoDB: use mflix db.users.find() Expanded Explanation: • In MongoDB, you switch to the database (use mflix) and then call db.users.find() to see all user documents. • MongoDB doesn’t have the concept of “*” for selecting all fields, so an empty projection object or simply calling .find() without a projection will return all fields. ⸻ Slide 23: SQL vs. Mongo Shell – Basic Filter SQL: SELECT * FROM users WHERE name = 'Davos Seaworth'; MongoDB: db.users.find({ name: \"Davos Seaworth\" }) Expanded Explanation: • MongoDB uses a JSON-style filter object. • The equality operator in MongoDB is implicit when you use a key-value pair like { name: \"Davos Seaworth\" }. ⸻ Slide 24: Filtering by Rated Field\n",
      "Stored embedding for: SQL: SELECT * FROM movies WHERE rated IN ('PG', 'PG-13'); MongoDB: db.movies.find({ rated: { $in: [\"PG\", \"PG-13\"] } }) Expanded Explanation: • $in is a MongoDB operator that checks if the field’s value matches any value in the specified array. ⸻ Slide 25: Filtering by IMDB Rating >= 7 Key Points: • Return movies with an IMDb rating of at least 7. MongoDB Example: db.movies.find( { \"imdb.rating\": { $gte: 7 } } ) Expanded Explanation: • Note how MongoDB uses dot notation (\"imdb.rating\") to refer to a nested field. • $gte stands for “greater than or equal to.” ⸻ Slide 26: Complex Query with Multiple Conditions Requirement: • Return movies from the movies collection which were released in 2010 and either: • Won at least 5 awards, OR • Have a genre of Drama.\n",
      "Stored embedding for: MongoDB Example: db.movies.find({ $and: [ { year: 2010 }, { $or: [ { awards: { $gte: 5 } }, { genre: \"Drama\" } ] } ] }) Expanded Explanation: • $and and $or are logical operators. • You can nest logical operators for more complex queries. ⸻ Slide 27: Comparison Operators Reference Common MongoDB Operators: • $eq (equals) • $gt (greater than) • $gte (greater than or equal) • $in (in array of values) • $lt (less than) • $lte (less than or equal) • $ne (not equal) • $nin (not in array of values) Expanded Explanation: • These operators let you build flexible queries. • They can be combined with logical operators ($and, $or, $not, $nor) for complex conditions. ⸻ Slide 28: mongosh – countDocuments() Key Points: • countDocuments() is analogous to SELECT COUNT(*) in SQL.\n",
      "Stored embedding for: • Example: db.movies.countDocuments({ genre: \"Drama\" }) Expanded Explanation: • You can pass a filter object to countDocuments() to count only matching documents. • This is typically more accurate than the older count() method because it respects the filter and uses indexes efficiently. ⸻ Slide 29: mongosh – Projection Key Points: • Projection specifies which fields to include (1) or exclude (0). • Example: db.movies.find( { year: 2010 }, { title: 1, _id: 0 } ) • 1 means include the field, 0 means exclude. • _id: 0 commonly excludes the _id field. Expanded Explanation: • Projections can reduce bandwidth usage and clutter if you only need specific fields. • More advanced projections can reshape documents using aggregation pipelines. ⸻ Slide 30: PyMongo Key Points: • PyMongo is the official Python library for interacting with MongoDB. Expanded Explanation: • It provides classes and methods to connect, query, insert, update, and delete documents from Python code.\n",
      "Stored embedding for: • Great for integrating MongoDB with data science workflows in Python (e.g., using Jupyter notebooks). ⸻ Slide 31: PyMongo – Basic Usage Connection Example: from pymongo import MongoClient client = MongoClient(\"mongodb://localhost:27017/\") Expanded Explanation: • MongoClient handles the connection to a local or remote MongoDB instance. • You can pass in authentication details, replica set information, etc., as needed. ⸻ Slide 32: Getting a Database and Collection in PyMongo Example: db = client[\"myDatabase\"] collection = db[\"myCollection\"] Expanded Explanation: • Once you have a client, you can access a database by name (like a dictionary key). • You can then access a collection similarly. • Operations on collection now directly interact with the MongoDB server. ⸻ Slide 33: Inserting a Single Document in PyMongo Example: post = { \"title\": \"My First Post\", \"content\": \"Hello, MongoDB!\", \"tags\": [\"intro\", \"pymongo\"]\n",
      "Stored embedding for: } inserted_id = collection.insert_one(post).inserted_id print(inserted_id) Expanded Explanation: • insert_one() returns an object containing the _id of the inserted document. • _id is a unique identifier automatically generated if not specified. • PyMongo also supports insert_many() for bulk inserts. ⸻ Slide 34: Count Documents in a Collection (PyMongo) Key Points: • The PyMongo equivalent of countDocuments() can be done via the aggregation pipeline or a helper method, depending on the version of PyMongo. • Example: count = collection.count_documents({}) print(count) This counts all documents in the collection. Expanded Explanation: • You can pass a filter object to count_documents({ ... }) to count only matching documents. • For advanced counting and metrics, consider using the MongoDB aggregation framework. ⸻ Final Notes This presentation covers the fundamentals of document databases, JSON/BSON data formats, and MongoDB. Key takeaways include: • Document Databases store semi-structured data in JSON-like formats, offering flexibility and scalability. • MongoDB is a leading document database, well-suited for modern applications requiring agile schema evolution and horizontal scaling. • Core Operations in MongoDB—like find(), insert(), update(), delete()— are analogous to SQL’s CRUD but use JSON-based query syntax.\n",
      "Stored embedding for: • Tools such as the MongoDB shell (mongosh), MongoDB Compass, and language-specific drivers (e.g., PyMongo) make it straightforward to integrate MongoDB into various development workflows. By understanding these concepts, you can decide if a document-oriented approach is right for your application’s data needs and start leveraging MongoDB’s powerful feature set.\n",
      " -----> Processed Document Databases & MongoDB.pdf\n",
      "Stored embedding for: AWS Introduction 1. AWS Introduction Title Slide Information • Topic: AWS Introduction Key Point: This introduction sets the stage for learning about AWS, a leading cloud service provider. It highlights that the course or lecture will cover the fundamental concepts, service categories, infrastructure, and shared responsibility model of AWS. ⸻ 2. Amazon Web Services Overview What is AWS? • Leading Cloud Platform: AWS offers over 200 fully featured services. It has one of the largest market shares in the cloud computing industry. • High Availability: AWS achieves reliability and redundancy by hosting its services in multiple regions and availability zones around the world. This global footprint ensures minimal downtime and fast response times. • Scalability on Demand: AWS can rapidly scale resources (e.g., compute power, storage, databases) up or down to meet workload demands. • Pay-As-You-Go Model: Instead of paying for and maintaining on- premises hardware, you pay only for what you use on AWS. This cost model often reduces capital expenditure and aligns expenses with usage. Why is AWS so Popular? • Rapid deployment of new services • Broad set of tools for developers and businesses • Large community and ecosystem support • Continuous innovation in emerging areas such as AI/ML, IoT, and serverless computing ⸻ 3. History of AWS • Initial Launch (2006): AWS started with three core services: • Amazon S3 (Simple Storage Service) • Amazon EC2 (Elastic Compute Cloud) • Amazon SQS (Simple Queue Service) • Rapid Growth: From these initial offerings, AWS quickly expanded to over 200 services covering compute, storage, databases, analytics, machine\n",
      "Stored embedding for: learning, and more. • Market Share: • AWS: ~33% • Microsoft Azure: ~22% • Google Cloud Platform (GCP): ~9% • Other providers share the remaining portion of the market. • Global Reach: AWS operates large-scale data centers worldwide, generating billions of dollars in revenue and serving a wide variety of customers, from startups to large enterprises. Key Takeaway: AWS began with a small set of foundational cloud services and evolved into a massive ecosystem that dominates the cloud market. ⸻ 4. AWS Service Categories AWS groups its many offerings into categories, each addressing a different layer of infrastructure, application development, or data management. Common categories include: 1. Compute – Services that provide virtual servers, containers, and serverless functions (e.g., Amazon EC2, AWS Lambda, AWS Fargate). 2. Storage – Services to store and manage data (e.g., Amazon S3, Amazon EBS, Amazon EFS). 3. Databases – Managed database offerings (e.g., Amazon RDS, Amazon DynamoDB, Amazon Redshift). 4. Networking & Content Delivery – Services that help configure virtual networks and deliver content globally (e.g., Amazon VPC, Amazon CloudFront). 5. Analytics – Tools for data processing, data warehousing, and real- time analytics (e.g., Amazon EMR, AWS Glue, Amazon Kinesis). 6. Machine Learning & AI – Services providing ML platforms and AI- driven APIs (e.g., Amazon SageMaker, Amazon Rekognition). 7. Security & Identity – Services to secure AWS environments and manage identities (e.g., AWS IAM, AWS KMS). 8. Management & Governance – Tools for monitoring, auditing, and organizing AWS resources (e.g., Amazon CloudWatch, AWS CloudFormation). 9. Developer Tools – Services that streamline software development and CI/CD (e.g., AWS CodeCommit, AWS CodeBuild, AWS CodeDeploy). ⸻ 5. Cloud Models\n",
      "Stored embedding for: When discussing cloud computing, it is important to distinguish between different service models. Each model shifts certain responsibilities between the cloud provider and the customer: 1. Infrastructure as a Service (IaaS) • AWS provides fundamental building blocks (compute, storage, networking). • Customers manage operating systems, software, and configurations. • Example: Amazon EC2 (virtual servers on which you install and manage your own OS and applications). 2. Platform as a Service (PaaS) • AWS provides an environment for building, testing, and deploying applications. • Underlying infrastructure (servers, OS, runtime) is abstracted away. • Example: AWS Elastic Beanstalk (you focus on the code; AWS handles deployment and capacity). 3. Software as a Service (SaaS) • Complete applications provided by AWS or third parties, managed entirely by the service provider. • Users simply use the software; there is no need to manage or configure infrastructure or platform. • Example: AWS-provided applications or third-party software offered in the AWS Marketplace. Key Idea: Each model offers different levels of control and responsibility for the user. As you move from IaaS to PaaS to SaaS, you manage fewer details but also have less flexibility in customizing the environment. ⸻ 6. The Shared Responsibility Model (AWS Responsibilities) AWS uses a shared responsibility model for security and compliance: • AWS Responsibilities (Security of the cloud) • Protecting the infrastructure that runs AWS services (hardware, software, networking, facilities). • Maintaining physical security of data centers. • Managing the host operating system and virtualization layer for AWS compute services. • Patching and fixing flaws within the AWS global infrastructure. • Ensuring redundancy and availability of core systems.\n",
      "Stored embedding for: In other words, AWS takes care of the underlying cloud environment so that customers can focus on their applications and data. ⸻ 7. The Shared Responsibility Model (Client Responsibilities) • Client Responsibilities (Security in the cloud) • Configuration of Services: Correctly configuring security groups, access policies, encryption settings, and network parameters. • Managing Data: Ensuring that data is classified properly, encrypted if needed, and backed up according to the organization’s requirements. • Operating System and Application Maintenance: For services like EC2, the customer must patch and maintain the operating system and installed software. • Identity and Access Management: Defining IAM policies and best practices, such as using least privilege, rotating keys, and enabling multi-factor authentication (MFA). • Compliance: Ensuring that internal policies, industry regulations, and legal requirements are met at the application and data level. Key Takeaway: While AWS secures the underlying infrastructure, the customer must secure and manage everything they build on top of it. ⸻ 8. The AWS Global Infrastructure • Regions: Distinct geographical areas around the world (e.g., US East (N. Virginia), EU (Ireland), Asia Pacific (Sydney)). Each region contains multiple Availability Zones. • Availability Zones (AZs): Data centers that are physically isolated from each other within a region. AZs provide redundancy and fault tolerance; if one AZ fails, others remain available. • Edge Locations: Content delivery network (CDN) endpoints for services like Amazon CloudFront. By caching content closer to end users, AWS reduces latency and speeds up data delivery. Importance of Choosing the Right Region: • Latency: Deploy services in regions closer to your users to reduce response times. • Compliance: Some regions have specific data sovereignty or regulatory requirements.\n",
      "Stored embedding for: • Cost: Pricing can differ slightly by region, so choosing the most cost- effective region may reduce expenses. ⸻ 9. Global Infrastructure Map A typical AWS global infrastructure map shows: • Number of launched regions worldwide • Number of Availability Zones across those regions • Edge locations for services like CloudFront Key Point: AWS is continually expanding into new regions and adding new availability zones to increase resilience and global reach. ⸻ 10. Compute Services AWS offers several ways to run workloads, each suited to different needs: 1. VM-based: • Amazon EC2 (Elastic Compute Cloud): Provides resizable virtual machines. • Amazon EC2 Spot: Use spare compute capacity at discounted rates, though instances can be interrupted if capacity is needed elsewhere. 2. Container-based: • Amazon ECS (Elastic Container Service): Orchestrates Docker containers across a cluster of EC2 instances. • Amazon EKS (Elastic Kubernetes Service): Managed Kubernetes for container orchestration. 3. Serverless: • AWS Lambda: Runs code without provisioning or managing servers. You pay only for the compute time you consume. • Ideal for event-driven applications (e.g., responding to S3 uploads, API Gateway requests). ⸻ 11. Storage Services 1. Amazon S3 (Simple Storage Service) • Object storage for the internet. • Highly scalable, durable (designed for 99.999999999% durability), and secure.\n",
      "Stored embedding for: • Ideal for static assets, backups, and big data. 2. Amazon EBS (Elastic Block Store) • Block-level storage volumes for use with Amazon EC2 instances. • Persistent storage that remains even if the EC2 instance is stopped or terminated. 3. Amazon EFS (Elastic File System) • Fully managed file storage for use with AWS compute services. • Scales automatically, provides a file system interface. 4. Amazon S3 Glacier • Low-cost storage for data archiving and long-term backup. • Retrieval times vary (minutes to hours), optimized for infrequent access. 5. AWS Storage Gateway • Hybrid storage service enabling on-premises applications to use AWS cloud storage seamlessly. ⸻ 12. Database Services 1. Relational Databases: • Amazon RDS (Relational Database Service) supports MySQL, PostgreSQL, MariaDB, Oracle, Microsoft SQL Server, and Amazon Aurora. • Automated backups, patching, and scaling. 2. NoSQL Databases: • Amazon DynamoDB: Key-value and document database, fully managed, highly scalable, low latency. 3. Document Databases: • Amazon DocumentDB: Compatible with MongoDB, for JSON- based workloads. 4. Graph Databases: • Amazon Neptune: Fully managed graph database service for highly connected data (e.g., social networks, knowledge graphs). ⸻ 13. Analytics Services 1. Amazon Athena • Interactive query service that analyzes data in Amazon S3 using standard SQL. • Serverless; you pay only for the queries you run. 2. AWS Glue • ETL (Extract, Transform, Load) service that prepares data for\n",
      "Stored embedding for: analytics. • Automatically discovers schema and creates metadata in a data catalog. 3. Amazon EMR (Elastic MapReduce) • Managed Hadoop framework for processing large amounts of data at scale. • Integrates with big data frameworks like Spark and Hive. 4. Amazon Kinesis • Real-time data ingestion and streaming service. • Collect, process, and analyze real-time streaming data (e.g., logs, IoT data). 5. Amazon QuickSight • Scalable business intelligence (BI) service. • Allows creation of interactive dashboards and visualizations. ⸻ 14. Machine Learning (ML) and AI Services 1. Amazon SageMaker • Fully managed ML platform for building, training, and deploying machine learning models. • Provides hosted Jupyter notebooks, built-in algorithms, and automatic model tuning. 2. AWS AI Services (Pre-trained Models) • Amazon Rekognition: Image and video analysis (object detection, facial recognition, etc.). • Amazon Transcribe: Automated speech-to-text. • Amazon Translate: Neural machine translation for many languages. • Amazon Comprehend: Natural language processing for text analysis (sentiment, key phrases). These AI services enable developers to add advanced capabilities to applications without deep ML expertise. ⸻ 15. Important Services for Data Analytics/Engineering For anyone focusing on data analytics or data engineering, the following services are especially relevant: • Amazon EC2 and AWS Lambda (compute for data processing) • Amazon S3 (data lake, object storage) • Amazon RDS and Amazon DynamoDB (structured and NoSQL\n",
      "Stored embedding for: databases) • AWS Glue (ETL and data catalog) • Amazon Athena (serverless SQL queries on S3) • Amazon EMR (big data processing with Hadoop/Spark) • Amazon Redshift (data warehousing and analytics) ⸻ 16. AWS Free Tier AWS offers a Free Tier that provides limited access to certain services for 12 months. This allows users to experiment and learn without incurring large costs: • Amazon EC2: 750 hours/month of t2.micro or t3.micro instances (Linux/ Windows) • Amazon S3: 5 GB of standard storage (20k GET requests, 2k PUT requests) • Amazon RDS: 750 hours/month of certain database engines (with size limits) • Others: Many services have free or trial components under certain usage thresholds Key Takeaway: The Free Tier is a great way to gain hands-on experience with AWS, learn about its services, and prototype applications without worrying about high bills. ⸻ Conclusion Amazon Web Services is a comprehensive cloud platform that covers a vast range of services from compute and storage to analytics, machine learning, and more. Understanding the shared responsibility model, the global infrastructure, and the different service models (IaaS, PaaS, SaaS) is crucial to effectively leveraging AWS in a secure and cost-efficient manner. Whether you are building simple websites, data analytics pipelines, or enterprise- scale applications, AWS offers tools and services that can help streamline development, reduce overhead, and enable innovative solutions. By taking advantage of AWS’s Free Tier and carefully designing architectures to use the right services, businesses and developers can harness the power of the cloud while optimizing both performance and cost.\n",
      " -----> Processed AWS Introduction.pdf\n",
      "Stored embedding for: Moving Beyond the Relational Model Benefits of the Relational Model: - **Standardized Data Model and Query Language**: Relational databases typically use Structured Query Language (SQL), providing a universal method to manage and query data. - **ACID Compliance**: - **Atomicity**: Transactions complete entirely or not at all. - **Consistency**: Database remains in a valid state before and after transactions. - **Isolation**: Concurrent transactions don't interfere with each other. - **Durability**: Once transactions commit, changes are permanent even after system failures. - Works efficiently with highly structured data, capable of handling large datasets. - Well-established technology with extensive tooling and deep industry expertise. --- Relational Database Performance: RDBMSs improve efficiency through: - **Indexing**: Speeds data retrieval operations. - **Storage Management**: Direct control over data storage enhances performance. - **Column vs. Row-Oriented Storage**: Optimizes data access based on usage patterns. - **Query Optimization**: Automatically optimizes SQL queries for best performance. - **Caching/Prefetching**: Reduces data access latency. - **Materialized Views**: Stores query results for frequent or expensive queries. - **Precompiled Stored Procedures**: Faster execution through precompiled logic. - **Data Replication & Partitioning**: Enhances availability, fault tolerance, and parallelism. --- Transaction Processing: A **Transaction** is a group of database operations performed as a single logical unit, adhering to: - **COMMIT**: Transaction succeeds entirely. - **ROLLBACK** or **ABORT**: Transaction fails completely, undoing all changes.\n",
      "Stored embedding for: Transactions ensure: - Data Integrity - Error Recovery - Concurrency Control - Reliable Data Storage - Simplified Error Handling --- ACID Properties (Detailed): - **Atomicity**: Transactions are indivisible—either fully completed or fully aborted. - **Consistency**: Transactions transition databases from one consistent state to another, maintaining integrity constraints. - **Isolation**: Concurrent transactions don't adversely affect each other, preventing issues like: - **Dirty Reads**: Reading uncommitted changes. - **Non-repeatable Reads**: Inconsistent reads within the same transaction due to another committed transaction. - **Phantom Reads**: Different results in successive reads caused by concurrent insertions or deletions. - **Durability**: Committed transactions persist through failures. --- Isolation Issues Explained: - **Dirty Read**: Occurs when a transaction reads data modified but not committed by another transaction. This can lead to inconsistencies if the uncommitted transaction is rolled back. - **Non-repeatable Read**: A transaction reads data twice, getting different results because another transaction modified and committed the data between the two reads. - **Phantom Reads**:\n",
      "Stored embedding for: Rows appear or disappear between two reads within the same transaction due to inserts or deletes by concurrent transactions. --- Example Transaction (Money Transfer): Transactions exemplified by a bank transfer, demonstrating checks for sufficient funds, account debit, credit, logging of transactions, and error handling through rollback mechanisms if conditions fail. --- Beyond Relational Databases: Relational databases aren't ideal for: - Rapid schema evolution. - Applications where full ACID compliance is unnecessary. - Expensive JOIN operations. - Handling semi-structured/unstructured data (JSON, XML). - Efficient horizontal scaling. - Real-time or low-latency applications. --- Scalability – Vertical vs. Horizontal: - **Vertical Scaling** (scaling up): Adding resources to existing nodes, easy but limited. - **Horizontal Scaling** (scaling out): Adding more nodes, offers better scalability and fault tolerance but adds complexity. --- Distributed Systems: Defined as a collection of independent computers appearing as one system. Characteristics: - Concurrent operations - Independent failures - Lack of a global clock --- Distributed Data Storage:\n",
      "Stored embedding for: Two main strategies: - **Replication**: Same data stored on multiple nodes for redundancy and fault tolerance. - **Sharding**: Data divided across nodes to improve scalability. Relational and non-relational databases both offer replication and sharding (e.g., MySQL, PostgreSQL, CockroachDB, various NoSQL systems). Network partitions are inevitable, making partition tolerance essential for system resilience. --- CAP Theorem: States a distributed data store cannot simultaneously guarantee all three: - **Consistency**: Latest data on every read. - **Availability**: Always returns a non-error response. - **Partition Tolerance**: Continues operation despite network partitions. Systems typically prioritize two at the cost of the third. --- CAP Theorem in Practice: - **Consistency + Availability (CA)**: Prioritizes latest data but struggles with network issues. - **Consistency + Partition Tolerance (CP)**: Always returns latest data or no response during partitions. - **Availability + Partition Tolerance (AP)**: Always responds, though data might be outdated. Practical interpretation: You must prioritize which guarantees to sacrifice depending on application needs and fault tolerance requirements.\n",
      " -----> Processed Moving Beyond the Relational Model.pdf\n",
      "Stored embedding for: DS 4300 MongoDB + PyMongo Mark Fontenot, PhD Northeastern University\n",
      "Stored embedding for: PyMongo ●PyMongo is a Python library for interfacing with MongoDB instances 2 from pymongo import MongoClient client = MongoClient( ‘mongodb://user_name:pw@localhost:27017’ )\n",
      "Stored embedding for: Getting a Database and Collection 3 from pymongo import MongoClient client = MongoClient( ‘mongodb://user_name:pw@localhost:27017’ ) db = client[‘ds4300’] # or client.ds4300 collection = db[‘myCollection’] #or db.myCollection\n",
      "Stored embedding for: Inserting a Single Document 4 db = client[‘ds4300’] collection = db[‘myCollection’] post = { “author”: “Mark”, “text”: “MongoDB is Cool!”, “tags”: [“mongodb”, “python”] } post_id = collection.insert_one(post).inserted_id print(post_id)\n",
      "Stored embedding for: Find all Movies from 2000 5 from bson.json_util import dumps # Find all movies released in 2000 movies_2000 = db.movies.find({\"year\": 2000}) # Print results print(dumps(movies_2000, indent = 2))\n",
      "Stored embedding for: Jupyter Time - Activate your DS4300 conda or venv python environment - Install pymongo with pip install pymongo - Install Jupyter Lab in you python environment - pip install jupyterlab - Download and unzip > this < zip ﬁle - contains 2 Jupyter Notebooks - In terminal, navigate to the folder where you unzipped the ﬁles, and run jupyter lab 6\n",
      "Stored embedding for: ?? 7\n",
      " -----> Processed 08 - PyMongo.pdf\n",
      "Stored embedding for: NoSQL & Key-Value Databases Overview ### Distributed Databases and ACID Distributed databases often implement concurrency control to maintain data integrity and consistency. #### Pessimistic Concurrency - **Definition**: Transactions lock data to prevent other transactions from conflicting, assuming conflicts will occur. - **Purpose**: Ensures data safety. - **Implementation**: Locks resources until transaction completion (both read/ write locks). - **Analogy**: Borrowing a book from a library; if one person has it, others cannot. #### Optimistic Concurrency - **Definition**: Assumes conflicts rarely occur, transactions proceed without locking data. - **Implementation**: Adds timestamp/version columns, verifies data consistency at transaction end. - **Applicability**: - **Low-conflict systems** (analytics databases, backup systems) use optimistic concurrency for higher efficiency. - **High-conflict systems** are better with pessimistic concurrency due to frequent rollbacks. ### Introduction to NoSQL - **Historical Note**: The term \"NoSQL\" initially described relational databases without SQL, coined in 1998 by Carlo Strozzi. - **Modern Interpretation**: Typically means \"Not Only SQL\" or non-relational databases. - **Usage Context**: Designed for processing unstructured web-based data efficiently. ### CAP Theorem The CAP theorem highlights a fundamental limitation in distributed database systems, stating that only two out of three properties can simultaneously be achieved: - **Consistency**: Every user sees identical data. - **Availability**: Database remains operational despite failures. - **Partition Tolerance**: Operates despite network partitions. #### CAP Combinations: - **Consistency + Availability (CA)**: Responses always reflect the most recent data but may fail during network partitions.\n",
      "Stored embedding for: - **Consistency + Partition Tolerance (CP)**: Guarantees latest data or no response at all. - **Availability + Partition Tolerance (AP)**: Guarantees responses but potentially outdated data. ### ACID Alternative: BASE (for distributed systems) - **Basically Available**: - Data availability prioritized over strict consistency. - Occasional inconsistencies or \"unreliable\" states permitted. - **Soft State**: - System state may change without direct input, driven by eventual consistency. - Data stores aren't immediately write-consistent. - **Eventual Consistency**: - All system replicas will eventually converge, even if temporary inconsistencies occur. ### Key-Value Databases - **Data Model**: Simple key-value pairs. - **Design principles**: - **Simplicity**: Straightforward CRUD operations. - **Speed**: Often O(1) retrieval due to hash-based implementations. - **Scalability**: Easy horizontal scaling and eventual consistency guarantee. #### Key-Value Database Use Cases: - **Data Science**: Experiment results, feature stores, and model monitoring metrics. - **Software Engineering**: Session management, user profiles, shopping carts, caching layers. ### Redis Database - **Definition**: Redis (Remote Directory Server) is an open-source, in-memory database and key-value store. - **Performance**: Over 100,000 operations per second possible. - **Capabilities**: Supports durability via snapshots or append-only logs, developed initially in C++ (2009). - **Limitations**: Only supports key-based lookups, no complex queries or secondary indexing. #### Redis Data Types: - **Keys**: Usually strings or binary sequences. - **Values**: Strings, lists, sets, sorted sets, hashes, and geospatial data. #### Redis Interaction - **Databases**: Offers 16 default databases, numbered 0-15.\n",
      "Stored embedding for: - **Commands**: `SET`, `GET`, `DEL`, `EXISTS`, `KEYS`, `SELECT`. - **Atomic Increment/Decrement**: `INCR`, `INCRBY`, `DECR`, `DECRBY`. - **Conditional Sets**: `SETNX` only sets values if a key does not exist. ### Redis Data Structures and Commands #### Hashes - Collection of field-value pairs for object-like structures. - Commands: `HSET`, `HGET`, `HGETALL`, `HMGET`, `HINCRBY`. - Use cases: Session tracking, event tracking, structured user data. #### Lists - Linked lists storing ordered string values. - Operations: Implement stacks (`LPUSH`, `LPOP`) and queues (`LPUSH`, `RPOP`). - Other operations: `LLEN`, `LRANGE`. - Use cases: Queues, message passing, logging, batch processing. #### Sets - Unordered collection of unique elements. - Commands: `SADD`, `SISMEMBER`, `SCARD`, `SINTER`, `SDIFF`, `SREM`, `SRANDMEMBER`. - Use cases: Tracking unique items, access control lists, social networking structures. ### Redis JSON Type - Fully supports JSON standards using JSONPath. - Fast navigation and storage via internal binary tree structures. - Efficient access to sub-elements. ### Redis in Docker - Installation and setup easily via Docker Desktop. - Default Redis port is **6379**. - Important Security Note: Avoid exposing Redis port directly to prevent vulnerabilities, especially in production environments. ### Connecting to Redis from Tools - Example: DataGrip. - Steps: Create new Redis Data Source, ensure port **6379** is correct, test connection. This expanded explanation details each component from your slides, enhancing clarity and providing thorough coverage of NoSQL databases, key-value stores, Redis, concurrency strategies, and the CAP theorem.\n",
      " -----> Processed NoSQL & Key-Value Databases Overview.pdf\n",
      "Stored embedding for: Comprehensive Document on Data Structures, Databases, and Distributed Systems This document provides an extensive and detailed exploration of key concepts in data structures, database systems, distributed computing, and their applications in modern computing environments. Each section addresses specific topics with in-depth explanations, additional context, practical examples, and insights into their implications for system design and performance. The goal is to expand each point thoroughly to ensure a comprehensive understanding of these foundational concepts. 1. Data Structures Data structures are the backbone of efficient data organization and manipulation in computer science. Among the most fundamental are contiguous arrays and linked lists, which differ significantly in how they store data, how they are accessed, and how they perform under various operations. A detailed comparison of these structures, including their memory allocation strategies, access times, and use cases, is critical for understanding their practical applications. 1.1 Lists and Memory Allocation 1.1.1 Contiguous Arrays Memory Allocation: Arrays allocate memory as a single, continuous block in RAM. This contiguous arrangement means that all elements are stored sequentially, with no gaps between them, allowing the operating system and hardware to manage memory efficiently. Technical Insight: In a language like C, declaring int arr[10]; reserves a block of memory equal to 10 * sizeof(int) bytes (e.g., 40 bytes on a system where an integer is 4 bytes). The starting address of this block, known as the base address, is used to compute the location of any element. Real-World Example: In image processing, a 2D array might represent a pixel grid (e.g., 1920x1080 for a Full HD image), where each element stores a color value. The contiguous layout ensures fast access to any pixel via its coordinates. Access Time: Arrays provide O(1) (constant-time) access to elements by index. This efficiency stems from the ability to calculate an element’s memory address directly using the formula: address = base + index * sizeof(element) . Detailed Explanation: If the base address of an array is 0x1000 and each element is 4 bytes, the address of arr[5] is 0x1000 + 5 * 4 = 0x1014 . This arithmetic operation is performed by the CPU in a single instruction, making access instantaneous regardless of array size. Performance Note: This assumes the array fits in memory and is not swapped to disk. Cache locality also enhances performance, as nearby elements are often loaded into the CPU cache together. Insertion and Deletion: Insertion: Adding an\n",
      "Stored embedding for: memory address directly using the formula: address = base + index * sizeof(element) . Detailed Explanation: If the base address of an array is 0x1000 and each element is 4 bytes, the address of arr[5] is 0x1000 + 5 * 4 = 0x1014 . This arithmetic operation is performed by the CPU in a single instruction, making access instantaneous regardless of array size. Performance Note: This assumes the array fits in memory and is not swapped to disk. Cache locality also enhances performance, as nearby elements are often loaded into the CPU cache together. Insertion and Deletion: Insertion: Adding an element at a specific position (e.g., the middle) requires shifting all subsequent elements to the right to make space, an operation with O(n) time complexity, where n is the number of elements. Step-by-Step Example: For an array [1, 2, 3, 4, 5] , inserting 6 at index 2 results in [1, 2, 6, 3, 4, 5] . Elements at indices 2, 3, and 4 (values 3, 4, 5) must shift one position right, requiring 3 moves. Edge Case: If the array is full, insertion requires resizing—allocating a new, larger block of memory (e.g., doubling the size), copying all existing elements, and then inserting the new element, which is also O(n). Deletion: Removing an element requires shifting all subsequent elements left to fill the gap, also O(n). Example: Deleting 3 from [1, 2, 3, 4, 5] at index 2 results in [1, 2, 4, 5] , with elements 4 and 5 shifting left. Trade-Off: The cost of shifting makes arrays inefficient for frequent insertions or deletions, especially in large datasets. Use Cases: Arrays excel in scenarios requiring fast, random access with a fixed or rarely changing size, such as lookup tables, mathematical matrices, or static datasets like a list of days in a week.\n",
      "Stored embedding for: in a week.\n",
      "Stored embedding for: Practical Example: In a video game, an array might store the positions of all static objects (e.g., trees in a forest), allowing quick retrieval by index for rendering. 1.1.2 Linked Lists Memory Allocation: Linked lists use a node-based structure, where each node contains two parts: the data itself and a pointer to the next node (in a singly linked list) or pointers to both the next and previous nodes (in a doubly linked list). Memory is allocated dynamically for each node, meaning nodes can be scattered across RAM rather than stored contiguously. Technical Insight: In C++, a node might be defined as struct Node { int data; Node* next; }; . Each node is created with new Node() , allocating memory wherever space is available. Real-World Example: In a music playlist, each song could be a node with a pointer to the next song, allowing the list to grow or shrink as songs are added or removed without requiring contiguous memory. Access Time: Accessing an element by index requires traversing the list from the head (or tail in some doubly linked implementations), resulting in O(n) time complexity. Detailed Explanation: To access the 5th element, the program must follow pointers: head → node1 → node2 → node3 → node4 → node5 , checking each node sequentially. This linear traversal contrasts sharply with arrays’ direct access. Performance Note: Cache misses are more frequent with linked lists because nodes are not necessarily adjacent in memory, reducing the benefits of CPU caching compared to arrays. Insertion and Deletion: Efficient Operations: When a pointer to the target node is known, insertion or deletion is O(1) because it involves only updating pointers, not shifting data. Insertion Example: To insert a new node with value 6 after a node with value 2 in [1, 2, 3] , the steps are: 1. Create new_node with data = 6 . 2. Set new_node.next = 2.next (pointing to 3). 3. Set 2.next = new_node . Result: [1, 2, 6, 3] . Deletion Example: To delete the node with value 2 from [1, 2, 3] , adjust the previous node’s pointer: 1.next = 2.next , resulting in [1, 3] . The deleted node is then freed from memory. Locating Nodes: If the position is specified by index rather than a pointer, finding the target node takes O(n) time, negating some of the efficiency for random modifications. Use Cases: Linked lists shine\n",
      "Stored embedding for: the steps are: 1. Create new_node with data = 6 . 2. Set new_node.next = 2.next (pointing to 3). 3. Set 2.next = new_node . Result: [1, 2, 6, 3] . Deletion Example: To delete the node with value 2 from [1, 2, 3] , adjust the previous node’s pointer: 1.next = 2.next , resulting in [1, 3] . The deleted node is then freed from memory. Locating Nodes: If the position is specified by index rather than a pointer, finding the target node takes O(n) time, negating some of the efficiency for random modifications. Use Cases: Linked lists shine in scenarios with frequent insertions and deletions, especially when the position is known or when the size is dynamic and unpredictable. Practical Example: In an operating system’s process scheduler, a linked list might track active processes, allowing quick addition or removal as processes start or terminate. 1.1.3 When Linked Lists Are Faster Linked lists outperform arrays in specific situations due to their structural advantages: Frequent Insertions and Deletions in the Middle: Unlike arrays, linked lists do not require shifting elements. Once the insertion or deletion point is located (or if a pointer is already available), the operation is O(1). Detailed Scenario: In a text editor managing a document as a linked list of characters, inserting a letter in the middle (e.g., after the 50th character) involves traversing to the 50th node (O(n)) and then linking a new node in O(1). In contrast, an array would shift all subsequent characters (O(n)), making it slower for large documents. Real-World Example: A web browser’s history feature might use a doubly linked list to allow efficient insertion of new pages and removal of old ones, with pointers enabling quick navigation forward and backward.\n",
      "Stored embedding for: Dynamic Resizing: Adding elements to a linked list avoids the need to reallocate and copy an entire memory block, as required in arrays when they reach capacity. Each new node is allocated independently, making growth seamless and avoiding the O(n) resizing cost. Detailed Example: In a queue handling customer requests in a call center, a linked list can grow indefinitely as calls come in, with each new request appended in O(1) if the tail pointer is maintained. An array-based queue hitting its limit would require resizing, copying all existing requests, and potentially disrupting service. Memory Efficiency in Sparse Data: Linked lists only allocate memory for actual elements, whereas arrays may waste space if pre-allocated but underutilized. This is particularly useful in memory-constrained environments. Example: A sparse matrix (e.g., a large grid with mostly zeros) could be represented as a linked list of non-zero elements, saving memory compared to a full array representation. 2. Balanced Binary Trees and Tree Rotations Balanced binary trees are advanced data structures that maintain logarithmic height to ensure efficient operations like searching, insertion, and deletion. AVL trees, named after their inventors Adelson-Velsky and Landis, are a classic example of self-balancing binary search trees (BSTs). Understanding their mechanics, including how they use rotations to maintain balance, is key to appreciating their role in memory-based applications. 2.1 AVL Trees Definition: An AVL tree is a BST where the height difference between the left and right subtrees of any node—known as the balance factor —is at most 1. This strict balancing ensures that the tree’s height remains O(log n), where n is the number of nodes, guaranteeing efficient operations. Balance Factor Calculation: For any node, balance_factor = height(left_subtree) - height(right_subtree) . Acceptable values are -1 , 0 , or 1 . A value outside this range indicates an imbalance requiring correction. Visual Example: A tree with root 10 , left child 5 , and right child 15 has a balance factor of 0 (both subtrees have height 1), assuming no further children. Insertion Process: BST Rules: Insertions follow standard BST logic: values less than the current node go left, values greater go right. Step-by-Step Example: 1. Start with an empty tree. 2. Insert 10 : Tree becomes [10] . 3. Insert 5 : Tree becomes [10, 5] , with 5 as the left child of 10 . Balance factor of 10 is -1 (left height 1, right height 0).\n",
      "Stored embedding for: requiring correction. Visual Example: A tree with root 10 , left child 5 , and right child 15 has a balance factor of 0 (both subtrees have height 1), assuming no further children. Insertion Process: BST Rules: Insertions follow standard BST logic: values less than the current node go left, values greater go right. Step-by-Step Example: 1. Start with an empty tree. 2. Insert 10 : Tree becomes [10] . 3. Insert 5 : Tree becomes [10, 5] , with 5 as the left child of 10 . Balance factor of 10 is -1 (left height 1, right height 0). 4. Insert 15 : Tree becomes [10, 5, 15] , with 15 as the right child of 10 . Balance factor of 10 is 0 (both subtrees height 1). Imbalance Detection: After each insertion, the balance factor of every node along the path from the root to the inserted node is recalculated. If any node’s balance factor becomes 2 or -2 , the tree is imbalanced, and rotations are triggered. Example of Imbalance: Insert 3 into [10, 5, 15] : Tree becomes [10, 5, 15] with 5 having a left child 3 . Heights: Left subtree of 10 (rooted at 5 ) has height 2, right subtree (rooted at 15 ) has height 1. Balance factor of 10 becomes -2 , indicating an imbalance. Performance Impact:\n",
      "Stored embedding for: The self-balancing property ensures that search, insertion, and deletion operations remain O(log n), even in worst-case scenarios, unlike an unbalanced BST, which could degrade to O(n) (e.g., a skewed tree resembling a linked list). 2.2 Rotations for Rebalancing Rotations are the mechanisms AVL trees use to restore balance after insertions or deletions disrupt the balance factor constraint. There are four types of rotations: two single rotations and two double rotations, each addressing a specific imbalance pattern. Single Rotations: Left Rotation: When Used: Applied when a node’s balance factor is 2 , meaning the right subtree is too heavy (height difference of 2). Mechanism: The right child of the unbalanced node is promoted to take its place, and the unbalanced node becomes the left child of the new root. The left subtree of the right child (if any) becomes the right subtree of the demoted node. Detailed Example: Tree: [10, null, 20] with 20 having right child 30 . After inserting 30 , tree becomes [10, null, 20] with 20 → 30 . Balance factor of 10 is 2 (right height 2, left height 0). Left rotation: 1. 20 becomes the new root. 2. 10 becomes the left child of 20 . 3. 30 remains the right child of 20 . Result: [20, 10, 30] , with all nodes balanced (balance factor 0 ). Right Rotation: When Used: Applied when a node’s balance factor is -2 , meaning the left subtree is too heavy. Mechanism: The left child of the unbalanced node is promoted, and the unbalanced node becomes the right child of the new root. The right subtree of the left child (if any) becomes the left subtree of the demoted node. Detailed Example: Tree: [10, 5, null] with 5 having left child 3 . After inserting 3 , balance factor of 10 is -2 (left height 2, right height 0). Right rotation: 1. 5 becomes the new root. 2. 10 becomes the right child of 5 . 3. 3 remains the left child of 5 . Result: [5, 3, 10] , all balanced. Double Rotations: Left-Right Rotation: When Used: Occurs when the left child’s right subtree causes the imbalance (a \"zig-zag\" pattern). The balance factor of the root is -2 , and the left child’s balance factor is 1 . Mechanism: Two steps: 1. Perform a left rotation on the left child to align the subtree. 2. Perform a\n",
      "Stored embedding for: factor of 10 is -2 (left height 2, right height 0). Right rotation: 1. 5 becomes the new root. 2. 10 becomes the right child of 5 . 3. 3 remains the left child of 5 . Result: [5, 3, 10] , all balanced. Double Rotations: Left-Right Rotation: When Used: Occurs when the left child’s right subtree causes the imbalance (a \"zig-zag\" pattern). The balance factor of the root is -2 , and the left child’s balance factor is 1 . Mechanism: Two steps: 1. Perform a left rotation on the left child to align the subtree. 2. Perform a right rotation on the original unbalanced node. Detailed Example: Tree: [10, 5, null] with 5 having right child 7 . Insert 7 : Tree becomes [10, 5, null] with 5 → 7 . Balance factor of 10 is -2 , and 5 has balance factor 1 . Steps: 1. Left rotation on 5 : [5, null, 7] becomes [7, 5, null] .\n",
      "Stored embedding for: 2. Right rotation on 10 : [10, 7, null] with 7 → 5 becomes [7, 5, 10] . Result: [7, 5, 10] , balanced. Right-Left Rotation: When Used: Occurs when the right child’s left subtree causes the imbalance (a \"zag-zig\" pattern). The balance factor of the root is 2 , and the right child’s balance factor is -1 . Mechanism: Two steps: 1. Perform a right rotation on the right child. 2. Perform a left rotation on the original unbalanced node. Detailed Example: Tree: [10, null, 20] with 20 having left child 15 . Insert 15 : Balance factor of 10 is 2 , and 20 has balance factor -1 . Steps: 1. Right rotation on 20 : [20, 15, null] becomes [15, null, 20] . 2. Left rotation on 10 : [10, null, 15] with 15 → 20 becomes [15, 10, 20] . Result: [15, 10, 20] , balanced. Practical Implications: Rotations ensure that AVL trees remain efficient for dynamic datasets where insertions and deletions are frequent. However, the overhead of rebalancing makes them less suitable for disk-based storage, where minimizing I/O is more critical than maintaining perfect balance (see B+ trees below). 3. Disk-Based Indexing and Trees Optimized for Disk Storage For datasets too large to fit in RAM, disk-based indexing structures like B+ trees are designed to optimize retrieval performance by minimizing disk I/O operations, which are orders of magnitude slower than memory access. This section explores B+ trees in depth and their role in large-scale database systems. 3.1 B+ Trees Structure: Nodes as Disk Pages: Each node in a B+ tree corresponds to a disk page (typically 4KB or 8KB), allowing a large branching factor—the number of children a node can have. A high branching factor reduces the tree’s height, minimizing the number of disk accesses needed to reach a leaf. Technical Detail: If a node can store 100 keys and 101 pointers (a branching factor of 101), a tree with 1 million keys might have a height of only 3 levels (root, internal nodes, leaves), compared to a binary tree’s height of around 20. Example: In a database index, an internal node might store keys [100, 200, 300] with pointers to child nodes containing ranges <100 , 100-200 , 200-300 , and >300 . Leaf-Level Data: Unlike binary trees, B+ trees store all data (or pointers to data) in the leaf nodes, which are linked\n",
      "Stored embedding for: needed to reach a leaf. Technical Detail: If a node can store 100 keys and 101 pointers (a branching factor of 101), a tree with 1 million keys might have a height of only 3 levels (root, internal nodes, leaves), compared to a binary tree’s height of around 20. Example: In a database index, an internal node might store keys [100, 200, 300] with pointers to child nodes containing ranges <100 , 100-200 , 200-300 , and >300 . Leaf-Level Data: Unlike binary trees, B+ trees store all data (or pointers to data) in the leaf nodes, which are linked sequentially in a linked list-like structure. Internal nodes contain only keys and pointers to guide traversal. Advantage: Sequential linking enables efficient range queries (e.g., “find all records between 100 and 200”) by traversing the leaf level without revisiting higher levels. Example: A B+ tree indexing a table of employee IDs might have leaf nodes storing [101, 102, 103] → [104, 105, 106] , linked for quick sequential access. Advantages Over AVL Trees for Large Datasets: Reduced Disk Accesses: The high branching factor ensures a shorter tree height. Since each node access is a disk read (taking ~10ms on a typical hard drive vs. ~100ns for RAM), fewer levels mean significantly faster queries.\n",
      "Stored embedding for: Comparison: An AVL tree with 1 million nodes has a height of ~20, requiring up to 20 disk reads for a search. A B+ tree with a branching factor of 100 might have a height of 3, requiring only 3 reads—a 6x reduction in I/O operations. Efficient Range Queries: The linked leaf nodes allow sequential access to consecutive keys, ideal for queries like “find all sales between January and March.” Example: In a filesystem, a B+ tree might index file metadata, enabling rapid retrieval of all files modified within a date range. Optimized for Disk I/O: Disk I/O is the bottleneck in large-scale systems. B+ trees align node size with disk page size, ensuring each read fetches a full page of useful data, maximizing throughput. Technical Note: If a disk page is 4KB and a key-pointer pair is 16 bytes, a node can store ~250 entries, supporting a branching factor of 251. Trade-Offs: B+ trees sacrifice some insertion and deletion efficiency (due to splitting or merging nodes) for superior query performance on disk, making them less ideal for in-memory use compared to AVL trees. 3.2 Disk-Based Indexing Concept: Disk-based indexing involves creating data structures (like B+ trees) stored on disk to accelerate access to large datasets that cannot fit in RAM. These indexes map keys to physical locations (e.g., disk block addresses) of the actual data. Example: In a relational database, an index on a customer_id column might map IDs to the disk locations of customer records, avoiding a full table scan for queries like SELECT * FROM customers WHERE customer_id = 123 . Importance: Efficient Query Performance: Without indexes, querying a large table requires scanning every record (O(n)), which is impractical for datasets with millions or billions of rows. Indexes reduce this to O(log n) or better. Real-World Example: In an e-commerce database with 10 million orders, an index on order_date enables fast retrieval of all orders from a specific month. Minimized I/O Operations: By organizing keys hierarchically, indexes ensure that only a few disk pages are read to locate a record. This is critical given disk I/O’s latency (e.g., 10ms per read vs. 0.1µs for RAM). Optimization: Database systems tune B+ tree parameters (e.g., node size) to match hardware characteristics, such as disk block size or SSD page size. Data Integrity and Recovery: Indexes support efficient updates and consistency checks. After a crash, transaction logs can rebuild indexes,\n",
      "Stored embedding for: In an e-commerce database with 10 million orders, an index on order_date enables fast retrieval of all orders from a specific month. Minimized I/O Operations: By organizing keys hierarchically, indexes ensure that only a few disk pages are read to locate a record. This is critical given disk I/O’s latency (e.g., 10ms per read vs. 0.1µs for RAM). Optimization: Database systems tune B+ tree parameters (e.g., node size) to match hardware characteristics, such as disk block size or SSD page size. Data Integrity and Recovery: Indexes support efficient updates and consistency checks. After a crash, transaction logs can rebuild indexes, ensuring data remains accessible and correct. Example: In a banking system, an index on account_number ensures quick lookups while maintaining consistency with transaction logs. 4. Transactions in Database Systems Transactions are the cornerstone of reliable database operations, ensuring that complex, multi-step processes execute correctly even in the face of failures or concurrent access. 4.1 Definition of a Transaction\n",
      "Stored embedding for: A transaction is a sequence of database operations (e.g., reads, writes) treated as a single, indivisible unit. Either all operations succeed, and the transaction is committed, or none are applied, and it is rolled back. Detailed Example: Transferring $100 from Account A (balance $500) to Account B (balance $200): Operations: 1. Read A’s balance: $500. 2. Update A’s balance: $500 - $100 = $400. 3. Read B’s balance: $200. 4. Update B’s balance: $200 + $100 = $300. If any step fails (e.g., a crash after step 2), the transaction rolls back, restoring A to $500 and B to $200. Scope: Transactions can span multiple tables, databases, or even distributed systems, depending on the application’s complexity. 4.2 ACID Properties The ACID properties—Atomicity, Consistency, Isolation, and Durability—define the guarantees that transactions provide for reliability and data integrity: Atomicity: Ensures that all operations in a transaction are completed successfully, or none are applied. This \"all-or-nothing\" principle prevents partial updates. Mechanism: Databases use rollback mechanisms (e.g., undoing changes via logs) if a transaction fails. Example: In the money transfer above, if the system crashes after debiting A but before crediting B, atomicity ensures the debit is reversed, avoiding a $100 loss. Real-World Context: In an airline reservation system, booking a seat involves reserving the seat and charging the customer. Atomicity ensures both happen or neither does, preventing overbooking or uncharged reservations. Consistency: Guarantees that a transaction moves the database from one valid state to another, respecting all rules, constraints, and schemas (e.g., primary keys, foreign keys, data types). Detailed Example: If a constraint requires account_balance >= 0 , a transaction withdrawing $600 from A ($500) would fail, rolling back to maintain consistency. Technical Insight: Consistency is enforced by the database’s constraint checker and transaction manager, which validate each operation against defined rules. Broader Implication: Beyond technical constraints, consistency can include business logic, such as ensuring a customer’s order total matches the sum of item prices. Isolation: Ensures that transactions execute independently, with intermediate states invisible to other transactions. This prevents concurrency issues like dirty reads (reading uncommitted data) or lost updates (overwriting changes). Levels of Isolation: Databases offer varying isolation levels (e.g., Read Uncommitted, Read Committed, Serializable), balancing consistency with performance. Example: If Transaction T1 debits A and Transaction T2 reads A’s balance concurrently, isolation ensures T2 sees either the pre-debit ($500) or post-commit ($400) value, not an intermediate state. Mechanism: Achieved via\n",
      "Stored embedding for: Implication: Beyond technical constraints, consistency can include business logic, such as ensuring a customer’s order total matches the sum of item prices. Isolation: Ensures that transactions execute independently, with intermediate states invisible to other transactions. This prevents concurrency issues like dirty reads (reading uncommitted data) or lost updates (overwriting changes). Levels of Isolation: Databases offer varying isolation levels (e.g., Read Uncommitted, Read Committed, Serializable), balancing consistency with performance. Example: If Transaction T1 debits A and Transaction T2 reads A’s balance concurrently, isolation ensures T2 sees either the pre-debit ($500) or post-commit ($400) value, not an intermediate state. Mechanism: Achieved via locking (e.g., exclusive locks on modified rows) or multi-version concurrency control (MVCC), where each transaction sees a snapshot of the database. Real-World Example: In an inventory system, isolation prevents two users from simultaneously reserving the last item, ensuring accurate stock levels. Durability: Guarantees that once a transaction is committed, its changes are permanently saved, surviving system failures like power outages or crashes.\n",
      "Stored embedding for: Mechanism: Achieved through write-ahead logging (WAL), where changes are written to a persistent log before being applied to the database. After a crash, the log is replayed to recover committed changes. Detailed Example: After committing the $100 transfer, the new balances ($400, $300) are written to disk. If the system crashes immediately after, durability ensures these values are restored on restart. Hardware Consideration: Durability depends on non-volatile storage (e.g., SSDs, HDDs) and can be enhanced with battery- backed caches or replication. 5. Distributed Systems, CAP Theorem, and Scaling Distributed systems distribute data and computation across multiple nodes, introducing challenges in consistency, availability, and fault tolerance. The CAP theorem and scaling strategies provide frameworks for designing such systems. 5.1 CAP Theorem Concept: The CAP theorem states that a distributed system can only guarantee two of three properties during a network partition (when nodes cannot communicate): Consistency: All nodes see the same data at the same time (e.g., every read reflects the latest write). Availability: Every request receives a response, even if some nodes are unavailable. Partition Tolerance: The system continues operating despite network failures splitting nodes into isolated groups. Trade-Off: Designers must prioritize based on use case: CP (Consistency + Partition Tolerance): Sacrifices availability (e.g., banking systems prioritize accurate balances over uptime during partitions). AP (Availability + Partition Tolerance): Sacrifices consistency (e.g., social media platforms may show slightly outdated data to remain accessible). Detailed Example: In a distributed key/value store with nodes A and B: Write key = 5 to A, but a partition prevents B from syncing. A CP system blocks reads from B until synced (consistent but unavailable). An AP system allows B to return an old value (e.g., key = 3 ), prioritizing availability. Single-Node Systems: In a single-node system (e.g., a standalone MySQL instance), partitions don’t occur, so CAP trade-offs are irrelevant. The system can provide both consistency and availability unless the node itself fails. Implication: CAP becomes significant only when scaling to multiple nodes, where network issues introduce partitions. 5.2 Scaling Techniques Horizontal Scaling (Scaling Out): Definition: Adds more nodes to distribute load and data, increasing capacity and fault tolerance. Example: A web application with 1 million users might deploy 10 servers behind a load balancer, each handling 100,000 users. Implementation: Requires partitioning data (sharding) or replicating it across nodes. Sharding Example: Split a user database by ID range (e.g., 0-999,999 on Node 1, 1,000,000-1,999,999 on\n",
      "Stored embedding for: so CAP trade-offs are irrelevant. The system can provide both consistency and availability unless the node itself fails. Implication: CAP becomes significant only when scaling to multiple nodes, where network issues introduce partitions. 5.2 Scaling Techniques Horizontal Scaling (Scaling Out): Definition: Adds more nodes to distribute load and data, increasing capacity and fault tolerance. Example: A web application with 1 million users might deploy 10 servers behind a load balancer, each handling 100,000 users. Implementation: Requires partitioning data (sharding) or replicating it across nodes. Sharding Example: Split a user database by ID range (e.g., 0-999,999 on Node 1, 1,000,000-1,999,999 on Node 2). Replication Example: Each node holds a full copy of the data, with writes synchronized via a consensus protocol (e.g., Raft, Paxos). Advantages: Scales indefinitely with more nodes. Enhances availability (if one node fails, others continue). Challenges:\n",
      "Stored embedding for: Data consistency across nodes (e.g., syncing replicas). Complexity in partitioning and load balancing. Vertical Scaling (Scaling Up): Definition: Increases the capacity of a single node by adding resources (e.g., more CPU cores, RAM, or disk space). Example: Upgrading a database server from 16GB to 64GB RAM to handle larger in-memory datasets. Implementation: Requires no architectural changes—just hardware upgrades. Advantages: Simpler to manage (no distributed logic). Immediate performance boost for single-threaded tasks. Limitations: Hardware Ceiling: Physical limits cap upgrades (e.g., a server might max out at 128 cores or 2TB RAM). Single Point of Failure: If the node crashes, the entire system goes down, unlike horizontal scaling’s redundancy. Cost: High-end hardware grows exponentially expensive (e.g., a 1TB RAM server costs far more than four 256GB servers). Choosing Between Techniques: Horizontal: Preferred for massive scale, high availability, and fault tolerance (e.g., Google’s infrastructure with thousands of nodes). Vertical: Suitable for simpler systems or when latency is critical and redundancy isn’t (e.g., a small business database). Hybrid Approach: Many systems combine both—starting with vertical scaling, then adding nodes as demand grows. 6. Key/Value Stores and Their Role in Machine Learning Key/value stores are lightweight, high-performance databases optimized for simple key-based lookups, making them valuable in various domains, including machine learning. 6.1 Key/Value Stores Definition: A key/value store maps unique keys to values, functioning like a distributed hash table or dictionary. Keys are typically strings or integers, and values can be simple (e.g., numbers) or complex (e.g., JSON objects). Examples: Redis (in-memory), DynamoDB (cloud-based), Memcached (caching). As a Feature Store in Machine Learning: Role: In ML pipelines, feature stores manage precomputed features—input data used by models for training and inference. Features are stored as key/value pairs, where the key might be a user ID or timestamp, and the value is a feature vector (e.g., [age, income, clicks] ). Detailed Example: In a recommendation system: Key: user_123 . Value: { \"last_purchase\": \"2023-10-01\", \"categories_viewed\": [\"electronics\", \"books\"], \"avg_spend\": 50.75 } . The model queries this data in real time to predict items user_123 might like. Benefits: Low Latency: In-memory stores like Redis deliver sub-millisecond lookups, critical for real-time predictions (e.g., ad targeting). Ease of Updating: Features can be updated incrementally (e.g., appending a new click event) without rewriting the entire dataset. Scalability: Distributed key/value stores (e.g., DynamoDB) scale horizontally, handling massive feature sets for millions of users.\n",
      "Stored embedding for: [age, income, clicks] ). Detailed Example: In a recommendation system: Key: user_123 . Value: { \"last_purchase\": \"2023-10-01\", \"categories_viewed\": [\"electronics\", \"books\"], \"avg_spend\": 50.75 } . The model queries this data in real time to predict items user_123 might like. Benefits: Low Latency: In-memory stores like Redis deliver sub-millisecond lookups, critical for real-time predictions (e.g., ad targeting). Ease of Updating: Features can be updated incrementally (e.g., appending a new click event) without rewriting the entire dataset. Scalability: Distributed key/value stores (e.g., DynamoDB) scale horizontally, handling massive feature sets for millions of users.\n",
      "Stored embedding for: Real-World Context: Netflix might use a feature store to track user watch history, enabling rapid feature retrieval for its recommendation engine. 7. Redis: History and Commands Redis is a high-performance, in-memory key/value store widely used for caching, queuing, and real-time applications. 7.1 Redis Overview History: Redis (Remote Dictionary Server) was created by Salvatore Sanfilippo (aka antirez) and first released in 2009. Initially designed as a fast key/value store for a web analytics tool, it evolved into a versatile data structure server supporting lists, sets, hashes, and more. Evolution: Open-sourced under the BSD license, Redis gained traction for its speed (in-memory operations) and simplicity, with features like persistence (via snapshots or logs) added later. Command Differences: INCR: An atomic command that increments the integer value of a key by 1. If the key doesn’t exist, it initializes it to 0 before incrementing. Syntax: INCR key . Example: SET visits 10 → INCR visits → Value becomes 11 . Use Case: Tracking page views or event counts in real time, ensuring no race conditions in concurrent updates. INC: Not a valid Redis command. It may be a typo or confusion with INCR . Redis’s command set includes INCRBY (increment by a specified amount) but no INC . Clarification: If a user references INC , they likely mean INCR . 8. MongoDB: Querying, Operators, and Data Formats MongoDB is a leading NoSQL database using a document-oriented model, storing data in BSON (Binary JSON) format for flexibility and performance. 8.1 BSON vs. JSON BSON (Binary JSON): A binary-encoded extension of JSON, adding data types like Date , Binary , ObjectId , and Int64 that JSON lacks. Advantages: Efficiency: Binary format enables faster parsing and smaller storage than JSON’s text-based structure. Indexing: Supports rich queries and indexing on fields like ObjectId (a unique identifier for documents). Example: { \"_id\": ObjectId(\"507f1f77bcf86cd799439011\"), \"date\": ISODate(\"2023-01-01T00:00:00Z\"), \"data\": BinData(0, \"abc\") } . JSON: A lightweight, text-based format supporting strings, numbers, booleans, arrays, and objects. Limitations: Lacks native support for dates or binary data, requiring string encodings that are less efficient. Example: { \"name\": \"Alice\", \"age\": 30, \"active\": true } . Comparison:\n",
      "Stored embedding for: documents). Example: { \"_id\": ObjectId(\"507f1f77bcf86cd799439011\"), \"date\": ISODate(\"2023-01-01T00:00:00Z\"), \"data\": BinData(0, \"abc\") } . JSON: A lightweight, text-based format supporting strings, numbers, booleans, arrays, and objects. Limitations: Lacks native support for dates or binary data, requiring string encodings that are less efficient. Example: { \"name\": \"Alice\", \"age\": 30, \"active\": true } . Comparison:\n",
      "Stored embedding for: JSON is human-readable and interoperable but slower to process. BSON trades readability for performance, making it ideal for MongoDB’s internal storage and querying. 8.2 MongoDB Query Language Query Construction: Queries use the find() method to retrieve documents matching a condition, expressed as a JSON-like object. Basic Example: db.users.find({ \"age\": 25 }) returns all documents where age is 25. Projection: Controls which fields are returned using a second argument. Example: db.users.find({ \"age\": 25 }, { \"name\": 1, \"_id\": 0 }) returns only the name field, excluding _id . Operators: $gte (Greater Than or Equal To), $lte (Less Than or Equal To): Define ranges for numeric or date fields. Example: db.orders.find({ \"total\": { $gte: 100, $lte: 500 } }) finds orders with totals between 100 and 500 inclusive. Date Example: db.events.find({ \"date\": { $gte: ISODate(\"2023-01-01\"), $lte: ISODate(\"2023-12-31\") } }) retrieves 2023 events. $nin (Not In): Excludes documents where a field’s value matches any in a specified array. Example: db.products.find({ \"category\": { $nin: [\"electronics\", \"clothing\"] } }) finds products not in those categories. Use Case: Filtering out irrelevant items, like excluding certain statuses in a task tracker. 9. Bringing It All Together 9.1 Integration of Concepts Data Structures and Indexing: Arrays and linked lists underpin basic storage, while AVL trees and B+ trees optimize higher-level operations. AVL trees excel in memory for balanced, logarithmic-time access, whereas B+ trees dominate disk-based indexing by minimizing I/O with high branching factors and sequential leaf access. Design Impact: A memory-constrained system might use AVL trees for small, dynamic datasets (e.g., a real-time analytics dashboard), while a database with terabytes of data relies on B+ trees for efficient querying. Transactions and ACID: ACID properties ensure reliability in both single-node and distributed databases. In a distributed context, they interact with CAP trade-offs—e.g., a CP system might enforce strict ACID compliance at the cost of availability during partitions. Example: An e-commerce platform uses transactions to process orders (atomic updates to inventory and payment) and B+ tree indexes to quickly locate products. Scaling, CAP, and Data Stores: Scaling decisions (horizontal vs. vertical) and CAP priorities shape data store choices. Key/value stores like Redis offer low- latency caching for horizontally scaled systems, while MongoDB’s document model suits flexible, distributed storage with eventual consistency (AP). System Design: A social media app might use Redis for real-time notifications (AP-focused), MongoDB for user profiles (scalable storage), and B+ trees in its backend database for search\n",
      "Stored embedding for: compliance at the cost of availability during partitions. Example: An e-commerce platform uses transactions to process orders (atomic updates to inventory and payment) and B+ tree indexes to quickly locate products. Scaling, CAP, and Data Stores: Scaling decisions (horizontal vs. vertical) and CAP priorities shape data store choices. Key/value stores like Redis offer low- latency caching for horizontally scaled systems, while MongoDB’s document model suits flexible, distributed storage with eventual consistency (AP). System Design: A social media app might use Redis for real-time notifications (AP-focused), MongoDB for user profiles (scalable storage), and B+ trees in its backend database for search indexes, balancing performance and consistency. By weaving these concepts together, system architects can design solutions that optimize for specific needs—whether speed, reliability, scalability, or a blend of all three—tailored to the application’s domain and scale.\n",
      " -----> Processed Comprehensive Document on Data Structures, Databases, and Distributed Systems.pdf\n"
     ]
    }
   ],
   "source": [
    "src.ingest.clear_redis_store()\n",
    "src.ingest.create_hnsw_index()\n",
    "\n",
    "src.ingest.process_pdfs(\"./data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 RAG Search Interface\n",
      "Type 'exit' to quit\n",
      "context_str: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minteractive_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DS 4300/P02_DG_BK-main/src/search.py:126\u001b[0m, in \u001b[0;36minteractive_search\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m context_results \u001b[38;5;241m=\u001b[39m search_embeddings(query)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Generate RAG response\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_rag_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Response ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[0;32m~/Documents/DS 4300/P02_DG_BK-main/src/search.py:104\u001b[0m, in \u001b[0;36mgenerate_rag_response\u001b[0;34m(query, context_results)\u001b[0m\n\u001b[1;32m     92\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are a helpful AI assistant. \u001b[39m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124m    Use the following context to answer the query as accurately as possible. If the context is \u001b[39m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124m    not relevant to the query, say \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt know\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m \n\u001b[1;32m    101\u001b[0m \u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;66;03m# Generate response using Ollama\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeepseek-r1:latest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_predict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/practicals/lib/python3.11/site-packages/ollama/_client.py:333\u001b[0m, in \u001b[0;36mClient.chat\u001b[0;34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchat\u001b[39m(\n\u001b[1;32m    290\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    291\u001b[0m   model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    298\u001b[0m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[1;32m    300\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;124;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mChatResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/chat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/practicals/lib/python3.11/site-packages/ollama/_client.py:178\u001b[0m, in \u001b[0;36mClient._request\u001b[0;34m(self, cls, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpart)\n\u001b[1;32m    176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m--> 178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[0;32m~/anaconda3/envs/practicals/lib/python3.11/site-packages/ollama/_client.py:118\u001b[0m, in \u001b[0;36mClient._request_raw\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_request_raw\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    117\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     r\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/anaconda3/envs/practicals/lib/python3.11/site-packages/httpx/_client.py:825\u001b[0m, in \u001b[0;36mClient.request\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    810\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    812\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[1;32m    813\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    814\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    823\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[1;32m    824\u001b[0m )\n\u001b[0;32m--> 825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/practicals/lib/python3.11/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/anaconda3/envs/practicals/lib/python3.11/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/practicals/lib/python3.11/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/practicals/lib/python3.11/site-packages/httpx/_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1011\u001b[0m     )\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/anaconda3/envs/practicals/lib/python3.11/site-packages/httpx/_transports/default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    248\u001b[0m )\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    255\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    256\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    257\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    258\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    259\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/practicals/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m~/anaconda3/envs/practicals/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/anaconda3/envs/practicals/lib/python3.11/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/practicals/lib/python3.11/site-packages/httpcore/_sync/http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/anaconda3/envs/practicals/lib/python3.11/site-packages/httpcore/_sync/http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/anaconda3/envs/practicals/lib/python3.11/site-packages/httpcore/_sync/http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/practicals/lib/python3.11/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/anaconda3/envs/practicals/lib/python3.11/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "src.search.interactive_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import experiment\n",
    "\n",
    "\n",
    "embedding_models = [\"nomic-embed-text\", \"all-MiniLM-L6-v2\", \"deepseek-r1\", \"mistral\"]\n",
    "chunk_sizes = [200, 500, 1000]\n",
    "overlaps = [0, 50, 100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 RAG Search Interface\n",
      "Type 'exit' to quit\n",
      "context_str: \n"
     ]
    },
    {
     "ename": "ResponseError",
     "evalue": "model is required (status code: 400)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# for embedding_model in embedding_models:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m embedding_model \u001b[38;5;241m=\u001b[39m embedding_models[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m experiment \u001b[38;5;241m=\u001b[39m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m experiment\u001b[38;5;241m.\u001b[39mrun_experiment()\n",
      "File \u001b[0;32m~/Documents/DS 4300/P02_DG_BK-main/experiment.py:30\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(embedding_model)\u001b[0m\n\u001b[1;32m     28\u001b[0m initial_memory \u001b[38;5;241m=\u001b[39m measure_memory()\n\u001b[1;32m     29\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 30\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minteractive_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     32\u001b[0m final_memory \u001b[38;5;241m=\u001b[39m measure_memory()\n",
      "File \u001b[0;32m~/Documents/DS 4300/P02_DG_BK-main/src/search.py:129\u001b[0m, in \u001b[0;36minteractive_search\u001b[0;34m(model, query)\u001b[0m\n\u001b[1;32m    126\u001b[0m context_results \u001b[38;5;241m=\u001b[39m search_embeddings(model, query)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Generate RAG response\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_rag_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Response ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[0;32m~/Documents/DS 4300/P02_DG_BK-main/src/search.py:105\u001b[0m, in \u001b[0;36mgenerate_rag_response\u001b[0;34m(query, context_results)\u001b[0m\n\u001b[1;32m     93\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are a helpful AI assistant. \u001b[39m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124m    Use the following context to answer the query as accurately as possible. If the context is \u001b[39m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124m    not relevant to the query, say \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt know\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m \n\u001b[1;32m    102\u001b[0m \u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# Generate response using Ollama\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mMODEL\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m:latest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_predict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Interactive search interface.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/practicals/lib/python3.11/site-packages/ollama/_client.py:333\u001b[0m, in \u001b[0;36mClient.chat\u001b[0;34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchat\u001b[39m(\n\u001b[1;32m    290\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    291\u001b[0m   model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    298\u001b[0m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[1;32m    300\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;124;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mChatResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/chat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/practicals/lib/python3.11/site-packages/ollama/_client.py:178\u001b[0m, in \u001b[0;36mClient._request\u001b[0;34m(self, cls, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpart)\n\u001b[1;32m    176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m--> 178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[0;32m~/anaconda3/envs/practicals/lib/python3.11/site-packages/ollama/_client.py:122\u001b[0m, in \u001b[0;36mClient._request_raw\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m r\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 122\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mConnectError:\n\u001b[1;32m    124\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(CONNECTION_ERROR_MESSAGE) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mResponseError\u001b[0m: model is required (status code: 400)"
     ]
    }
   ],
   "source": [
    "# for embedding_model in embedding_models:\n",
    "embedding_model = embedding_models[2]\n",
    "experiment = experiment.main(embedding_model)\n",
    "experiment.run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practicals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
